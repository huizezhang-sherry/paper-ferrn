---
title: "Visual Diagnostics for Constrained Optimisation with Application to Guided Tours"
author:
  - name: H.Sherry Zhang
    affiliation: Monash University 
    address: Department of Econometrics and Business Statistics
    email:  huize.zhang@monash.edu
  - name: Dianne Cook
    affiliation: Monash University 
    address: Department of Econometrics and Business Statistics
    email:  dicook@monash.edu
  - name: Ursula Laa
    affiliation: University of Natural Resources and Life Sciences
    address: Institute of Statistics
    email:  ursula.laa@boku.ac.at  
  - name: Nicolas Langrené
    affiliation: CSIRO Data61 
    address: 34 Village Street, Docklands VIC 3008 Australia
    email: nicolas.langrene@csiro.au
  - name: Patricia Menéndez
    affiliation: Monash University 
    address: Department of Econometrics and Business Statistics
    email:  patricia.menendez@monash.edu 

      
abstract: >
  Guided tour searches for interesting low-dimensional views of high-dimensional data via optimising a projection pursuit index function. The first paper of projection pursuit by @friedman1974projection stated that "the technique used for maximising the projection index strongly influences both the statistical and the computational aspects of the procedure." While many indices have been proposed in the literature, less work has been done on evaluating the performance of the optimisers. In this paper, we implement a data collection object for the optimisation in the guided tour and introduce visual diagnostics based on the data object collected. These diagnostics and workflows can be applied to a broad class of optimisers, to assess their performance. An R package, \pkg{ferrn}, has been created to implement the diagnostics.
preamble: >
  \usepackage{amssymb, amsmath, mathtools, dsfont, bbm, array, booktabs}
  \usepackage[ruled,vlined, linesnumbered]{algorithm2e}
output: rticles::rjournal_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE,
                      root.dir = here::here(), 
                      fig.path = here::here("figs/"), 
                      cache = TRUE,
                      fig.align = "centre",
                      fig.height = 3,
                      out.width = "100%")
```

```{r external, include = FALSE, cache = FALSE}
library(knitr)
library(tidyverse)
library(ferrn)
read_chunk(here::here('scripts/toy.R'))
read_chunk(here::here('scripts/diagnose.R'))
read_chunk(here::here('scripts/kol_result.R'))
read_chunk(here::here('scripts/implementation.R'))
```

```{r load-pkg}
```

```{r }
# spelling::spell_check_files(here::here("paper/zhang-cook-laa-langrene-menendez.Rmd"), lang = "en-GB", ignore = spelling::get_wordlist())
```


# Introduction

Visualisation is widely used in exploratory data analysis [@tukey1977exploratory; @unwin2015graphical;  @healy2018data; @wilke2019fundamentals]. Presenting information in graphics often unveils information that would otherwise not be aware of and provides a more comprehensive understanding of the problem at hand. Task specific tools presented by @li2020visualizing show how visualisation can be used to understand the behaviour of neural network on classification models, but no general visualisation tool available for diagnosing optimisation procedure. The work presented in this paper brings visualization tools into optimisation problems with an aim to better understand the performance of the optimisers in practice.

The goal of continuous optimisation is to find the best solution within the space of all feasible solutions where typically the best solution is decided by an objective function. Broadly speaking, optimization can be unconstrained or constrained [@kelley1999iterative]. The unconstrained problem can be formulated as a minimization (or maximization)  problem such as 
$\min_{x} f(x)$ where $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is an objective function with  certain properties defined in an $L^p$ space. In this case, solutions rely on gradient descent or ascent methods. In the constrained optimization problem additional restrictions  are introduced via a set of functions that can be convex or non-convex: $g_i:\mathbb{R}^n \rightarrow \mathbb{R}$ for $i = 1, \ldots k$ and hence the problem can be written as
$\min_{x} f(x)$ *subject to* $g_i(x) \leq 0$. Here methods such as multipliers and convex optimization methods including linear and quadratic programming can be used.

The focus of this paper is on the optimisation problem arising in the projection pursuit guided tour [@buja2005computational] which is an exploratory data analysis tool that is defined to detect *interesting structures* or features in high-dimensional data through a set of lower-dimensional projections that cover the entire high dimensional space using interpolation methods called tours [@cook2008grand]. The target of the optimisation is to identify the most *interesting* low-dimensional views of the data given by a corresponding projection matrix. The most *interesting* structures are formally defined by a function of projections, called index function which is optimized to uncover the most revelling structures in a high dimensional space [@cook1993projection]. 


The optimization challenges encountered in the projection pursuit guided tour problem are common to those of optimization in general. Examples of those include the existence of multiple maxima (local and global), the trade off between computational burden and proximity to the maxima,  dealing with noisy objective functions that might be non smooth and non differentiable [@jones1998efficient]. Those  are not unique to this context and therefore the visualization tools and optimization methods presented in this paper can be easily applied to any other optimization problems.

The remainder of the paper is organised as follows. 
Section \ref{optim} provides an overview of optimisation methods, specifically line search methods. 
Section \ref{tour} reviews projection pursuit guided tour, defines the optimisation problem and introduces three existing algorithms.
Section \ref{vis-diag} presents the new visual diagnostics. A data structure is defined to capture information during the optimisation, and used in different types of diagnostic plots.
Section \ref{application} shows applications of how these plots can be used to understand and compare different algorithms. We also discuss how these insights contribute to modifications that improve the algorithms.
Finally, Section \ref{implementation} describes the R package: \pkg{ferrn}, that implements the visual diagnostics.


# Optimisation methods {#optim}

Optimization problems are ubiquitous in many areas of study. While in some cases analytical solutions can be found, the majority of problems rely on numerical methods to find the optimal solution. These numerical methods follow iterative approaches that aim at finding the optimum by progressively improving the current solution until a desirable accuracy is achieved. Although this principle seems uncomplicated, a number of challenges arise such as the possible existence of multiple maxima (local and global), constraints and noisy objective function, and the trade-off between desirable accuracy and computational burden. In addition, the optimization results might depend on the algorithm starting values, affecting the consistency of results. 

Optimization methods can be divided into various classes, such as global optimisation [@kelley1999iterative; @fletcher2013practical], convex optimisation [@boyd2004convex] or stochastic optimisation [@nocedal2006numerical]. Our interest is on constrained optimization [@bertsekas2014constrained] as defined in the introduction section, and assuming it is not possible to find a solution to the problem in the way of a closed-form. That is, the problem consists of finding the minimum or maximum of a function $f \in L^p$ in the constrained $\mathbb{A}$ space. 

A large class of methods utilises the gradient information of the objective function to perform the optimisation iterations, with the most notable one being the gradient ascent (descent) method. Although gradient optimization methods are popular, they rely on the availability of the objective function derivatives and on the complexity of the constraints. Derivative-free methods, which do not rely on the knowledge of the gradient, are more generally applicable. Derivative-free methods have been developed over the years, where the emphasis is on finding, in most cases, a near optimal solution. Examples of those include response surface methodology [@box1951experimental], stochastic approximation [@robbins1951stochastic], random search [@fu2015handbook] and heuristic methods [@sorensen2013metaheuristics]. Later, we will present a simulated annealing optimisation algorithm, which belongs to the class of random search methods, for optimisation with the guided tour.

A common search scheme utilised by both derivative-free methods and gradient methods is line search. In line search methods, users are required to provide an initial estimate $x_{1}$ and, at each iteration, a search direction $S_k$ and a step size $\alpha_k$ are generated. Then one moves on to the next point following $x_{k+1} = x_k + \alpha_kS_k$ and the process is repeated until the desire convergence is reached. While gradient-based methods choose the search direction by the gradient, derivative-free methods uses local information of the objective function to determine the search direction. The choice of step size also needs considerations, as inadequate step sizes might prevent the optimisation method to converge to an optimum. An ideal step size can be chosen via finding the value of $\alpha_k \in \mathbb{R}$ that maximises $f(x_k + \alpha_kS_k)$ with respect to $\alpha_k$ at each iteration. 

Several R implementations address optimization problems with both general purpose as well as task specifics solvers. The most prominent one within the general solvers is \code{optim()} in the \CRANpkg{stats} [@stats] package, which provides both gradient-based and derivative-free optimisation functions. Another general solver specialised in non-linear optimisation is \CRANpkg{nloptr} [@nloptr]. Specific solvers for simulated annealing includes \code{optim(..., method  "SANN")} and package \CRANpkg{GenSA} [@gensa] that deals with more complicated objective functions. For other task specific solvers, readers are recommended to visit the relevant sections in CRAN task review on \ctv{optimisation and mathematical programming} [@crantaskreviewoptim].

# Projection pursuit guided tour {#tour}

The projection pursuit guided tour combines two different methods in exploratory data analysis, focusing on different aspects. Projection pursuit, coined by @friedman1974projection, detects interesting structures (e.g. clustering, outliers and skewness) in multivariate data via low dimensions projection. The guided tour is using ideas from projection pursuit to define a particular variation in a broader class of data visualisation methods, building on the grand tour approach [@As85].

To define projection pursuit, we first need to establish the notation used. Let $\mathbf{X}_{n \times p}$ be the data matrix, with $n$ observations in $p$ dimensions. A d-dimensional projection can be seen as a linear transformation from $\mathbb{R}^p$ into $\mathbb{R}^d$, and defined as $\mathbf{Y} = \mathbf{X} \cdot \mathbf{A}$, where $\mathbf{Y}_{n \times d}$ is the projected data and $\mathbf{A}_{p\times d}$ is the projection matrix. Define $f: \mathbb{R}^{n \times d} \mapsto \mathbb{R}$ to be an index function that maps the projected data $\mathbf{Y}$ (corresponding to an associated projection matrix $\mathbf{A}$) onto an index value $I$ (QUESTION: Isn't f the index function?). This is commonly known as the projection pursuit index function, or just index function, and is used to measure the "interestingness" of a given projection.

A number of index functions have been proposed in the literature to detect different data structures, including Legendre index [@friedman1974projection], Hermite index [@hall1989polynomial], natural Hermite index [@cook1993projection], chi-square index [@posse1995projection], LDA index [@lee2005projection] for supervised classification problems, and PDA index [@lee2010projection], which is an extension of the LDA index. 

As a general visualisation method, a tour produces animations of high dimensional data via rotations between low dimension planes. Different tour types choose these planes differently, for example, a grand tour [@cook2008grand] selects the planes randomly to provide a general overview and a manual tour [@cook1997manual] gradually phases in and out one variable, to understand the contribution of that variable in the projection. Guided tour, the main interest of this paper, chooses planes with the aid of projection pursuit to gradually reveal the most interesting projection in the low dimension space. Given a random start, projection pursuit iteratively finds bases with higher index values and the guided tour constructs the geodesic interpolation between these planes to form a tour path. Intuitively, Figure \ref{fig:tour-path} shows a sketch of the tour path where the blue frames are produced by the projection pursuit optimisation algorithm, and the white frames interpolate between them. Mathematical details of the geodesic interpolation can be found in @buja2005computational.  The tour method has been implemented in the R package \CRANpkg{tourr} [@tourr].

```{r tour-path, out.height="25%",out.width="50%",fig.cap="Each square (frame) represents the projected data with a corresponding basis. Blue frames are found by an optimisation algorithm iteratively whilst the white frames are constructed between two blue frames by geodesic interpolation."}
include_graphics(here::here("img/tour_path.png"))
```

## Optimisation in the tour {#tour-optim}

The optimisation problem in the tour context is stated as follows: Given a randomly generated starting basis $\mathbf{A}_1$, projection pursuit finds the final projection basis $\mathbf{A}_T$ that satisfies the following optimisation problem: 


\begin{align}
&\arg \max_{\mathbf{A} \in \mathcal{A}} f(\mathbf{X} \cdot \mathbf{A}) \\
&s.t.  \mathbf{A}^{\prime} \mathbf{A} = I_d
\end{align}

\noindent where $I_d$ is the $d$-dimensional identity matrix and the constraint requires the projection bases $\mathbf{A}$ to be orthogonal matrices.

Several features of this optimisation are worth noticing. First of all, this is a constrained optimisation problem as the decision variables form the entries of a projection basis, which is required to be orthonormal. It is also likely that the objective function may not be differentiable for a constructed index function and in these cases, gradient-based methods may not work well. Although finding the global maximum is the goal of an optimisation problem, it is also interesting to inspect local maximum in projection pursuit since it could present unexpected interesting projections. Lastly, there is also one computational consideration: the optimisation procedure needs to be fast to compute since the tour animation is played in real-time. 

## Existing algorithms

Below we introduce three line search algorithms in tour: simulated annealing (SA), simulated annealing with jump out (SAJO), and pseudo derivative (PD).

\begin{algorithm}
\SetAlgoLined
  \SetKwInOut{input}{input}
  \SetKwInOut{output}{output}
    \input{$f(.)$, $\alpha_1$, $l_{\max}$, $\text{cooling}$} 
    \output{$\mathbf{A}_{l}$}
    generate random start $\mathbf{A}_1$ and set $\mathbf{A}_{\text{cur}} \coloneqq \mathbf{A}_1$, $I_{\text{cur}} = f(\mathbf{A}_{\text{cur}})$, $j = 1$\;
  \Repeat{$\mathbf{A}_l$ is too close to $\mathbf{A}_{\text{cur}}$ in terms of geodesic distance}{
   set $l = 1$\;
  \Repeat{$l > l_{\max}$ or $I_{l} > I_{\text{cur}}$}{
    generate $\mathbf{A}_{l} = (1- \alpha_j)\mathbf{A}_{\text{cur}} + \alpha_j \mathbf{A}_{\text{rand}}$ and orthogonalise $\mathbf{A}_{l}$\;
    compute $I_{l}  = f(\mathbf{A}_{l})$\;
    update $l = l + 1$\;
  }
  update $\alpha_{j+1} = \alpha_j * \text{cooling}$\;
  construct the geodesic interpolation between $\mathbf{A}_{\text{cur}}$ and $\mathbf{A}_l$\; 
  update $\mathbf{A}_{\text{cur}} = \mathbf{A}_l$ and $j = j + 1$\;
}
  \caption{Simulated annealing (SA)}
  \label{random-search}
\end{algorithm}

Simulated annealing (SA) is a random search device that samples a candidate basis $\mathbf{A}_{l}$ in the neighbourhood of the current basis $\mathbf{A}_{\text{cur}}$ by $\mathbf{A}_{l} = (1- \alpha)\mathbf{A}_{\text{cur}} + \alpha \mathbf{A}_{\text{rand}}$ where $\alpha$ controls the radius of the sampling neighbourhood and $\mathbf{A}_{\text{rand}}$ is a randomly generated matrix with the same dimension as $\mathbf{A}_{\text{cur}}$. $\mathbf{A}_{l}$ is then orthogonalised to ensure the orthonormal constraint is fulfilled. When a basis is found with index value higher than the current basis $\mathbf{A}_{\text{cur}}$, the search terminates and outputs the basis for guided tour to construct an interpolation path. The next iteration of search begins after adjusting $\alpha$ by a cooling parameter: $\alpha_{j+1} = \alpha_j * \text{cooling}$. The termination condition is when the maximum number of iteration $l_{\max}$ is reached. The algorithm of simulated annealing is summarised in Algorithm \ref{random-search}.  A slightly different cooling scheme has been proposed by @posse1995projection to avoid the search space being reduced too fast. A halving parameter $c$ is introduced and $\alpha$ is only adjusted if the last search takes more than $c$ times to find an accepted basis.

\begin{algorithm}
\SetAlgoLined
\Repeat{$l > l_{\max}$ or $I_{l} > I_{\text{cur}}$ or $P > U$}{
    generate $\mathbf{A}_{l} = (1- \alpha_j)\mathbf{A}_{\text{cur}} + \alpha_j \mathbf{A}_{\text{rand}}$ and orthogonalise $\mathbf{A}_{l}$\;
    compute $I_{l}  = f(\mathbf{A}_{l})$, $T(l) = \frac{T_0}{\log(l + 1)}$ and $P= \min\left\{\exp\left[-\frac{I_{\text{cur}} -I_{l}}{T(l)}\right],1\right\}$\;
    draw $U$ from a uniform distribution: $U \sim \text{Unif(0, 1)}$\;
    update $l = l + 1$\;
  }
  \caption{Simulated annealing with jump out (SAJO)}
  \label{simulated-annealing}
\end{algorithm}

Simulated annealing with jump out (SAJO) [@kirkpatrick1983optimization; @bertsimas1993simulated] uses the same sampling process but allows a probabilistic acceptance of a inferior basis with lower index value based on the annealing $T(l)$. Given an initial $T_0$, the temperature at iteration $l$ is defined as $T(l) = \frac{T_0}{\log(l + 1)}$. When a candidate basis fails to have an index value larger than the current basis, simulated annealing gives it a second chance to be accepted with probability $$P= \min\left\{\exp\left[-\frac{\mid I_{\text{cur}} - I_{l} \mid}{T(l)}\right],1\right\}$$ where $I_{(\cdot)}$ denotes the index value of a given basis. This implementation allows the algorithm to jump out of a local maximum and enables a more holistic search of the whole parameter space. This feature is particularly useful when local maxima are present. The algorithm \ref{simulated-annealing} highlights how SAJO differs from SA in the inner loop.



\begin{algorithm}
\SetAlgoLined
\Repeat{$l > l_{\max}$ or $p_{\text{diff}} > 0.001$}{
  generate $n$ random directions $\mathbf{A}_{\text{rand}}$ \;
  compute $2n$ candidate bases deviate from $\mathbf{A}_{\text{cur}}$ by an angle of $\delta$ while ensure orthogonality\;
  compute the corresponding index value for each candidate bases\;
  determine the search direction as from $\mathbf{A}_{\text{cur}}$ to the candidate bases with the largest index value\;
  determine the step size via optimising the index value on the search direction over a 90 degree window\;
  find the optima $\mathbf{A}_{**}$ and compute $I_{**} = f(\mathbf{A}_{**})$, $p_{\text{diff}} = (I_{**} - I_{\text{cur}})/I_{**}$\;
  update $l = l + 1$\;
}
\caption{Pseudo derivative (PD)}
\label{search-geodesic}
\end{algorithm}


In pseudo derivative search [@cook1995grand], the search direction is computed using the most prominent direction that deviating an tiny angle of $\delta$ from the current basis. The step size is chosen by optimising the index value along the geodesic direction over an 90 degree angle from $-\pi/4$ to $\pi/4$ along the search direction chosen. The optima $\mathbf{A}_{**}$ is returned for the current iteration if it meets the percentage improve condition or when $l_{\max}$ is reached.  Algorithm \ref{search-geodesic} summarises the inner loop of the pseudo derivative search.




# Visual diagnostics {#vis-diag}

To be able to make diagnostics on the optimisers, the algorithms need to populate a data structure with key elements of the algorithm. When the algorithms run, key information regarding the decision variable, objective function and hyper-parameters needs to be recorded and stored as a data object so that it is ready to be supplied to the plotting functions for diagnostics. 

## Data structure for diagnostics 

Three main elements are recorded for the projection pursuit optimisers: 1) projection bases: $\mathbf{A}$, 2) index values: $I$, and 3) State: $S$, which labels the observation with detailed stage in the optimisation. For optimiser SA and SAJO, possible values of the state include `random_search`,  `new_basis`, and `interpolation`. pseudo derivative search has a wider variety of state including `new_basis`, `direction_search`, `best_direction_search`, `best_line_search`, and `interpolation`.

Multiple iterators are also  needed to index the data collected at different levels: $t$ being a unique identifier that prescribes the natural ordering of each observation while $j$ and $l$ being the counter of the outer and inner loop, respectively, in Algorithm \ref{random-search}, \ref{simulated-annealing} and \ref{search-geodesic} above. Other parameters of interest recorded include $V_1 = \text{method}$ that tags the name of the optimiser, and $V_2 = \text{alpha}$ that indicates the sampling neighbourhood size.  A matrix notation of the data structure is presented in Equation \ref{eq:data-structure}.

\begin{equation}
\renewcommand\arraystretch{2}  % default value: 1.0
\left[
\begin{array}{c|ccc|cc|cc}
t & \mathbf{A} & I & S & j &  l  & V_{1} & V_{2}\\
\hline
1 & \mathbf{A}_1 & I_1 & S_1 & 1 & 1 & V_{11} & V_{12}\\
\hline
2 & \mathbf{A}_2 & I_2 & S_2 & 2 & 1  & V_{21}  & V_{22}\\
3 & \mathbf{A}_3 & I_3 & S_3 & 2 & 2  & V_{31}  & V_{32}\\
\vdots & \vdots &\vdots &\vdots  &\vdots & \vdots &\vdots  &\vdots\\
\vdots & \vdots & \vdots &\vdots & 2 & l_2 & \vdots  & \vdots\\
\hline
\vdots &\vdots & \vdots &\vdots & 2  & 1& \vdots & \vdots\\
\vdots &\vdots &\vdots &\vdots & 2 & 2& \vdots &  \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots & \vdots  &\vdots \\
\vdots &\vdots &\vdots &\vdots & 2 & k_2 &\vdots  & \vdots\\
\hline
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
\hline
\vdots & \vdots & \vdots &\vdots  & J &  1 & \vdots & \vdots \\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
T & \mathbf{A}_T & I_T &S_T  & J &  l_{J} & V_{T1}& V_{T2}\\
\hline
\vdots &\vdots & \vdots &\vdots & J  & 1& \vdots & \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots & \vdots  &\vdots \\
\vdots &\vdots &\vdots &\vdots & J & k_J &\vdots  & \vdots\\
\hline
\vdots& \vdots & \vdots & \vdots & J+1 & 1 & \vdots& \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
T^\prime & \mathbf{A}_{T^\prime} & I_{T^\prime} &S_{T^\prime}  & J+1 &  l_{J+1} & V_{T^\prime 1}& V_{T^\prime 2}\\
\end{array}
\right]
= 
\left[
\begin{array}{c}
\text{column name} \\
\hline
\text{search (start basis)} \\
\hline
\text{search} \\
\text{search} \\
\vdots \\
\text{search (accepted basis)} \\
\hline
\text{interpolate} \\
\text{interpolate} \\
\vdots \\
\text{interpolate} \\
\hline
\vdots \\
\hline
\text{search} \\
\vdots \\
\text{search (final basis)} \\
\hline
\text{interpolate} \\
\vdots \\
\text{interpolate} \\
\hline
\text{search (no output)} \\
\vdots \\
\text{search (no output)} \\
\end{array}
\right]
\label{eq:data-structure}
\end{equation}

\noindent where $T^{\prime} = T + k_{J}+ l_{J+1}$. Note that there is no output in iteration $J + 1$ since the optimiser cannot find a better basis in the last iteration and the algorithm terminates. The final basis found is $A_T$ with the highest index value $I_T$.

The data structure constructed above meets the tidy data principle [@wickham2014tidy] that requires each observation to form a row and each variable to form a column. With tidy data structure, data wrangling and visualisation can be significantly simplified by well-developed packages such as \CRANpkg{dplyr} [@dplyr] and \CRANpkg{ggplot2} [@ggplot2].

The construction of diagnostic plots adopts the core concept in ggplot2:  grammar of graphics [@wickham2010layered]. In grammar of graphics, plots are not produced via calling the commands, named by the appearance of the plot, i.e., boxplot and histogram, but via the concept of stacked layers. Seeing plots as stacked layers empowers us to composite diagnostic plots with an emphasis on any variable in the data object without the redundancy of creating different commands for the same type of plots that highlights on different variables.

## Checking how hard the optimiser is working

A starting point of diagnosing an optimiser is to understand how many search the optimiser has conducted. One may want to simply plot the index value of the search points across its natural order. A point geometry may work well if each iteration has a similar number of points,  however, the plot will be disproportionate if some iterations have considerably more points than other - it over-emphasizes the iterations that have more search points! An alternative is to summarise the search in each iteration using boxplot where each iteration will be spaced out equally. Occasionally, one may still want to switch back to point geometry if the number of points is small and this can be achieved via the `cutoff` argument. Additional annotation can be added with new layers, for example, the number of points searched in each iteration can be added as text label at the bottom of each iteration; the anchor bases to interpolate are connected and highlighted in a larger size; and the colour of the last iteration is in a grey scale to indicate no better basis found in this iteration. 

Figure \ref{fig:toy-search} shows the searches of two different optimisers: SA (left) and SAJO (right). Both optimisers quickly find better bases at the first few iterations,  then take longer in the later iterations, and finally finish when the maximum number of evaluation is reached. The anchor bases, the ones with the highest index value in each iteration, always have an increased index value in the optimiser SA while this is not the case for SAJO. This feature gives SA an advantage in simple example, like the one presented while the jump out component in SAJO allows for a more holistic search in more complicated examples.

```{r toy-search, fig.cap="A comparison of search by two otimisers: SA (left) and SAJO (right) on a 2D projection of a six-variable dataset, \\code{boa6}, using the holes index. Both optimisers find the final basis with similar index value while it takes SAJO more iteration because the algorithm allows a probabilistic acceptance of bases with lower index value, as observed in iteration 4 and 6-8."}
```

## Examining the optimisation progress {#toy-interp}

```{r toy-interp, fig.cap = "Trace plots of the interpolating bases with the same optimisation routine as the previous figure. Both traces are smooth while the change of index value in each iteration may not be monotinic."}
```

Another interest of diagnosing is to examine how the index value progresses for the points on the interpolation path since the projection on these bases are played by the tour animation. Trace plots are created with plotting the index value of the interpolation points against time. Figure \ref{fig:toy-interp} presents the trace plot of the two optimisation routines as \ref{fig:toy-search}. The trace plot is smooth in both plots and this is a character of the holes index function used. An interesting discovery is that with optimiser SAJO, the interpolator, in iteration 6-8, first passes through some bases with higher index value than the anchor bases.

## Understanding the optimiser's coverage of the search space {#toy-pca}

```{r toy-pca, fig.height=5, fig.cap = "The PCA plot of 1D projection problem on the 5-variable dataset, \\code{boa5}, using the holes indexwith two optimisers: SA and PD. All the bases in PD has been flipped and a grey dashed line has been annotated to indicate the symmetry of two starting bases."}
```

Apart from checking the progression of an optimiser, looking at how the points evaluated by the optimisers look like in the space is another interest. Given the orthonormality constraint, the space of projection bases $\mathbf{A}_{p \times d}$ is a $p \times d$ dimension sphere and  dimension reduction methods, i.e. principal component analysis is applied first to project all the bases onto a 2D space. In a projection pursuit guided tour optimisation, there are various type of bases involved: 1) The start basis; 2) The anchor bases that have the highest index value in each iteration; 3) The search bases that an optimiser evaluated to produce the anchor bases; 4) The interpolating bases on the interpolation path; and finally 5) the end basis. The importance of these basis differs with the most important ones being the start, interpolating and end bases. Anchor and search bases can be turned on with argument `details = TRUE`. Sometimes, two optimisers can start with the same basis but finish with bases of opposite sign. This happens because the projection is invariant to the sign difference of the bases and so does the index value, however, this creates difficulties for comparing end bases. A flip sign device is implemented to flip the sign of all the bases in one optimisation if the situation described above happens and to ensure different routines finish close to each other.

Several annotations have been made to help understanding this plot. The original space of the bases is a high dimensional sphere and random bases on the sphere can be generated via the CRAN package \CRANpkg{geozoo} [@geozoo]. Along with the bases recorded during the optimisation and a zero basis, PCA is performed to get the first two PC coordinates of all the bases. The search space in the 2D space is a circle with the origin being the PC coordinates of the zero matrix, and radius estimated as the largest distance between the origin and all the bases. The theoretical best basis is known with simulated data and can be labelled for comparing the how close the ending basis to the theoretical one for different optimisers. Various aesthetics, i.e. size, alpha and colour, are applicable to emphasize the crucial elements and adjust for the presentation. For example, anchor points and search points are less important and a smaller size and alpha is used; The interpolation paths are more informative if alpha increases from start to finish (that is, the path is more opaque towards the end).

Figure \ref{fig:toy-pca} presents the PCA plot of a 1D projection of a 5-variable dataset, \code{boa5}, using the holes index with two optimisers, PD and SA, being compared. Both optimisers get close to the theoretical bases, annotated by "*" and in this particular example pseudo derivative search gets closer. One can also appreciate the nature of the two different searches: PD first evaluates points in a small neighbourhood for a promising direction, while SA evaluates points randomly in the search space to search for the next target. There are dashed lines annotated for SA and it describes the interruption of the interpolation, which has been briefly mentioned in Figure \ref{fig:toy-interp} and will be discussed in details in Section \ref{monotonic}.

## Animating the diagnostic plots

```{r toy-pca-animated, fig.cap = "Six selected frames from the animated PCA plots in the previous figure. With animation, the start and end position of each optimisation is easier to identify.  A full video of this aimation can be found at \\url{https://vimeo.com/user132007430/review/504242845/b73f37175a}"}
```

Animation is another display to show how the search progresses from start to finish in the space and an  `animate = TRUE` argument is used to enable the animation. Figure \ref{fig:toy-pca-animated} shows six frames from the animation of the PCA plot in Figure \ref{fig:toy-pca}. An additional piece of information one can learn from this animation is that the SA search finds its end basis quicker than the PD search since SA has finished in the 5th frame while PD is still making more progression.

## The tour looking at itself

```{r toy-tour, fig.cap="Six selected frames from the tour animation for viewing the same two optmisations as previous two figures. The tour animation allows for an appreciation of the search bases in the original high dimensional space. A full video of this aimation can be found at \\url{https://vimeo.com/user132007430/review/504328122/9be84db563}"}
```

As mentioned in Section \ref{toy-pca}, the original $p \times d$ dimension space can be simulated via randomly generated bases via \CRANpkg{geozoo} [@geozoo] package. A tour plot rotates these random bases and the ones collected by the optimisation in the original space and hence provides a stereoscopic view of the search. While the PCA plot projects the bases from the direction that maximises the variance, the tour plot display of the original high dimensional space from various directions using animation. Figure \ref{fig:toy-tour} shows the tour plot of the same two optimisations in the 5D space. 

## Forming a torus


Something to fill here ...

# Diagnosing an optimiser {#application}

In this section, several examples will be presented to show how the diagnostic plots discover something unexpected in the optimisation algorithm, and guide the implementation of some new features.

## Simulation setup

Random variables with different distributions have been simulated and the distributional form of each variable is presented in Equations \ref{eq:sim-norm} to \ref{eq:sim-x7}. Variable `x1`, `x8`, `x9` and `x10` are normal noise with zero mean and unit variance, and `x2` to `x7` are normal mixtures with varied weights and locations. All the variables have been scaled to have an overall unit variance before projection pursuit.

\begin{align}
x_1 \overset{d}{=} x_8 \overset{d}{=} x_9 \overset{d}{=} x_{10}& \sim \mathcal{N}(0, 1) \label{eq:sim-norm} \\
x_2 &\sim 0.5 \mathcal{N}(-3, 1) + 0.5 \mathcal{N}(3, 1)\label{eq:sim-x2}\\
\Pr(x_3) &= 
\begin{cases}
0.5 & \text{if $x_3 = -1$ or $1$}\\
0 & \text{otherwise}
\end{cases}\label{eq:sim-x3}\\
x_4 &\sim 0.25 \mathcal{N}(-3, 1) + 0.75 \mathcal{N}(3, 1) \label{eq:sim-x4}\\
x_5 &\sim \frac{1}{3} \mathcal{N}(-5, 1) + \frac{1}{3} \mathcal{N}(0, 1) + \frac{1}{3} \mathcal{N}(5, 1)\label{eq:sim-x5}\\
x_6 &\sim 0.45 \mathcal{N}(-5, 1) + 0.1 \mathcal{N}(0, 1) + 0.45 \mathcal{N}(5, 1)\label{eq:sim-x6}\\
x_7 &\sim 0.5 \mathcal{N}(-5, 1) + 0.5 \mathcal{N}(5, 1) 
\label{eq:sim-x7}
\end{align}

## A problem of non-monotonicity {#monotonic}

An example of non-monotonic interpolation has been given in Figure \ref{fig:toy-interp}: a path that passes bases with higher index value than the target one. With optimiser SAJO, a non-monotonic interpolation is justified since the target bases do not necessarily have a higher index value than the current one, while this is not the case for SA. The original trace plot for a 2D projection, optimised by SA, is shown on the left panel of Figure \ref{fig:interruption} and one can observe clearly that the non-monotonic interpolation has undermined the optimiser to realise its full potential. Hence, an interruption is implemented to stop the interpolation at the basis with the highest index value. The right panel of Figure \ref{fig:interruption} presents the trace plot after implementing such interruption and it can be observed that while the first two interpolations are identical, in the third interpolation the basis at time 61 has a higher index value than the target. Rather than starting the next iteration from the target basis on Time 65 as shown in the left panel, the optimiser, on the right panel, starts the next iteration from the interpolation basis on Time 61. With the interruption, the optimiser is able to find the final basis with a index value higher than the most promising basis when the interruption is not implemented.

```{r interruption, fig.cap = "Trace plot of 2D projection on \\code{boa6} data with holes index, optimised by optimiser SA."}
```

## Close but not close enough


```{r polish, fig.height= 5, fig.cap = "Trace plot of 2D projection on \\code{boa6} data with holes index, optimised first by SA and then by polish search. The polish step results in clearer cut on the edges of the four clusters in the data structure."}
```

Once the final basis has been found by an optimiser, one may want to push further in the close neighbourhood to see if an even better basis can be found. A polish search takes the final basis of an optimiser as the start of a new guided tour to search for local breakthrough. The polish algorithm is similar to the SA with three main distinctions: 1) 100 rather than one candidate bases are generated each time in the inner loop; 2) the search neighbourhood, `alpha`, is reduced in the inner loop, rather than in the outer loop, once a set of candidate bases fails to make an improvement; and 3) three more termination conditions have been added to ensure the new basis generated is distinguishable from the current in terms of the distance in the space,  percentage change in the index value, and neighbourhood: 

1) the distance between the basis found and the current basis needs to be larger than 1e-3;
2) the percentage change of the index value need to be larger than 1e-5; and
3) the alpha parameter on itself needs to be larger than 0.01

Figure \ref{fig:polish} presents the trace plot of a 2D projection, optimised by SA and followed by the polish search, along with the projection on the final basis of each algorithm. The end basis found by SA reveals the four clusters in the data, but the edges of each cluster are not clean-cut. Polish works with this end basis and further pushes the index value to produce even clear structure in the data. One can observe that the projection after polishing better distinguishes the four clusters with more trimmed edges, espeically along the vertical axis.


## Reconciling the orientation

Something to fill here ...

```{r flip-sign, fig.cap = "PCA plot of 1D projection of boa6 data, optimised by SA, with seed 2463."}

```


## Seeing the signal in the noise 

The index function, up until this point, are all smooth, while this is not the case for all. A 1D projection function based on the Kolmogorov test, `norm_kol`, compares the projected data, $\mathbf{Y}_{n \times 1}$ to a randomly generated normal distribution, $y_n$ based on the empirical cumulated distribution function (ECDF). Let $F_{.}(u)$ be the ECDF function with the subscript $P$ and $u$ indicating the projected and randomly generated ECDF respectively,  the index is defined as: 

$$\max \left[F_{P}(u) - F_{y}(u)\right]$$
With a more complex index function, it is interesting to understand 1) if all the three optimisers can reach the optimum, 2) when a local optimum is presented, whether the simulated annealing algorithm can escape that local optimum and find the global one.


```{r noisy-better-geo, fig.cap = "1D projection on 5\\-variable dataset \\code{boa5} with \\code{norm\\_kol} index. The trace plot shows the failure of pseudo derivative on non-smooth index. The PCA plot also shows that simulated annealing is searching the area close to the theoretical best while pseudo derivative can't couple with the non-continuity in the index."}
```

Figure \ref{fig:noisy-better-geo} presents the tracing plots of two optimisers: pseudo derivative and simulated annealing and as expected, the interpolated path is no longer smooth in either case. There is barely any improvement on the index value in the pseudo derivative algorithm while the simulated annealing algorithm has managed to get close to the index value of the theoretical best, indicated by the horizontal dashed line. In the PCA plot, simulated annealing is able to move gradually towards the theoretical best while the pseudo derivative method breaks after the initial few attempts.

```{r kol-result, fig.height = 5, fig.cap="1D projection of data \\code{boa6} on \\code{norm\\_kol} index,  optimised by SA and SAJO. Two different neighbourhood size of 0.5 and 0.7 is used. The finish point of each path is larger than the start point and optimums are separately labelled with * representing the global optimum and x the local optimum The colour of the path is based on whether the optimiser finds the global or local optimum, after polishing."}
```

The next experiment compares the two simulated annealing optimisers, SA and SAJO,  for 20 simulation paths. Two search neighbourhood sizes, `alpha`, of 0.5 and 0.7 are also compared to understand the effect of neighbourhood size where a larger value indicating a wider search. Figure \ref{fig:kol-result} shows 80 (20 * 4) PCA projected interpolation paths faceted by optimiser and alpha. Several results can be drawn through the comparison: 1) With optimiser SA and a 0.5 neighbourhood search size, despite being the simplest and fastest, the optimiser fails to optimise for three instances where they finish neither near the local nor the global optimum. 2) With the same optimiser but a larger neighbourhood size of 0.7, more seeds have found the global optimum. 3) Comparing between the optimiser SA and SAJO with the same search size of 0.5, SAJO finishes closer to the theoretical for the ones that find the local optimum. This indicates that it is working harder to examine if there is other optimum in the space but a small search neighbourhood size makes it hard to conduct a holistic search of the whole space. 4) With optimiser SAJO and a larger neighbourhood size of 0.7, more seeds have found the global optimum. Also some finish points are getting closer to the position of the theoretical best, as comparing with the 0.5 neighbourhood size, and this would indicate fewer work for the subsequent polish.

# Implementation {#implementation}

The implementation of this projection has been divided into two packages: the data collection object is implemented in the existing CRAN package \CRANpkg{tourr} [@tourr] while the optimiser diagnostics have been implemented in a new package, \pkg{ferrn}. When a guided tour is run, the users can choose if the data from optimisation should be collected via the `verbose` argument. Once the data object has been obtained, the package, \pkg{ferrn}, can provide four diagnostic plots as shown in Section \ref{vis-diag}. The structure of package functionality has been listed below.

- Main plotting functions: 

  - \code{explore\_trace\_search()} produces summary plots, as shown in Figure \ref{fig:toy-search}
  - \code{explore\_trace\_interp()}produces trace plots for the interpolation points, as shown in Figure \ref{fig:toy-interp}
  - \code{explore\_space\_pca()} produces plots of projection basis on the reduced space by PCA, as shown in Figure \ref{fig:toy-pca}. Animated version in Figure \ref{fig:toy-pca-animated} can be turned on via the argument `animate = TRUE`
  - \code{explore\_space\_tour()} produces animated tour view on the full space of the projection bases, as shown in Figure \ref{fig:toy-tour}.

- \code{get\_*()} extracts and manipulates certain components from the existing data object.
  - \code{get\_best()} extracts the best basis found in the data object
  - \code{get\_start()} extracts the starting basis
  - \code{get\_interp()} extracts the observations in the interpolation
  - \code{get\_interp\_last()} extracts the end observations of the interpolation in each iteration
  - \code{get\_anchor()} extracts the target observations found by the optimiser
  - \code{get\_search()} extracts the search observations evaluated by the optimiser
  - \code{get\_search\_count()} produces the summary table of the number of observation in each iteration
  - \code{get\_centre()} produces the centre point of the basis space estimated by the starting points
  - \code{get\_space\_param()} produces the coordinates of the centre and radius of the basis space
  - \code{get\_theo()} extracts the theoretical observations from the data object
  - \code{get\_interrupt()} extract the end point of the interpolation and the target point when an interruption happens
  - \code{get\_basis\_matrix()}: flattens all the bases into a matrix

- \code{bind\_*()} incorporates additional information outside the tour optimisation into the data object.
  - \code{bind\_theoretical()} incorporates the best possible basis to the existing data object with the supply of the index function and original data for producing the index value. 
  - \code{bind\_random()} generates 1000 points on the high dimensional surface of a sphere and binds it to the existing data object and output as a tibble object. 
  - \code{bind\_random\_matrix()} binds the points to the basis matrix. 

- Utilities
  - \code{add\_*()} are internal wrapper functions that facilitate the composition of PCA plot
  - \code{theme\_fern()} and \code{format\_label()} for better display of the grid lines and axis formatting
  - \code{clean\_method()} for clean up the name of the optimisers
  - \code{botanical\_palettes} is a collection of colour palettes from Australian native plants. Quantitative palettes include daisy, banksia and cherry and sequential palettes contain fern and acacia.
  - \code{botanical\_pal()} as the colour interpolator 

# Conclusion

This paper has illustrated setting up a data object that can be used for diagnosing a complex optimisation procedure. The ideas were illustrated using the optimisers available for projection pursuit guided tour. Here the constraint is the orthonormality condition of the projection bases. The approach used here could be broadly applied to understand other constrained optimisers.

<!-- something about the diagnostic plots  -->
Four diagnostic plots have been introduced to investigate the progression and the projection space of an optimiser. The implementation of these visualisations is designed to be easy-to-use with each plot can be produced with a simple supply of the data object. More advanced users may decide to modify on top of the basic plots or even build their own. 

<!-- summarise what we have done and what might be done in the future. -->
Most of the work in this project has been translated into code in two packages: the collection of the data object is implemented in the existing \CRANpkg{tourr}[@tourr] package; manipulation and visualisation of the data object are implemented in the new \pkg{ferrn} package. Equipped with handy tools to diagnose the performance of optimisers, future work can extend the diagnostics to a wider range of index functions, i.e. scagnostics, association, and information index [@laa2020using] and understand how the optimisers behave for index functions with different structures.

# Acknowledgements

This article is created using \CRANpkg{knitr}[@knitr] and \CRANpkg{rmarkdown} [@rmarkdown]  in R. The source code for reproducing this paper can be found at: \url{https://github.com/huizezhang-sherry/paper-tour-vis}.

\bibliography{zhang-cook-laa-langrene-menendez}
