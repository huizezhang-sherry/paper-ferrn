---
title: "Visual Diagnostics for Constrained Optimisation with Application to Guided Tours"
author:
  - name: H.Sherry Zhang
    affiliation: Monash University 
    address: Department of Econometrics and Business Statistics
    email:  huize.zhang@monash.edu
  - name: Dianne Cook
    affiliation: Monash University 
    address: Department of Econometrics and Business Statistics
    email:  dicook@monash.edu
  - name: Ursula Laa
    affiliation: University of Natural Resources and Life Sciences
    address: Institute of Statistics
    email:  ursula.laa@boku.ac.at  
  - name: Nicolas Langrené
    affiliation: CSIRO Data61 
    address: 34 Village Street, Docklands VIC 3008 Australia
    email: nicolas.langrene@csiro.au
  - name: Patricia Menéndez
    affiliation: Monash University 
    address: Department of Econometrics and Business Statistics
    email:  patricia.menendez@monash.edu 
abstract: |
  Guided tour searches for interesting low-dimensional views of high-dimensional data via optimising a projection pursuit index function. The first paper of projection pursuit by @friedman1974projection stated that "the technique used for maximising the projection index strongly influences both the statistical and the computational aspects of the procedure." While much work has been done in proposing  indices in the literature, less has been done on evaluating the performance of the optimisers. In this paper, we implement a data collection object in the optimisation of projection pursuit guided tour and introduce visual diagnostics based on the data object collected. These diagnostics and this workflow can be applied to a broad class of optimisers, to assess their performance. An R package, \pkg{ferrn}, has been created to implement the diagnostics. 
  
  \noindent \textit{keywords}: graphics, multivariate, optimization
preamble: >
  \usepackage{amssymb, amsmath, mathtools, dsfont, bbm, array, booktabs}
  \usepackage[ruled,vlined, linesnumbered]{algorithm2e}
output: rticles::rjournal_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE,
                      root.dir = here::here(), 
                      fig.path = here::here("figs/"), 
                      cache = TRUE,
                      fig.align = "center",
                      fig.height = 3,
                      out.width = "100%")
```

```{r external, include = FALSE, cache = FALSE}
library(knitr)
library(tidyverse)
library(ferrn)

knitr::read_chunk(
  here::here("scripts/toy.R")
)

knitr::read_chunk(
  here::here("scripts/diagnose.R")
)

knitr::read_chunk(
  here::here("scripts/implementation.R")
)

knitr::read_chunk(
  here::here("scripts/kol_result.R")
)

```

```{r load-pkg}
```

```{r }
#spelling::spell_check_files(here::here("paper/zhang-cook-laa-langrene-menendez.Rmd"), lang = "en-GB", ignore = spelling::get_wordlist())
```


# Introduction

Visualisation is widely used in exploratory data analysis [@tukey1977exploratory; @unwin2015graphical;  @healy2018data; @wilke2019fundamentals]. Presenting information in graphics often unveils information that would otherwise not be discovered and provides a more comprehensive understanding of the problem at hand. Task specific tools such as @li2020visualizing show how visualisation can be used to understand, for instance, the behaviour of neural network on classification models. However, no general visualisation tool is available for diagnosing optimisation procedures. The work presented in this paper brings visualization tools into optimisation problems with the aim to better understand the performance of optimisers in practice.

The goal of continuous optimisation is to find the best solution within the space of all feasible solutions where typically the best solution is decided by an objective function. Broadly speaking, optimization can be unconstrained or constrained [@kelley1999iterative]. The unconstrained problem can be formulated as a minimization (or maximization)  problem such as 
$\min_{x} f(x)$ where $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is an objective function with certain properties defined in an $L^p$ space. In this case, solutions rely on gradient descent or ascent methods. In the constrained optimization problem additional restrictions  are introduced via a set of functions that can be convex or non-convex: $g_i:\mathbb{R}^n \rightarrow \mathbb{R}$ for $i = 1, \ldots k$ and hence the problem can be written as
$\min_{x} f(x)$ *subject to* $g_i(x) \leq 0$. Here methods such as multipliers and convex optimization methods including linear and quadratic programming can be used.

The focus of this paper is on the optimisation problem arising in the projection pursuit guided tour [@buja2005computational], an exploratory data analysis tool used for detecting interesting structures in high-dimensional data through a set of lower-dimensional projections [@cook2008grand]. The goal of the optimisation is to identify the projection, characterised by the projection basis, that gives the most interesting low-dimensional view. The interestingness of the structure is defined by the index function, a function of the projection basis.

The optimization challenges encountered in the projection pursuit guided tour problem are common to those of optimization in general. Examples of those include the existence of multiple optima (local and global), the trade off between computational burden and proximity to the optima,  dealing with noisy objective functions that might be non-smooth and non-differentiable [@jones1998efficient]. Those  are not unique to this context and therefore the visualization tools and optimization methods presented in this paper can be easily applied to any other optimization problems.

The remainder of the paper is organised as follows. 
Section \ref{optim} provides an overview of optimisation methods, specifically line search methods. 
Section \ref{tour} reviews projection pursuit guided tour, defines the optimisation problem and outlines three existing algorithms.
Section \ref{vis-diag} presents the new visual diagnostics. A data structure is defined to capture information during the optimisation, and different diagnostic plots are designed with the data collected.
Section \ref{application} shows applications of how these diagnostic plots can be used to discover interesting aspects of different optimisers and guide improvement to the existing algorithms.
Finally, Section \ref{implementation} describes the R package: \pkg{ferrn}, that implements the visual diagnostics.


# Optimisation methods {#optim}

Optimisation problems are ubiquitous in many areas of study. While in some cases analytical solutions can be found, the majority of problems rely on numerical methods to find the optimal solution. These numerical methods follow iterative approaches that aim at finding the optimum by progressively improving the current solution until a desirable accuracy is achieved. Although this principle seems uncomplicated, a number of challenges arise such as the possible existence of multiple maxima (local and global), constraints and noisy objective function, and the trade-off between desirable accuracy and computational burden. In addition, the optimization results might depend on the algorithm starting values, affecting the consistency of results. 

Optimization methods can be divided into various classes, such as global optimisation [@kelley1999iterative; @fletcher2013practical], convex optimisation [@boyd2004convex] or stochastic optimisation [@nocedal2006numerical]. Our interest is on constrained optimization [@bertsekas2014constrained] as defined in the introduction section, and assuming it is not possible to find a solution to the problem in the way of a closed-form. That is, the problem consists of finding the minimum or maximum of a function $f \in L^p$ in the constrained $\mathbb{A}$ space. 

A large class of methods utilises the gradient information of the objective function to perform the optimisation iterations, with the most notable one being the gradient ascent (descent) method. Although gradient optimization methods are popular, they rely on the availability of the objective function derivatives and on the complexity of the constraints. Derivative-free methods, which do not rely on the knowledge of the gradient, are more generally applicable. Derivative-free methods have been developed over the years, where the emphasis is on finding, in most cases, a near optimal solution. Examples of those include response surface methodology [@box1951experimental], stochastic approximation [@robbins1951stochastic], random search [@fu2015handbook] and heuristic methods [@sorensen2013metaheuristics]. Later, we will present a simulated annealing optimisation algorithm, which belongs to the class of random search methods, for optimisation with the guided tour.

A common search scheme utilised by both derivative-free methods and gradient methods is line search. In line search methods, users are required to provide an initial estimate $x_{1}$ and, at each iteration, a search direction $S_k$ and a step size $\alpha_k$ are generated. Then one moves on to the next point following $x_{k+1} = x_k + \alpha_kS_k$ and the process is repeated until the desire convergence is reached. While gradient-based methods choose the search direction by the gradient, derivative-free methods uses local information of the objective function to determine the search direction. The choice of step size also needs considerations, as inadequate step sizes might prevent the optimisation method to converge to an optimum. An ideal step size can be chosen via finding the value of $\alpha_k \in \mathbb{R}$ that maximises $f(x_k + \alpha_kS_k)$ with respect to $\alpha_k$ at each iteration. 

Several R implementations address optimization problems with both general purpose as well as task specifics solvers. The most prominent one within the general solvers is \code{optim()} in the \CRANpkg{stats} [@stats] package, which provides both gradient-based and derivative-free optimisation functions. Another general solver specialised in non-linear optimisation is \CRANpkg{nloptr} [@nloptr]. Specific solvers for simulated annealing include \code{optim(..., method  "SANN")} and package \CRANpkg{GenSA} [@gensa] that deals with more complicated objective functions. For other task specific solvers, readers are recommended to visit the relevant sections in CRAN task review on \ctv{optimisation and mathematical programming} [@crantaskreviewoptim].

# Projection pursuit guided tour {#tour}

Projection pursuit guided tour combines two different methods (projection pursuit and guided tour) in exploratory data analysis. Projection pursuit, coined by @friedman1974projection, detects interesting structures (e.g. clustering, outliers and skewness) in multivariate data via low-dimensional projections. Guided tour is one variation of a broader class of data visualisation methods, tour, which displays high-dimensional data through a series of animated projections.

Let $\mathbf{X}_{n \times p}$ be the data matrix with $n$ observations in $p$ dimensions. A $d$-dimensional projection is a linear transformation from $\mathbb{R}^p$ into $\mathbb{R}^d$ defined as $\mathbf{Y} = \mathbf{X} \cdot \mathbf{A}$, where $\mathbf{Y}_{n \times d}$ is the projected data and $\mathbf{A}_{p\times d}$ is the projection basis matrix. We define $f: \mathbb{R}^{n \times d} \mapsto \mathbb{R}$ to be an index function that maps the projected data $\mathbf{Y}$ onto a scalar value. This is commonly known as the projection pursuit index function, or just index function, and is used to measure the "interestingness" of a given projection. In projection pursuit, higher index values are associated with interesting projections that depart from normality [@diaconis1984asymptotics; @huber1985projection], e.g. Legendre index [@friedman1974projection], Hermite index [@hall1989polynomial] and natural Hermite index [@cook1993projection] for measuring the distance between the density of projected data and a standard normal distribution, LDA index [@lee2005projection] and PDA index [@lee2010projection] for finding clustering structure in the data, and skewness-based index [@Loperfido2018] and kurtosis-based index [@Loperfido2020] for outlier detection. 

As a general visualisation method, tour produces animations of high dimensional data via rotations of low-dimensional planes. There are different tour implementations depending on how the high dimensional space is investigated: grand tour [@cook2008grand] selects the planes randomly to provide a general overview;  manual tour [@cook1997manual] gradually phases in and out one variable to understand the contribution of that variable in the projection. Guided tour, the main interest of this paper, chooses the planes with the aid of projection pursuit, to gradually reveal the most interesting projection. Given a random start, projection pursuit iteratively finds bases with higher index values and the guided tour constructs a geodesic interpolation between these planes to form a tour path. Figure \ref{fig:tour-path} shows a sketch of the tour path where the blue frames are produced by the projection pursuit optimisation and the white frames are interpolations  between the blue frames. Mathematical details of the geodesic interpolation can be found in @buja2005computational. The aforementioned tour method has been implemented in the R package \CRANpkg{tourr} [@tourr].

```{r tour-path, out.height="20%",out.width="50%", fig.cap="An illustration for demonstrating the frames in a tour path. Each square (frame) represents the projected data with a corresponding basis. Blue frames are returned by the projection pursuit optimisation and white frames are constructed between two blue frames by geodesic interpolation."}
include_graphics(here::here("img/tour-path.png"))
```

## Optimisation in the tour {#tour-optim}

In projection pursuit the optimisation problem aims at finding the global and local maxima that give interesting projections according to an index function. That is, it starts with a given randomly selected basis $\mathbf{A}_1$ and aims at finding an optimal final projection basis $\mathbf{A}_T$ that satisfies the following optimisation problem: 


\begin{align}
\arg \max_{\mathbf{A} \in \mathcal{A}} f(\mathbf{X} \cdot \mathbf{A})  ~~~ s.t. ~~~ \mathbf{A}^{\prime} \mathbf{A} = I_d
\end{align}

\noindent where $f$ and $\mathbf{X}$ are defined as in the previous section, $\mathcal{A}$ is the set of all $p$-dimensional projection bases, $I_d$ is the $d$-dimensional identity matrix, and the constraint ensures the projection bases, $\mathbf{A}$, to be orthonormal. It is worth noticing the following: 1) The optimisation is constrained and the orthonormality constraint imposes a geometrical structure on the bases space: it forms a Stiefel manifold. 2) There may be index functions for which the objective function might not be differentiable. 3) While finding the global optimum is the goal of the optimisation problem, interesting projections may also appear in the local optimum. 4) The optimisation should be computationally agile since the tour animation is viewed by the users during the optimisation.

## Existing algorithms

Three optimisers have been implemented in the \CRANpkg{tourr} [@tourr] package: Creeping random search (CRS), simulated annealing (SA), and pseudo derivative (PD). Creeping random search (CRS) is a random search optimiser that samples a candidate basis $\mathbf{A}_{l}$ in the neighbourhood of the current basis $\mathbf{A}_{\text{cur}}$ by $\mathbf{A}_{l} = (1- \alpha)\mathbf{A}_{\text{cur}} + \alpha \mathbf{A}_{\text{rand}}$ where $\alpha$ controls the radius of the sampling neighbourhood and $\mathbf{A}_{\text{rand}}$ is generated randomly. $\mathbf{A}_{l}$ is then orthonormalised to fulfil the basis constraint. If $\mathbf{A}_{l}$ has an index value higher than the current basis $\mathbf{A}_{\text{cur}}$, the optimiser outputs $\mathbf{A}_{l}$ for guided tour to construct an interpolation path. The neighbourhood parameter $\alpha$ is adjusted by a cooling parameter: $\alpha_{j+1} = \alpha_j * \text{cooling}$ before the next iteration starts. The optimiser terminates when the maximum number of iteration $l_{\max}$ is reached before a better basis can be found. The algorithm of SA is summarised in Algorithm \ref{random-search}. @posse1995projection has proposed a slightly different cooling scheme by introducing a halving parameter $c$. In his proposal, the $\alpha$ is only adjusted if the last iteration takes more than $c$ times to find a better basis.

\begin{algorithm}
\SetAlgoLined
  \SetKwInOut{input}{input}
  \SetKwInOut{output}{output}
    \input{$f(.)$, $\alpha_1$, $l_{\max}$, $\text{cooling}$} 
    \output{$\mathbf{A}_{l}$}
    Generate random start $\mathbf{A}_1$ and set $\mathbf{A}_{\text{cur}} \coloneqq \mathbf{A}_1$, $I_{\text{cur}} = f(\mathbf{A}_{\text{cur}})$, $j = 1$\;
  \Repeat{$\mathbf{A}_l$ is too close to $\mathbf{A}_{\text{cur}}$ in terms of geodesic distance}{
   Set $l = 1$\;
  \Repeat{$l > l_{\max}$ or $I_{l} > I_{\text{cur}}$}{
    Generate $\mathbf{A}_{l} = (1- \alpha_j)\mathbf{A}_{\text{cur}} + \alpha_j \mathbf{A}_{\text{rand}}$ and orthogonalise $\mathbf{A}_{l}$\;
    Compute $I_{l}  = f(\mathbf{A}_{l})$\;
    Update $l = l + 1$\;
  }
  Update $\alpha_{j+1} = \alpha_j * \text{cooling}$\;
  Construct the geodesic interpolation between $\mathbf{A}_{\text{cur}}$ and $\mathbf{A}_l$\; 
  Update $\mathbf{A}_{\text{cur}} = \mathbf{A}_l$ and $j = j + 1$\;
}
  \caption{Creeping random search (CRS)}
  \label{random-search}
\end{algorithm}

Simulated annealing (SA) [@kirkpatrick1983optimization; @bertsimas1993simulated] uses the same sampling process as CRS but allows a probabilistic acceptance of a basis with lower index value than the current. Given an initial value of $T_0$, the temperature at iteration $l$ is defined as $T(l) = \frac{T_0}{\log(l + 1)}$. When a candidate basis fails to have an index value larger than the current basis, SAJO gives it a second chance to be accepted with probability $$P= \min\left\{\exp\left[-\frac{\mid I_{\text{cur}} - I_{l} \mid}{T(l)}\right],1\right\}$$ where $I_{(\cdot)}$ denotes the index value of a given basis. This implementation allows the optimiser to make a move and explore the basis space even the landing basis does have a higher index value and hence enable the optimiser to jump out of a local optimum. The algorithm \ref{simulated-annealing} highlights how SAJO differs from SA in the inner loop.

\begin{algorithm}
\SetAlgoLined
\Repeat{$l > l_{\max}$ or $I_{l} > I_{\text{cur}}$ or $P > U$}{
    Generate $\mathbf{A}_{l} = (1- \alpha_j)\mathbf{A}_{\text{cur}} + \alpha_j \mathbf{A}_{\text{rand}}$ and orthogonalise $\mathbf{A}_{l}$\;
    Compute $I_{l}  = f(\mathbf{A}_{l})$, $T(l) = \frac{T_0}{\log(l + 1)}$ and $P= \min\left\{\exp\left[-\frac{I_{\text{cur}} -I_{l}}{T(l)}\right],1\right\}$\;
    Draw $U$ from a uniform distribution: $U \sim \text{Unif(0, 1)}$\;
    Update $l = l + 1$\;
  }
  \caption{Simulated annealing (SA)}
  \label{simulated-annealing}
\end{algorithm}

Pseudo derivative (PD) search [@cook1995grand] uses a different strategy than SA and SAJO. Rather than randomly sample the basis space, PD first computes a search direction by evaluating vases close to the current basis. A step size is then chosen along the geodesic direction by another optimisation over an 90 degree angle from $-\pi/4$ to $\pi/4$. The resulting candidate basis $\mathbf{A}_{**}$ is returned for the current iteration if it has a higher index value than the current one. Algorithm \ref{search-geodesic} summarises the inner loop of the PD.

\begin{algorithm}
\SetAlgoLined
\Repeat{$l > l_{\max}$ or $p_{\text{diff}} > 0.001$}{
  Generate $n$ random directions $\mathbf{A}_{\text{rand}}$ \;
  Compute $2n$ candidate bases deviate from $\mathbf{A}_{\text{cur}}$ by an angle of $\delta$ while ensuring orthogonality\;
  Compute the corresponding index value for each candidate bases\;
  Determine the search direction as from $\mathbf{A}_{\text{cur}}$ to the candidate bases with the largest index value\;
  Determine the step size via optimising the index value on the search direction over a 90 degree window\;
  Find the optimum $\mathbf{A}_{**}$ and compute $I_{**} = f(\mathbf{A}_{**})$, $p_{\text{diff}} = (I_{**} - I_{\text{cur}})/I_{**}$\;
  Update $l = l + 1$\;
}
\caption{Pseudo derivative (PD)}
\label{search-geodesic}
\end{algorithm}

# Visual diagnostics {#vis-diag}

A data structure for diagnosing optimisers in projection pursuit guided tour is first defined. With a fixed data structure, the diagnostic plots presented later in this paper can be easily applied to diagnose other optimisation procedures.

## Data structure for diagnostics 

Three main pieces of information are recorded for the projection pursuit optimisers: 1) projection bases $\mathbf{A}$, 2) index values $I$, and 3) state $S$. For CRS and SA, possible states include `random_search`,  `new_basis`, and `interpolation`. Pseudo derivative (PD) has a wider variety of states including `new_basis`, `direction_search`, `best_direction_search`, `best_line_search`, and `interpolation`. Multiple iterators index the information collected at different levels: $t$ is a unique identifier prescribing the natural ordering of each observation;  $j$ and $l$ are the counter of the outer and inner loop respectively. Other parameters of interest recorded include `method` that tags the name of the optimiser, and `alpha` that indicates the sampling neighbourhood size for searching observations.  A matrix notation of the data structure is presented in equation \ref{eq:data-structure}.

\begin{equation}
\renewcommand\arraystretch{2}  % default value: 1.0
\left[
\begin{array}{c|ccc|cc|cc}
t & \mathbf{A} & I & S & j &  l  & V_{1} & V_{2}\\
\hline
1 & \mathbf{A}_1 & I_1 & S_1 & 1 & 1 & V_{11} & V_{12}\\
\hline
2 & \mathbf{A}_2 & I_2 & S_2 & 2 & 1  & V_{21}  & V_{22}\\
3 & \mathbf{A}_3 & I_3 & S_3 & 2 & 2  & V_{31}  & V_{32}\\
\vdots & \vdots &\vdots &\vdots  &\vdots & \vdots &\vdots  &\vdots\\
\vdots & \vdots & \vdots &\vdots & 2 & l_2 & \vdots  & \vdots\\
\hline
\vdots &\vdots & \vdots &\vdots & 2  & 1& \vdots & \vdots\\
\vdots &\vdots &\vdots &\vdots & 2 & 2& \vdots &  \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots & \vdots  &\vdots \\
\vdots &\vdots &\vdots &\vdots & 2 & k_2 &\vdots  & \vdots\\
\hline
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
\hline
\vdots & \vdots & \vdots &\vdots  & J &  1 & \vdots & \vdots \\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
T & \mathbf{A}_T & I_T &S_T  & J &  l_{J} & V_{T1}& V_{T2}\\
\hline
\vdots &\vdots & \vdots &\vdots & J  & 1& \vdots & \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots & \vdots  &\vdots \\
\vdots &\vdots &\vdots &\vdots & J & k_J &\vdots  & \vdots\\
\hline
\vdots& \vdots & \vdots & \vdots & J+1 & 1 & \vdots& \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
T^\prime & \mathbf{A}_{T^\prime} & I_{T^\prime} &S_{T^\prime}  & J+1 &  l_{J+1} & V_{T^\prime 1}& V_{T^\prime 2}\\
\end{array}
\right]
= 
\left[
\begin{array}{c}
\text{column name} \\
\hline
\text{search (start basis)} \\
\hline
\text{search} \\
\text{search} \\
\vdots \\
\text{search (accepted basis)} \\
\hline
\text{interpolate} \\
\text{interpolate} \\
\vdots \\
\text{interpolate} \\
\hline
\vdots \\
\hline
\text{search} \\
\vdots \\
\text{search (final basis)} \\
\hline
\text{interpolate} \\
\vdots \\
\text{interpolate} \\
\hline
\text{search (no output)} \\
\vdots \\
\text{search (no output)} \\
\end{array}
\right]
\label{eq:data-structure}
\end{equation}

\noindent where $T^{\prime} = T + k_{J}+ l_{J+1}$. Note that there is no output in iteration $J + 1$ since the optimiser does not find a better basis in the last iteration and terminates. The final basis found is $A_T$ with index value $I_T$.

The data structure constructed above meets the tidy data principle [@wickham2014tidy] that requires each observation to form a row and each variable to form a column. With tidy data structure, data wrangling and visualisation can be significantly simplified by well-developed packages such as \CRANpkg{dplyr} [@dplyr] and \CRANpkg{ggplot2} [@ggplot2].

The construction of diagnostic plots adopts the core concept of grammar of graphics [@wickham2010layered] in ggplot2. In grammar of graphics, plots are not produced by calling the commands, named by the appearance of the plot, i.e., boxplot and histogram, but by the concept of stacked layers. Seeing plots as stacked layers gives analysts the freedom to composite plots with multiple elements on hands.

## Checking how hard the optimiser is working

A starting point of diagnosing an optimiser is to understand how many searches an optimiser has conducted. One may want to simply plot the index value of the search points across its natural order, but a point geometry may work well if each iteration has a similar number of points. When some iterations have considerably more points than others, using a point geometry over-emphasizes the iterations that have more search points since these iterations will occupy the mast majority of the plot space. An alternative is to summarise the search in each iteration using boxplots and each iteration will then be spaced out equally. Occasionally, one may still want to switch back to a point geometry if the number of points is small in a particular iteration and this can be achieved via the `cutoff` argument in the search plot function. Additional annotations are added to facilitate better reading of the plot and these include  
1) the number of points searched in each iteration can be added as text label at the bottom of each iteration; 2) the anchor bases to interpolate are connected and highlighted in a larger size; and 3) the colour of the last iteration is in a grey scale to indicate no better basis found in this iteration. 

Figure \ref{fig:toy-search} shows an example of the search plot for CRS (left) and SA (right). Both optimisers quickly find better bases in the first few iterations and then take longer to find one in the later iterations. The anchor bases, the ones found with the highest index value in each iteration, always have an increased index value in the optimiser CRS while this is not the case for SA. This feature gives SA an advantage in this simple example to quickly find the optimum. 

```{r toy-search, fig.cap="A comparison of the searches by two optimisers: CRS (left) and SA (right) on a 2D projection problem of a six-variable dataset, \\code{boa6} using holes index. Both optimisers reach the final basis with a similar index value while it takes SA longer to find the final basis."}
```

## Examining the optimisation progress {#toy-interp}

```{r toy-interp, fig.cap = "An inspection of the index values as the optimisation progress for two optimisers: CRS (left) and SA (right). The holes index is optimised for a 2D projection problem on the six-variable dataset \\code{boa6}. Lines indicate the interpolation and dots indicate new target bases generated by the optimisers. Interpolation in both optimisation is smooth while SA is observed to first pass by some bases with higher index value before reach the target bases in time 76-130."}
```

Another interesting diagnostic is to examine how the index value improves between interpolating bases since the projection on these bases is played by the tour animation. Trace plots are created by plotting the index value against time. Figure \ref{fig:toy-interp} presents the trace plot of the same optimisers as Figure \ref{fig:toy-search} and one can observe that the trace is smooth in both cases. It may seem bizarre at the first sight that the interpolation in the left panel first passes some bases with higher index value before decreases to a lower target. This happens because on the one hand, the probabilistic acceptance in SA implies that some worse bases will be accepted by the optimiser. On the other hand, guided tour interpolate between the current and target basis to provide a smooth transition between projections. These two facts indicate that a non-monotonic interpolation can't be avoided for SA. Later, in section \ref{monotonic}, there will be a discussion on improving the non-monotonic interpolation for CRS.

## Understanding the optimiser's coverage of the search space {#toy-pca}

```{r toy-pca, fig.asp=1, fig.height=2.5, fig.cap = "Search paths of CRS (green) and PD (brown) in the PCA-reduced basis space for 1D projection problem on the five-variable dataset, \\code{boa5} using holes index. The basis space, a 5D unit sphere, is projected onto a 2D circle by PCA. All the bases in PD has been flipped for easier comparison of the final bases and a grey dashed line has been annotated to indicate the symmetry of the two start bases."}
```

Apart from checking the search and progression of an optimiser, looking at where the bases are positioned in the basis space is another interest. Given the orthonormality constraint, the space of projection bases $\mathbf{A}_{p \times d}$ is a $p \times d$-dimensional sphere. A dimensionality reduction method, e.g. principal component analysis is applied to first project all the bases onto a 2D space. In a projection pursuit guided tour optimisation, there are various types of bases involved: 1) The starting basis; 2) The search bases that the optimiser evaluated to produce the anchor bases; 3) The anchor bases that have the highest index value in each iteration; 4) The interpolating bases on the interpolation path; and finally 5) the end basis. The importance of these bases differs and the most important ones are the starting, interpolating and end bases. The anchor and search bases can be turned on with argument `details = TRUE`. Sometimes, two optimisers can start with the same basis but finish with bases of opposite sign. This happens because the projection is invariant to the sign difference of the bases and so does the index value, however, this creates difficulties for comparing the optimisers since the end bases will be symmetric with respect to the origin. A sign flipping step is conducted to flip the signs of all the bases in one routine if different optimisations finish at opposite places.

Several annotations have been made to help understanding this plot. As mentioned previously, the original basis space is a high-dimensional sphere and random bases on the sphere can be generated via the CRAN package \CRANpkg{geozoo} [@geozoo]. Along with the bases recorded during the optimisation and a zero basis, the parameters (centre and radius) of the 2D space can be estimated. PCA is performed to get the first two PC coordinates of all the bases and the centre of the circle is the PC coordinates of the zero matrix, and radius is estimated as the largest distance from the centre to the basis. The theoretical best basis is known for simulated data and can be labelled to compare how close the end basis found by the optimisers is to the theoretical best. Various aesthetics, i.e. size, alpha and colour, are applicable to emphasize critical elements and adjust for the presentation. For example, anchor points and search points are less important and hence a smaller size and alpha is used. Alpha can also be applied on the interpolation paths to show the start to finish from transparent to opaque.

Figure \ref{fig:toy-pca} shows the PCA plot of CRS and PD for a 1D projection problem. Both optimisers find the optimum but PD gets closer. With PCA plot, one can visually appreciate the nature of these two optimisers: PD first evaluates points in a small neighbourhood for a promising direction, while CRS evaluates points randomly in the search space to search for the next target. There are dashed lines annotated for CRS and it describes the interruption of the interpolation, which will be discussed in details in Section \ref{monotonic}.

## Animating the diagnostic plots

```{r toy-pca-animated, fig.cap = "Six frames selected from the animated version of the previous plot. With animation, the progression of the search paths from start to finish are better identified. CRS (green) finishes the optimisation quicker than PD (brown) since there is no further movement for CRS in the sixth frame. The full video of the animation can be found at \\url{https://vimeo.com/504242845}."}
```

Animation is another type of display to show how the search progresses from start to finish in the space. An  `animate = TRUE` argument is used to enable the animation in the PCA plot. Figure \ref{fig:toy-pca-animated} shows six frames from the animation of the PCA plot in Figure \ref{fig:toy-pca}. An additional piece of information one can learn from this animation is that CRS finds its end basis quicker than PD since CRS finishes its search in the 5th frame while PD is still making more progress.

## The tour looking at itself

```{r toy-tour, fig.cap="Six frames selected from rotating the high dimensional basis space, along with the same two search paths from Figure \\ref{fig:toy-pca} and \\ref{fig:toy-pca-animated}. The basis space in this example is a 5D unit sphere, on which points (grey) are randomly generated via the CRAN package \\CRANpkg{geozoo}. The full video of the animation can be found at \\url{https://vimeo.com/512885379}."}
```

As mentioned previously, the original $p \times d$ dimension space can be simulated via randomly generated bases in the \CRANpkg{geozoo} [@geozoo] package. While the PCA plot projects the bases from the direction that maximises the variance, the tour plot displays the original high-dimensional space from various directions using animation. Figure \ref{fig:toy-tour} shows some frames from the tour plot of the same two optimisations in its original 5D space.

## Forming a torus

While the previous few examples have looked at the space of 1D bases in a unit sphere, this section visualises the space of 2D bases. Recall that the columns in a 2D basis are orthogonal to each other, the space of $p \times 2$ bases is a $p$-D torus. If take $p = 3$, one would see a classical 3D torus shape as shown by the grey points in Figure \ref{fig:toy-torus}. The two circles of the torus can be observed to be perpendicular to each other and this can be linked back to the orthogonality of the columns in the bases. Two paths optimised by CRS and PD are plotted on top of the torus and coloured in green and brown respectively to match the previous few 1D space plots. The final basis found by PD and CRS are shown in a larger shape  and printed below respectively:

```{r}
holes_2d_geo_3var %>% get_best() %>% pull(basis) %>% .[[1]]
```

```{r}
holes_2d_better_3var %>% get_best() %>% pull(basis) %>% .[[1]]
```

Both optimisers have found the third variable in the first column, but the second variable in the second column differs by a sign.One would expect to see this in the torus plot as the final bases match each other when projected onto one torus circle (due to the same sign in the first column) and be symmetric when projected onto the other (due the the sign difference in the second column). In Figure \ref{fig:toy-torus}, this can be seen most clearly in frame 5 where the two circles are rotated into a line from our view.

```{r toy-torus, fig.cap="Six frames selected from rotating the 2D basis space along with two search paths optimised by PD (brown) and CRS (green). The projection problem is a 2D projection with three variables using holes index. The grey points are randomly generated 2D projection bases in the space and it can be observed that these points form a torus. The full video of the animation can be found at \\url{https://vimeo.com/512882784}."}
```

# Diagnosing an optimiser {#application}

In this section, several examples will be presented to show how the diagnostic plots discover something unexpected in projection pursuit optimisation, and guide the implementation of new features.

## Simulation setup

Random variables with different distributions have been simulated and the distributional form of each variable is presented in equations \ref{eq:sim-norm} to \ref{eq:sim-x7}. Variables `x1`, `x8`, `x9` and `x10` are normal noise with zero mean and unit variance and `x2` to `x7` are normal mixtures with varied weights and locations. All the variables have been scaled to have an overall unit variance before projection pursuit.

\begin{align}
x_1 \overset{d}{=} x_8 \overset{d}{=} x_9 \overset{d}{=} x_{10}& \sim \mathcal{N}(0, 1) \label{eq:sim-norm} \\
x_2 &\sim 0.5 \mathcal{N}(-3, 1) + 0.5 \mathcal{N}(3, 1)\label{eq:sim-x2}\\
\Pr(x_3) &= 
\begin{cases}
0.5 & \text{if $x_3 = -1$ or $1$}\\
0 & \text{otherwise}
\end{cases}\label{eq:sim-x3}\\
x_4 &\sim 0.25 \mathcal{N}(-3, 1) + 0.75 \mathcal{N}(3, 1) \label{eq:sim-x4}\\
x_5 &\sim \frac{1}{3} \mathcal{N}(-5, 1) + \frac{1}{3} \mathcal{N}(0, 1) + \frac{1}{3} \mathcal{N}(5, 1)\label{eq:sim-x5}\\
x_6 &\sim 0.45 \mathcal{N}(-5, 1) + 0.1 \mathcal{N}(0, 1) + 0.45 \mathcal{N}(5, 1)\label{eq:sim-x6}\\
x_7 &\sim 0.5 \mathcal{N}(-5, 1) + 0.5 \mathcal{N}(5, 1) 
\label{eq:sim-x7}
\end{align}

## A problem of non-monotonicity {#monotonic}

An example of non-monotonic interpolation has been given in Figure \ref{fig:toy-interp}: a path that passes bases with higher index value than the target one. For SA, a non-monotonic interpolation is justified since target bases do not necessarily have a higher index value than the current one, while this is not the case for CRS. The original trace plot for a 2D projection problem, optimised by CRS, is shown on the left panel of Figure \ref{fig:interruption} and one can observe clearly that the non-monotonic interpolation has undermined the optimiser to realise its full potential. Hence, an interruption is implemented to stop at the best basis found in the interpolation. The right panel of Figure \ref{fig:interruption} shows the trace plot after implementing the interruption and while the first two interpolations are identical, the basis at Time 61 has a higher index value than the target in the third interpolation. Rather than starting the next iteration from the target basis on Time 65, CRS starts the next iteration at Time 61 on the right panel and reaches a better final basis.

```{r interruption, fig.cap = "Comparison of the interpolation before and after implementing the interruption for the 2D projection problem on \\code{boa6} data using holes index, optimised by CRS. On the left panel, basis with higher index value is found during the interpolation but not used. On the right panel, the interruption stops the interpolation at the basis with the highest index value for each iteration and results in a final basis with higher index value, as shown on the right panel."}
```

## Close but not close enough


```{r polish, fig.height= 5, fig.cap = "Comparison of the projected data before and after using polishing along with the trace of index value of a 2D projection problem on \\code{boa6} data using holes index. Optimiser CRS is used before the polish. The clustering structure in the data is detected by CRS but the polish step improves the index value and produces clearer boundaries of the clusters, especially along the vertial axis."}
```

Once the final basis has been found by an optimiser, one may want to push further in the close neighbourhood to see if an even better basis can be found. A polish search takes the final basis of an optimiser as the start of a new guided tour to search for local breakthrough. The polish algorithm is similar to the CRS but with three distinctions: 1) a hundred rather than one candidate bases are generated each time in the inner loop; 2) the neighbourhood size is reduced in the inner loop, rather than in the outer loop; and 3) three more termination conditions have been added to ensure the new basis generated is distinguishable from the current one in terms of the distance in the space,  percentage change in the index value, and neighbourhood size: 

1) the distance between the basis found and the current basis needs to be larger than 1e-3;
2) the percentage change of the index value need to be larger than 1e-5; and
3) the alpha parameter on itself needs to be larger than 0.01

Figure \ref{fig:polish} presents the trace plot of a 2D projection, optimised by CRS and followed by the polish. The projection by the final basis of each algorithm is also shown. The end basis found by CRS reveals the four clusters in the data, but the edges of each cluster are not clean-cut. Polish works with this end basis and further pushes the index value to produce much clear edges of the cluster, especially along the vertical axis.

## Seeing the signal in the noise 

The holes index function used for all the examples before this section produces a smooth interpolation, while this is not the case for all the indices. A 1D projection function, $I^{K}(n)$, compar‚es the projected data, $\mathbf{Y}_{n \times 1}$, to a randomly generated normal distribution, $\mathcal{N}_{n \times 1}$, based on the Kolmogorov test. Let $F_{.}(n)$ be the ECDF function, with two possible subscripts $Y$ and $\mathcal{N}$ representing the projected and randomly generated data, and $n$ denoting the number of observation, the Normal Kolmogorov index, $I^{nk}(n)$, is defined as: 

$$I^{K}(n) = \max \left[F_{Y}(n) - F_{\mathcal{N}}(n)\right]$$
With a non-smooth index function, two research questions are raised: 

1) whether any optimiser fails to optimise this non-smooth index; and 
2) whether the optimisers can find the global optimum despite the presence of a local optimum

```{r noisy-better-geo, fig.height = 4, fig.cap = "Comparison of the three optimisers in optimising $I^{nk}(n)$ index for a 1D projection problem on a five-variable dataset, \\code{boa5}. Both CRS and SA succeeds in the optimisation, PD fails to optimise this non-smooth index. Further, SA takes  much longer than CRS to finish the optimisation, it finishes off closer to the theoretical best."}
```


Figure \ref{fig:noisy-better-geo} presents the trace and PCA plots of all three optimisers and as expected, none of the interpolated path is smooth. There is barely any improvement made by PD, indicating its failure in optimising non-smooth index. While CRS and SA have managed to get close to the index value of the theoretical best, trace plot shows that it takes SA much longer to interpolate towards the final basis. This long interpolation path is partially due to the fluctuation in the early iterations, where the SA tends to generously accept inferior bases before concentrating near the optimum. The PCA plot shows the interpolation path and search points excluding the last termination iteration. Pseudo Derivative (PD) quickly gets stuck near the starting position. Comparing the amount of random search done by CRS and SA, it is surprising that SA does not carry as many samples as CRS. Combining the insights from both the trace and PCA plot, one can learn the two different search strategies by CRS and SA: CRS frequently samples in the space and only make a move when an improvement is guaranteed to be made, while SA first broadly accepts bases in the space and then starts the extensive sampling in a narrowed subspace. The specific decision of which optimiser to use will depend on the index curve in the basis space but if the basis space is non-smooth, accepting inferior bases at first, as what SA has done, can lead to a more efficient search, in terms of the overall number of points evaluated.

```{r kol-result, fig.height = 4, fig.cap="Comparing 20 search paths in the PCA-projected basis space facetted by two optimisers: CRS and SA, and two search sizes: 0.5 and 0.7. The optimisation is on the 1D projecton index, $I^{nk}(n)$, for \\code{boa6} data, where a local optimum, annotated by the cross (x), is presented in this experiment, along with the global optimum (*)."}
```

The next experiment compares the performance of CRS and SA when a local maximum exists. Two search neighbourhood sizes: 0.5 and 0.7 are compared to understand how a large search neighbourhood would affect the final basis and the length of the search. Figure \ref{fig:kol-result} shows 80 paths simulated using 20 seeds in the PCA plot, faceted by  optimiser and search size. With CRS and a search size of 0.5, despite being the simplest and fastest, the optimiser fails in three instances where the final basis lands neither near the local nor the global optimum. With a larger search size of 0.7, more seeds have found the global maximum. Comparing CRS and SA for a search size of 0.5, SA does not seem to improve the final basis found, despite having longer interpolation paths. However, the denser paths near the local maximum is an indicator that SA is working hard to examine if there is any other optimum in the basis space but the relative small search size has diminished its ability to reach the global maximum. With a larger search size, almost all the seeds (16 out of 20) have found the global maximum and some final bases are much closer to the theoretical best, as compared to the three other cases. This indicates that SA, with a reasonable large search size, is able to overcome the local optimum and optimise close towards the global optimum.

## Reconciling the orientation

```{r flip-sign, fig.cap = "Comparison of the interpolation in the PCA-projected basis space before and after reconciling the orientation of the target basis. Optimisation is on the 1D projection index, $I^{nk}(n)$, for boa6 data using CRS with seed 2463. The dots represent the target basis in each iteration and the path shows the interpolation. On the left panel, one target basis is generated with an opposite orientation to the current basis (hence appear on the other side of the basis space) and the interpolator crosses the origin to perform the interpolation. The right panel shows the same interpolation after implementing an orientation check and the undesirable interpolation disappears."}
```

One interesting situation observed in the previous examples is that, for some simulations as shown on the left panel of Figure \ref{fig:flip-sign}, the target basis is generated on the other half of the basis space and the interpolator seems to draw a straight line to interpolate. As mentioned previously, bases with opposite signs do not affect the projection and index value, but clearly, we prefer the target to have the same orientation as the current basis. The orientation of two bases can be checked via calculating the determinant and a negative value indicates opposite direction. Hence an orientation check is carried out once a new target basis is generated and the sign in the target basis will be flipped if a negative determinant is obtained. The interpolation after implementing the orientation check is shown on the right panel of Figure \ref{fig:flip-sign} where the unsatisfactory interpolation no longer exist.

# Implementation {#implementation}

The implementation of this projection has been divided into two packages: the data collection object is implemented in the existing CRAN package \CRANpkg{tourr} [@tourr] while the optimiser diagnostics have been implemented in a new package, \pkg{ferrn}. When a guided tour is run, the users can choose to collect the data by binding `animate_*()` to a variable. Once the data object has been obtained, the \pkg{ferrn} package provides diagnostic plots shown in Section \ref{vis-diag}. The main functions of the \pkg{ferrn} package functionality is listed below.

- Main plotting functions: 

  - \code{explore\_trace\_search()} produces summary plots in Figure \ref{fig:toy-search}
  - \code{explore\_trace\_interp()}produces trace plots for the interpolation points in Figure \ref{fig:toy-interp}
  - \code{explore\_space\_pca()} produces the PCA plot of projection bases on the reduced space in Figure \ref{fig:toy-pca}. Animated version in Figure \ref{fig:toy-pca-animated} can be turned on via the argument `animate = TRUE`.
  - \code{explore\_space\_tour()} produces animated tour view on the full space of the projection bases in Figure \ref{fig:toy-tour}.

- \code{get\_*()} extracts and manipulates certain components from the existing data object.
  - \code{get\_anchor()} extracts target observations 
  - \code{get\_basis\_matrix()} flattens all the bases into a matrix
  - \code{get\_best()} extracts the observation with the highest index value in the data object
  - \code{get\_dir\_search()} extracts directional search observations for PD search
  - \code{get\_interp()} extracts interpolated observations
  - \code{get\_interp\_last()} extracts the ending interpolated observations in each iteration
  - \code{get\_interrupt()} extracts the ending interpolated observations and the target observations if  the interpolation is interrupted
  - \code{get\_search()} extracts search observations 
  - \code{get\_search\_count()} extracts the count of search observations 
  - \code{get\_space\_param()} produces the coordinates of the centre and radius of the basis space
  - \code{get\_start()} extracts the starting observation
  - \code{get\_theo()} extracts the theoretical best observations, if presented 
  

- \code{bind\_*()} incorporates additional information outside the tour optimisation into the data object.
  - \code{bind\_theoretical()} binds the theoretical best observation in simulated experiment
  - \code{bind\_random()} binds randomly generated bases in the projection bases space to the data object
  - \code{bind\_random\_matrix()} binds randomly generated bases and outputs in a matrix format

- \code{add\_*()} create ggprotos for different components for the PCA plot
  - \code{add\_anchor()} is a wrapper for plotting anchor bases
  - \code{add\_anno()} is a wrapper for annotating the symmetry of start bases
  - \code{add\_dir\_search()} is a wrapper for plotting the directional search bases with magnified distance
  - \code{add\_end()} is a wrapper for plotting end bases
  - \code{add\_interp()} is a wrapper for plotting the interpolation path
  - \code{add\_interp\_last()} is a wrapper for plotting the last interpolation bases for comparing with target bases when interruption is carried
  - \code{add\_interrupt()} is a wrapper for linking the last interpolation bases with target ones when interruption is carried
  - \code{add\_search()} is a wrapper for plotting search bases
  - \code{add\_space()} is a wrapper for plotting the circular space 
  - \code{add\_start()} is a wrapper for plotting start bases
  - \code{add\_theo()} is a wrapper for plotting theoretical best bases, if applicable

- Utilities
  - \code{theme\_fern()} and \code{format\_label()} for better display of the grid lines and axis formatting
  - \code{clean\_method()} to clean up the name of the optimisers
  - \code{botanical\_palettes} is a collection of colour palettes from Australian native plants. Quantitative palettes include daisy, banksia and cherry and sequential palettes contain fern and acacia
  - \code{botanical\_pal()} as the colour interpolator 
  - \code{scale\_color\_*()} and \code{scale\_fill\_*()} for scaling the colour and fill of the plot

# Conclusion

This paper has illustrated setting up a data object that can be used for diagnosing a complex optimisation procedure. The ideas were illustrated using the optimisers available for projection pursuit guided tour. Here the constraint is the orthonormality condition of the projection bases. The approach used here could be broadly applied to understand other constrained optimisers.

<!-- something about the diagnostic plots  -->
Four diagnostic plots have been introduced to investigate the progression and the projection space of an optimiser. The implementation of these visualisations is designed to be easy-to-use with each plot can be produced with a simple supply of the data object. More advanced users may decide to modify on top of the basic plots or even build their own. 

<!-- summarise what we have done and what might be done in the future. -->
Most of the work in this project has been translated into code in two packages: the collection of the data object is implemented in the existing \CRANpkg{tourr} [@tourr] package; while the manipulation and visualisation of the data object are implemented in the new \pkg{ferrn} package. Equipped with handy tools to diagnose the performance of optimisers, future work can extend the diagnostics to a wider range of index functions, i.e. scagnostics, association, and information index [@laa2020using] and understand how the optimisers behave for index functions with different structures.

# Acknowledgements

This article is created using \CRANpkg{knitr} [@knitr] and \CRANpkg{rmarkdown} [@rmarkdown]  in R. The source code for reproducing this paper can be found at: \url{https://github.com/huizezhang-sherry/paper-tour-vis}.

\bibliography{zhang-cook-laa-langrene-menendez}
