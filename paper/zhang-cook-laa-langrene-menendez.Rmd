---
title: "Visual Diagnostics for Constrained Optimisation with Application to Guided Tours"
author:
  - name: H.Sherry Zhang
    affiliation: Monash University 
    address: Department of Econometrics and Business Statistics
    email:  huize.zhang@monash.edu
  - name: Dianne Cook
    affiliation: Monash University 
    address: Department of Econometrics and Business Statistics
    email:  dicook@monash.edu
  - name: Ursula Laa
    affiliation: University of Natural Resources and Life Sciences
    address: Institute of Statistics
    email:  ursula.laa@boku.ac.at  
  - name: Nicolas Langrené
    affiliation: CSIRO Data61 
    address: 34 Village Street, Docklands VIC 3008 Australia
    email: nicolas.langrene@csiro.au
  - name: Patricia Menéndez
    affiliation: Monash University 
    address: Department of Econometrics and Business Statistics
    email:  patricia.menendez@monash.edu 

      
abstract: >
  Guided tour searches for interesting low-dimensional views of high-dimensional data via optimising a projection pursuit index function. The first paper of projection pursuit by @friedman1974projection stated that "the technique used for maximising the projection index strongly influences both the statistical and the computational aspects of the procedure." While many indices have been proposed in the literature, less work has been done on evaluating the performance of the optimisers. In this paper, we implement a data collection object for the optimisation in the guided tour and introduce visual diagnostics based on the data object collected. These diagnostics and workflows can be applied to a broad class of optimisers, to assess their performance. An R package, \pkg{ferrn}, has been created to implement the diagnostics.
preamble: >
  \usepackage{amssymb, amsmath, mathtools, dsfont, bbm, array, booktabs}
  \usepackage[ruled,vlined, linesnumbered]{algorithm2e}
output: rticles::rjournal_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE,
                      root.dir = here::here(), 
                      fig.path = here::here("figs/"), 
                      cache = TRUE,
                      fig.align = "center",
                      fig.height = 3,
                      out.width = "100%")
```

```{r external, include = FALSE, cache = FALSE}
library(knitr)
library(tidyverse)
library(ferrn)
read_chunk(here::here('scripts/toy.R'))
read_chunk(here::here('scripts/diagnose.R'))
read_chunk(here::here('scripts/kol_result.R'))
read_chunk(here::here('scripts/implementation.R'))
```

```{r load-pkg}
```

# Introduction

Visualisation is widely used in exploratory data analysis [@tukey1977exploratory; @unwin2015graphical;  @healy2018data; @wilke2019fundamentals]. Presenting information in graphics often unveils information that would otherwise not be aware of and provides a more comprehensive understanding of the problem at hand. Task specific tools presented by @li2020visualizing show how visualisation can be used to understand the behaviour of neural network on classification models, but no general visualisation tool available for diagnosing optimisation procedure. The work presented in this paper brings visualization tools into optimisation problems with an aim to better understand the performance of the optimisers in practice.

The goal of continuous optimisation is to find the best solution within the space of all feasible solutions where typically the best solution is decided by an objective function. Broadly speaking, optimization can be unconstrained or constrained [@kelley1999iterative]. The unconstrained problem can be formulated as a minimization (or maximization)  problem such as 
$\min_{x} f(x)$ where $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is an objective function with  certain properties defined in an $L^p$ space. In this case, solutions rely on gradient descent or ascent methods. In the constrained optimization problem additional restrictions  are introduced via a set of functions that can be convex or non-convex: $g_i:\mathbb{R}^n \rightarrow \mathbb{R}$ for $i = 1, \ldots k$ and hence the problem can be written as
$\min_{x} f(x)$ *subject to* $g_i(x) \leq 0$. Here methods such as Langrange multipliers and convex optimization methods including linear and quadratic programming can be used.

The focus of this paper is on the optimisation problem arising in the projection pursuit guided tour [@buja2005computational] which is an exploratory data analysis tool that is defined to detect *interesting structures* or features in high-dimensional data through a set of lower-dimensional projections that cover the entire high dimensional space using interpolation methods called tours [@cook2008grand]. The target of the optimisation is to identify the most *interesting* low-dimensional views of the data given by a corresponding projection matrix. The most *interesting* structures are formally defined by a function of projections, called index function which is optimized to uncover the most revelling structures in a high dimensional space [@cook1993projection]. 


The optimization challenges encountered in the projection pursuit guided tour problem are common to those of optimization in general. Examples of those include the existence of multiple maxima (local and global), the trade off between computational burden and proximity to the maxima,  dealing with noisy objective functions that might be non smooth and non differentiable [@jones1998efficient]. Those  are not unique to this context and therefore the visualization tools and optimization methods presented in this paper can be easily applied to any other optimization problems.

The remainder of the paper is organised as follows. 
Section \ref{optim} provides an overview of optimisation methods, specifically line search methods. 
Section \ref{tour} reviews projection pursuit guided tour, defines the optimisation problem and introduces three existing algorithms.
Section \ref{vis-diag} presents the new visual diagnostics. A data structure is defined to capture information during the optimisation, and used in different types of diagnostic plots.
Section \ref{application} shows applications of how these plots can be used to understand and compare different algorithms. We also discuss how these insights contribute to modifications that improve the algorithms.
Finally, Section \ref{implementation} describes the R package: \pkg{ferrn}, that implements the visual diagnostics.


# Optimisation methods {#optim}

Optimization problems are ubiquitous in many areas of study. While in some cases analytical solutions can be found, the majority of problems rely on numerical methods to find the optimal solution. These numerical methods follow iterative approaches that aim at finding the optimum by progressively improving the current solution until a desirable accuracy is achieved. Although this principle seems uncomplicated, a number of challenges arise such as the possible existence of multiple maxima (local and global), constraints and noisy objective function, and the trade-off between desirable accuracy and computational burden. In addition, the optimization results might depend on the algorithm starting values, affecting the consistency of results. 

Optimization methods can be divided into various classes, such as global optimisation [@kelley1999iterative; @fletcher2013practical], convex optimisation [@boyd2004convex] or stochastic optimisation [@nocedal2006numerical]. Our interest is on constrained optimization [@bertsekas2014constrained] as defined in the introduction section, and assuming it is not possible to find a solution to the problem in the way of a closed-form. That is, the problem consists of finding the minimum or maximum of a function $f \in L^p$ in the constrained $\mathbb{A}$ space. 

A large class of methods utilises the gradient information of the objective function to perform the optimisation iterations, with the most notable one being the gradient ascent (descent) method. Although gradient optimization methods are popular, they rely on the availability of the objective function derivatives and on the complexity of the constraints. Derivative-free methods, which do not rely on the knowledge of the gradient, are more generally applicable. Derivative-free methods have been developed over the years, where the emphasis is on finding, in most cases, a near optimal solution. Examples of those include response surface methodology [@box1951experimental], stochastic approximation [@robbins1951stochastic], random search [@fu2015handbook] and heuristic methods [@sorensen2013metaheuristics]. Later, we will present a simulated annealing optimisation algorithm, which belongs to the class of random search methods, for optimisation with the guided tour.

A common search scheme utilised by both derivative-free methods and gradient methods is line search. In line search methods, users are required to provide an initial estimate $x_{1}$ and, at each iteration, a search direction $S_k$ and a step size $\alpha_k$ are generated. Then one moves on to the next point following $x_{k+1} = x_k + \alpha_kS_k$ and the process is repeated until the desire convergence is reached. While gradient-based methods choose the search direction by the gradient, derivative-free methods uses local information of the objective function to determine the search direction. The choice of step size also needs considerations, as inadequate step sizes might prevent the optimisation method to converge to an optimum. An ideal step size can be chosen via finding the value of $\alpha_k \in \mathbb{R}$ that maximises $f(x_k + \alpha_kS_k)$ with respect to $\alpha_k$ at each iteration. 

Several R implementations address optimization problems with both general purpose as well as task specifics solvers. The most prominent one within the general solvers is \code{optim()} in the \CRANpkg{stats} [@stats] package, which provides both gradient-based and derivative-free optimisation functions. Another general solver specialised in non-linear optimisation is \CRANpkg{nloptr} [@nloptr]. Specific solvers for simulated annealing includes \code{optim(..., method  "SANN")} and package \CRANpkg{GenSA} [@gensa] that deals with more complicated objective functions. For other task specific solvers, readers are recommended to visit the relevant sections in CRAN task review on \ctv{optimisation and mathematical programming} [@crantaskreviewoptim].

# Projection pursuit guided tour {#tour}

The projection pursuit guided tour combines two different methods in exploratory data analysis, focusing on different aspects. Projection pursuit, coined by @friedman1974projection, detects interesting structures (e.g. clustering, outliers and skewness) in multivariate data via low dimensions projection. The guided tour is using ideas from projection pursuit to define a particular variation in a broader class of data visualisation methods, building on the grand tour approach [@As85].

To define projection pursuit, we first need to establish the notation used. Let $\mathbf{X}_{n \times p}$ be the data matrix, with $n$ observations in $p$ dimensions. A d-dimensional projection can be seen as a linear transformation from $\mathbb{R}^p$ into $\mathbb{R}^d$, and defined as $\mathbf{Y} = \mathbf{X} \cdot \mathbf{A}$, where $\mathbf{Y}_{n \times d}$ is the projected data and $\mathbf{A}_{p\times d}$ is the projection matrix. Define $f: \mathbb{R}^{n \times d} \mapsto \mathbb{R}$ to be an index function that maps the projected data $\mathbf{Y}$ (corresponding to an associated projection matrix $\mathbf{A}$) onto an index value $I$ (QUESTION: Isn't f the index function?). This is commonly known as the projection pursuit index function, or just index function, and is used to measure the "interestingness" of a given projection.

A number of index functions have been proposed in the literature to detect different data structures, including Legendre index [@friedman1974projection], Hermite index [@hall1989polynomial], natural Hermite index [@cook1993projection], chi-square index [@posse1995projection], LDA index [@lee2005projection] for supervised classification problems, and PDA index [@lee2010projection], which is an extension of the LDA index. 

As a general visualisation method, a tour produces animations of high dimensional data via rotations between low dimension planes. Different tour types choose these planes differently, for example, a grand tour [@cook2008grand] selects the planes randomly to provide a general overview and a manual tour [@cook1997manual] gradually phases in and out one variable, to understand the contribution of that variable in the projection. Guided tour, the main interest of this paper, chooses planes with the aid of projection pursuit to gradually reveal the most interesting projection in the low dimension space. Given a random start, projection pursuit iteratively finds bases with higher index values and the guided tour constructs the geodesic interpolation between these planes to form a tour path. Intuitively, Figure \ref{fig:tour-path} shows a sketch of the tour path where the blue frames are produced by the projection pursuit optimisation algorithm, and the white frames interpolate between them. Mathematical details of the geodesic interpolation can be found in @buja2005computational.  The tour method has been implemented in the R package \CRANpkg{tourr} [@tourr].

```{r tour-path, out.height="25%",out.width="50%",fig.cap="Each square (frame) represents the projected data with a corresponding basis. Blue frames are found by an optimisation algorithm iteratively whilst the white frames are constructed between two blue frames by geodesic interpolation."}
include_graphics(here::here("img/tour_path.png"))
```

## Optimisation in the tour {#tour-optim}

The optimisation problem in the tour context is stated as follows: Given a randomly generated starting basis $\mathbf{A}_1$, projection pursuit finds the final projection basis $\mathbf{A}_T$ that satisfies the following optimisation problem: 


\begin{align}
&\arg \max_{\mathbf{A} \in \mathcal{A}} f(\mathbf{X} \cdot \mathbf{A}) \\
&s.t.  \mathbf{A}^{\prime} \mathbf{A} = I_d
\end{align}

\noindent where $I_d$ is the $d$-dimensional identity matrix and the constraint requires the projection bases $\mathbf{A}$ to be orthogonal matrices.

Several features of this optimisation are worth noticing. First of all, this is a constrained optimisation problem as the decision variables form the entries of a projection basis, which is required to be orthonormal. It is also likely that the objective function may not be differentiable for a constructed index function and in these cases, gradient-based methods may not work well. Although finding the global maximum is the goal of an optimisation problem, it is also interesting to inspect local maximum in projection pursuit since it could present unexpected interesting projections. Lastly, there is also one computational consideration: the optimisation procedure needs to be fast to compute since the tour animation is played in real-time. 

## Existing algorithms

Below we introduce three line search algorithms in tour: simulated annealing (SA), simulated annealing with jump out (SAJO), and pseudo derivative (PD).

\begin{algorithm}
\SetAlgoLined
  \SetKwInOut{input}{input}
  \SetKwInOut{output}{output}
    \input{$f(.)$, $\alpha_1$, $l_{\max}$, $\text{cooling}$} 
    \output{$\mathbf{A}_{l}$}
    generate random start $\mathbf{A}_1$ and set $\mathbf{A}_{\text{cur}} \coloneqq \mathbf{A}_1$, $I_{\text{cur}} = f(\mathbf{A}_{\text{cur}})$, $j = 1$\;
  \Repeat{$\mathbf{A}_l$ is too close to $\mathbf{A}_{\text{cur}}$ in terms of geodesic distance}{
   set $l = 1$\;
  \Repeat{$l > l_{\max}$ or $I_{l} > I_{\text{cur}}$}{
    generate $\mathbf{A}_{l} = (1- \alpha_j)\mathbf{A}_{\text{cur}} + \alpha_j \mathbf{A}_{\text{rand}}$ and orthogonalise $\mathbf{A}_{l}$\;
    compute $I_{l}  = f(\mathbf{A}_{l})$\;
    update $l = l + 1$\;
  }
  update $\alpha_{j+1} = \alpha_j * \text{cooling}$\;
  construct the geodesic interpolation between $\mathbf{A}_{\text{cur}}$ and $\mathbf{A}_l$\; 
  update $\mathbf{A}_{\text{cur}} = \mathbf{A}_l$ and $j = j + 1$\;
}
  \caption{random search}
  \label{random-search}
\end{algorithm}

Simulated annealing (SA) is a random search device that samples a candidate basis $\mathbf{A}_{l}$ in the neighbourhood of the current basis $\mathbf{A}_{\text{cur}}$ by $\mathbf{A}_{l} = (1- \alpha)\mathbf{A}_{\text{cur}} + \alpha \mathbf{A}_{\text{rand}}$ where $\alpha$ controls the radius of the sampling neighbourhood and $\mathbf{A}_{\text{rand}}$ is a randomly generated matrix with the same dimension as $\mathbf{A}_{\text{cur}}$. $\mathbf{A}_{l}$ is then orthogonalised to ensure the orthonormal constraint is fulfilled. When a basis is found with index value higher than the current basis $\mathbf{A}_{\text{cur}}$, the search terminates and outputs the basis for guided tour to construct an interpolation path. The next iteration of search begins after adjusting $\alpha$ by a cooling parameter: $\alpha_{j+1} = \alpha_j * \text{cooling}$. The termination condition is when the maximum number of iteration $l_{\max}$ is reached. The algorithm of simulated annealing is summarised in Algorithm \ref{random-search}.  A slightly different cooling scheme has been proposed by @posse1995projection to avoid the search space being reduced too fast. A halving parameter $c$ is introduced and $\alpha$ is only adjusted if the last search takes more than $c$ times to find an accepted basis.

\begin{algorithm}
\SetAlgoLined
\Repeat{$l > l_{\max}$ or $I_{l} > I_{\text{cur}}$ or $P > U$}{
    generate $\mathbf{A}_{l} = (1- \alpha_j)\mathbf{A}_{\text{cur}} + \alpha_j \mathbf{A}_{\text{rand}}$ and orthogonalise $\mathbf{A}_{l}$\;
    compute $I_{l}  = f(\mathbf{A}_{l})$, $T(l) = \frac{T_0}{\log(l + 1)}$ and $P= \min\left\{\exp\left[-\frac{I_{\text{cur}} -I_{l}}{T(l)}\right],1\right\}$\;
    draw $U$ from a uniform distribution: $U \sim \text{Unif(0, 1)}$\;
    update $l = l + 1$\;
  }
  \caption{simulated annealing}
  \label{simulated-annealing}
\end{algorithm}

Simulated annealing with jump out (SAJO) [@kirkpatrick1983optimization; @bertsimas1993simulated] uses the same sampling process but allows a probabilistic acceptance of a inferior basis with lower index value based on the annealing $T(l)$. Given an initial $T_0$, the temperature at iteration $l$ is defined as $T(l) = \frac{T_0}{\log(l + 1)}$. When a candidate basis fails to have an index value larger than the current basis, simulated annealing gives it a second chance to be accepted with probability $$P= \min\left\{\exp\left[-\frac{\mid I_{\text{cur}} - I_{l} \mid}{T(l)}\right],1\right\}$$ where $I_{(\cdot)}$ denotes the index value of a given basis. This implementation allows the algorithm to jump out of a local maximum and enables a more holistic search of the whole parameter space. This feature is particularly useful when local maxima are present. The algorithm \ref{simulated-annealing} highlights how SAJO differs from SA in the inner loop.



\begin{algorithm}
\SetAlgoLined
\Repeat{$l > l_{\max}$ or $p_{\text{diff}} > 0.001$}{
  generate $n$ random directions $\mathbf{A}_{\text{rand}}$ \;
  compute $2n$ candidate bases deviate from $\mathbf{A}_{\text{cur}}$ by an angle of $\delta$ while ensure orthogonality\;
  compute the corresponding index value for each candidate bases\;
  determine the search direction as from $\mathbf{A}_{\text{cur}}$ to the candidate bases with the largest index value\;
  determine the step size via optimising the index value on the search direction over a 90 degree window\;
  find the optima $\mathbf{A}_{**}$ and compute $I_{**} = f(\mathbf{A}_{**})$, $p_{\text{diff}} = (I_{**} - I_{\text{cur}})/I_{**}$\;
  update $l = l + 1$\;
}
\caption{search geodesic}
\label{search-geodesic}
\end{algorithm}


In pseudo derivative [@cook1995grand] method, the search direction is computed using the most prominent direction that deviating an tiny angle of $\delta$ from the current basis. The step size is chosen by optimising the index value along the geodesic direction over an 90 degree angle from $-\pi/4$ to $\pi/4$ along the search direction chosen. The optima $\mathbf{A}_{**}$ is returned for the current iteration if it meets the percentage improve condition or when $l_{\max}$ is reached.  Algorithm \ref{search-geodesic} summarises the inner loop of the pseudo derivative search.




# Visual diagnostics {#vis-diag}

To be able to make diagnostics on the optimisers, the algorithms need to populate a data structure with key elements of the algorithm. When the algorithms run, key information regarding the decision variable, objective function and hyper-parameters needs to be recorded and stored as a data object so that it is ready to be supplied to the plotting functions for diagnostics. 

## Data structure for diagnostics 

Three main elements are recorded for the projection pursuit optimisers: 1) projection bases: $\mathbf{A}$, 2) index values: $I$, and 3) State: $S$, which labels the observation with detailed stage in the optimisation. For optimiser SA and SAJO, possible values of the state include `random_search`,  `new_basis`, and `interpolation`. pseudo derivative search has a wider variety of state including `new_basis`, `direction_search`, `best_direction_search`, `best_line_search`, and `interpolation`.

Multiple iterators are also  needed to index the data collected at different levels: $t$ being a unique identifier that prescribes the natural ordering of each observation while $j$ and $l$ being the counter of the outer and inner loop, respectively, in Algorithm \ref{random-search}, \ref{simulated-annealing} and \ref{search-geodesic} above. Other parameters of interest recorded include $V_1 = \text{method}$ that tags the name of the optimiser, and $V_2 = \text{alpha}$ that indicates the sampling neighbourhood size.  A matrix notation of the data structure is presented in Equation \ref{eq:data-structure}.

\begin{equation}
\renewcommand\arraystretch{2}  % default value: 1.0
\left[
\begin{array}{c|ccc|cc|cc}
t & \mathbf{A} & I & S & j &  l  & V_{1} & V_{2}\\
\hline
1 & \mathbf{A}_1 & I_1 & S_1 & 1 & 1 & V_{11} & V_{12}\\
\hline
2 & \mathbf{A}_2 & I_2 & S_2 & 2 & 1  & V_{21}  & V_{22}\\
3 & \mathbf{A}_3 & I_3 & S_3 & 2 & 2  & V_{31}  & V_{32}\\
\vdots & \vdots &\vdots &\vdots  &\vdots & \vdots &\vdots  &\vdots\\
\vdots & \vdots & \vdots &\vdots & 2 & l_2 & \vdots  & \vdots\\
\hline
\vdots &\vdots & \vdots &\vdots & 2  & 1& \vdots & \vdots\\
\vdots &\vdots &\vdots &\vdots & 2 & 2& \vdots &  \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots & \vdots  &\vdots \\
\vdots &\vdots &\vdots &\vdots & 2 & k_2 &\vdots  & \vdots\\
\hline
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
\hline
\vdots & \vdots & \vdots &\vdots  & J &  1 & \vdots & \vdots \\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
T & \mathbf{A}_T & I_T &S_T  & J &  l_{J} & V_{T1}& V_{T2}\\
\hline
\vdots &\vdots & \vdots &\vdots & J  & 1& \vdots & \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots & \vdots  &\vdots \\
\vdots &\vdots &\vdots &\vdots & J & k_J &\vdots  & \vdots\\
\hline
\vdots& \vdots & \vdots & \vdots & J+1 & 1 & \vdots& \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
T^\prime & \mathbf{A}_{T^\prime} & I_{T^\prime} &S_{T^\prime}  & J+1 &  l_{J+1} & V_{T^\prime 1}& V_{T^\prime 2}\\
\end{array}
\right]
= 
\left[
\begin{array}{c}
\text{column name} \\
\hline
\text{search (start basis)} \\
\hline
\text{search} \\
\text{search} \\
\vdots \\
\text{search (accepted basis)} \\
\hline
\text{interpolate} \\
\text{interpolate} \\
\vdots \\
\text{interpolate} \\
\hline
\vdots \\
\hline
\text{search} \\
\vdots \\
\text{search (final basis)} \\
\hline
\text{interpolate} \\
\vdots \\
\text{interpolate} \\
\hline
\text{search (no output)} \\
\vdots \\
\text{search (no output)} \\
\end{array}
\right]
\label{eq:data-structure}
\end{equation}

\noindent where $T^{\prime} = T + k_{J}+ l_{J+1}$. Note that there is no output in iteration $J + 1$ since the optimiser cannot find a better basis in the last iteration and the algorithm terminates. The final basis found is $A_T$ with the highest index value $I_T$.

The data structure constructed above meets the tidy data principle [@wickham2014tidy] that requires each observation to form a row and each variable to form a column. With tidy data structure, data wrangling and visualisation can be significantly simplified by well-developed packages such as \CRANpkg{dplyr} [@dplyr] and \CRANpkg{ggplot2} [@ggplot2].

The construction of diagnostic plots adopts the core concept in ggplot2:  grammar of graphics [@wickham2010layered]. In grammar of graphics, plots are not produced via calling the commands, named by the appearance of the plot, i.e., boxplot and histogram, but via the concept of stacked layers. Seeing plots as stacked layers empowers us to composite diagnostic plots with an emphasis on any variable in the data object without the redundancy of creating different commands for the same type of plots that highlights on different variables.

## Checking how hard the optimiser is working

A primary interest of diagnosing an optimiser is to study how it progressively finds its optimum. A simple treatment of plotting the index value across its natural order will cause the graph to be disproportional to the iteration since it usually takes much longer for an optimiser to find a better basis towards the end. Another option is to use summarisation in each iteration. Boxplot is a suitable candidate that can provide five points summary of each iteration and additional information can be separately added with new layers, for example, text information on the number of points searched in each iteration is added at the bottom of each iteration, the bases returned for interpolation are highlighted in larger size and linked, and information regarding the last iteration where no basis is returned is turned to grey scale. Furthermore, an option to switch from boxplot back to point geometry is helpful when the number of observation is small in one iteration and can be achieved via the `cutoff` argument. 

Figure \ref{fig:toy-search} shows the searches of two different optimisers: SA (left) and SAJO (right). Both optimisers progress quickly at the first few iterations, take longer to find better basis in the later iterations, and finish off when hitting the maximum number of try, 499 and 999, respectively. Also, the target basis found on the left panel always has an increased index value in each iteration while this is not the case for the plot on the right. This explains why in this example, the optimiser SAJO takes longer to find the final basis, but in more complicated scenarios, the feature of probabilistic acceptance allows a more holistic search of the basis space and is more likely to find the global optimum. 

```{r toy-search, fig.env = "widefigure", fig.cap="A comparison of search by two otimisers: SA and SAJO on a six-variable dataset \\code{boa6} with holes index on a 2D problem. Both optimisers finish when the maximum number of try is reached and attain similar index value in the end. Simulated annealing with jump out (SAJO) takes longer to get to the final basis because it allows for probabilistic acceptance of inferior basis, which is a feature useful for more complex problem."}
```

## Examining the optimisation progress {#toy-interp}

```{r toy-interp, fig.env = "widefigure", fig.cap = "Trace plots of the bases on the interpolation path for two optimisers with the same configuration as the previous figure. The right path shows that to reach a lower target basis, the interpolation actually first passes some higher basis and this could potentially be information to be incorporated into the optimisers."}
```

Viewing the index value of points on the interpolation path is another interest to the analysts since the projection on these bases will be played by the tour animation and it provides further information on how the index value changes when moving from one target basis to another. Figure \ref{fig:toy-interp} presents the index value against time on the interpolation path for the two optimisers with same configuration as \ref{fig:toy-search}. From Figure \ref{fig:toy-interp}, we further know that to reach a target basis with lower index value, the optimiser plotted on the right  first passes some bases with higher index value during the interpolation. If the bases on the interpolation path can be incorporated into the optimisation, we may be able to find the optimal basis quicker.

## Understanding the optimiser's coverage of the search space


```{r toy-pca, fig.height=3, fig.cap = "1D projection on the 5-variable dataset \\code{boa5} with two optimisers: Simulated Annealing (SA) and Pseudo Derivative (PD). All the bases in pseudo derivative has been flipped positive so that the two paths finish close to each other and to the theoretical best."}
```

Apart from checking the progression of an optimiser, another interesting aspect of the diagnostics is to visualise how the visited bases looks like in its parameter space. Given the orthonormality constraint, the space of projection bases $\mathbf{A}_{p \times d}$ is a $p \times d$ dimension sphere and even with 1D projection, the basis space is high-dimensional. Dimension reduction methods, i.e. principal component analysis can be used to present the bases on 2D space with appropriate annotation to highlight the different key components of the optimisation.

In a projection pursuit guided tour optimisation, there are 7 different components that are worth-noticing in the exploration: 1) The spherical space of all the basses; 2) The starting basis of an optimisation routine; 3) The anchor points where a new target basis is found; 4) The search basis that an optimiser has evaluated; 5) The interpolation path that constructed by the guided tour; 6) The annotation of the interrupted path and 7) The theoretical best basis, if applicable. Despite all have to be displayed on the diagnostic plot, these components differ in their importance with the most interesting ones being the starting point, anchor points and the interpolation path.

Various other components can be down-played via size, transparency, linetype and proper approximation. The basis space is generated via random bases on the original high dimensional space via the CRAN package \CRANpkg{geozoo} [@geozoo] and projected down to 2D. Displaying the search space with hundreds of randomly generated dots can be dizzy, especially when the more important start, anchor and search bases are also displayed with point geometry. Given the fact that the basis space is a circle when projected to 2D, we can estimate the center and radius with all the bases recorded in the data object. Comparing to the start and anchor points, search points are less important as they fail to improve on the index value during the optimisation and hence an adjusted size and transparency are applied to them. A potential scenario that could happen is that two optimisers can start at the same basis but finishes with bases in opposite signs. This sign difference would not make a difference in the projected data but the PCA projection of the bases will display the finish points symmetric to each other, which can be disturbing for comparing the path close to the finish. One solution is to flip the sign of one optimiser so that they start at symmetric position but finish close to each other. To remind the analyst of this symmetry in the PCA space, a light dashed line is drawn to connect the starting points of two paths. The theoretical best basis is available if the data is simulated and can serve as a guide on how the optimisers progress to the final basis. Also, the search paths also have an increasing alpha hue from start to finish to indicate the direction of the optimsation when the theoretical guide is not available. To allow flexible annotation in different scenarios, a range of arguments are made available to adjust in the main plotting function \code{explore\_space\_pca()}.

Figure \ref{fig:toy-pca} shows an example of the PCA plot on the same optimisers on a 1D projection problem with 5 variables.


## Animating the diagnostic plots

```{r toy-pca-animated, fig.cap = "A selected number of frames from the animated PCA plots. With animation, it is easier to track the progression of each optimiser from the start to finish. A full video of this aimation can be found at \\url{https://vimeo.com/user132007430/review/504242845/b73f37175a}"}
```

Animated plots can be informative in diagnostics, especially in the case of PCA plot when the starting and ending of the search is not clear. Figure \ref{fig:toy-pca-animated} shows six frames of an animated version of Figure \ref{fig:toy-pca} and this time, it shows that `search_better` finds the optimum quicker than `search_geodesic`.

## The tour looking at itself

```{r toy-tour, fig.cap="A selected number of frames from the tour animation for viewing the 5D space of all the projection bases. The tour animation allows for a more holistic view of the full space in high dimensions from different angles. A full video of this aimation can be found at \\url{https://vimeo.com/user132007430/review/504328122/9be84db563}"}
```

While viewing the bases on the reduced space via PCA shed some lights on the space the optimisers have explored, the visualisation on the original $p \times d$ dimension enables a more holistic stereoscopic view of the search. To view a high dimensional ($d \ge 3$) object on a screen, an approach is to play the rotation of the object in animation and this can be done via a regular grand tour. Compared to the PCA plot, the animated rotation (tour) displayed in Figure \ref{fig:toy-tour} gives a more well-rounded view of the search and one can view the curved region of the tour path from different angles, which may not be presented in the PCA plot. Also the grand tour animation encompasses the PCA projection since the rotation from PCA is just one angle that maximises the variance of the bases and the grand tour produces a sequence of angles that view the search from different directions. As an evidence, the last frame in Figure \ref{fig:toy-tour} is a frame select from the tour animation that is close to the PCA angle and the projection looks similar to the one in Figure \ref{fig:toy-pca}. 

# Diagnosing an optimiser {#application}

For a particular index function, the best algorithm to optimise depends on the character of the index and the data. If the index function is smooth and has a single maximum, all of the three algorithms introduced above can find the maximum.  When multiple optima are present, simulated annealing (SA) may get stuck at a local maximum.  In the case where the index function is non-smooth, pseudo derivative search may even fail to find the maximum. In this section, examples will be presented to outline how the diagnostic plots can be used to compare the performance of optimisers.

## Simulation setup

Random variables with different distributions have been simulated and the distributional form of each variable is presented in Equations \ref{eq:sim-norm} to \ref{eq:sim-x7}. Variable `x1`, `x8`, `x9` and `x10` are normal noise with zero mean and unit variance and `x2` to `x7` are normal mixtures with varied weights and locations. All the variables have been scaled to have an overall unit variance before running the projection pursuit.

\begin{align}
x_1 \overset{d}{=} x_8 \overset{d}{=} x_9 \overset{d}{=} x_{10}& \sim \mathcal{N}(0, 1) \label{eq:sim-norm} \\
x_2 &\sim 0.5 \mathcal{N}(-3, 1) + 0.5 \mathcal{N}(3, 1)\label{eq:sim-x2}\\
\Pr(x_3) &= 
\begin{cases}
0.5 & \text{if $x_3 = -1$ or $1$}\\
0 & \text{otherwise}
\end{cases}\label{eq:sim-x3}\\
x_4 &\sim 0.25 \mathcal{N}(-3, 1) + 0.75 \mathcal{N}(3, 1) \label{eq:sim-x4}\\
x_5 &\sim \frac{1}{3} \mathcal{N}(-5, 1) + \frac{1}{3} \mathcal{N}(0, 1) + \frac{1}{3} \mathcal{N}(5, 1)\label{eq:sim-x5}\\
x_6 &\sim 0.45 \mathcal{N}(-5, 1) + 0.1 \mathcal{N}(0, 1) + 0.45 \mathcal{N}(5, 1)\label{eq:sim-x6}\\
x_7 &\sim 0.5 \mathcal{N}(-5, 1) + 0.5 \mathcal{N}(5, 1) 
\label{eq:sim-x7}
\end{align}

## A problem of non-monotonicity {#monotonic}

A non-monotonic interpolation path has already been given in Figure \ref{fig:toy-interp}: to get to the target basis from the current one, the interpolator may have passed a basis with even higher index value. This motivates the design of an interruption to check whether higher index value can be reached during the interpolation. If such a basis can be found, the optimiser will start the next iteration from that basis instead of the target one. Figure \ref{fig:interruption} contrasts the trace with and without the interruption for the simulated annealing optimiser and it can be observed that while the first two interpolation are identical, a better basis has been found than the target one in the third interpolation. Rather than starting the next iteration from the "target basis" on Time 62, as shown in the left panel, the interruption, on the right panel, starts the next iteration from the "interpolation basis" on Time 61. The interruption has also detected better basis on the fourth interpolation. With the interruption, the optimiser is able to find the final basis with a higher index value (0.865 vs. 0.879).

```{r interruption, fig.env = "widefigure", fig.cap = "Trace plot of 2D projection on \\code{boa6} data with holes index optimised by optimiser SA. The purpose of the plot is to compare the effect of implementing the interruption that avoid fully interpolating to the target basis if a better one can be found during the interpolation. With this implementation, the optimiser is able to find a better final basis."}
```

## Close but not close enough


```{r polish, fig.height= 2.5, fig.cap = "Two-D projection on \\code{boa6} data with holes index optimised by pseudo derivative. The left panel shows the final projected data before polish and the right panel shows the one after. The separation of the clusters on the y axis becomes sharper after the polish. "}
```

Once the final basis has been found by an optimiser, one may want to push further in the close neighbourhood to find an even better basis. A polish search takes the final basis as the start of a new guided tour to search for local breakthrough. 

The polish algorithm is similar to the simulated annealing one with three main distinctions on 1) the number of candidate basis generated each time in the inner loop; 2) the search neighbourhood, `alpha`, and 3) the termination conditions. In simulated annealing, only one candidate basis $\mathbf{A}_l$ is generated every time while the polish algorithm allows for a specified number of basis to be evaluated for each repetition in the inner loop via an argument `n_sample`. The search neighbourhood in simulated annealing is reduced in the outer loop, that is, the alpha for one iteration of search remains the same, while in polish search, `alpha` is reduced once a set of candidate bases fails to make an improvement. To avoid the case the neighbourhood size becoming too small to generate meaningful search, more termination criteria have been added, apart from the original `max.tries` condition and  these include:

1) the distance between the basis found and the current basis needs to be larger than 1e-3;
2) the percentage change of the index value need to be larger than 1e-5; and
3) the alpha parameter on itself needs to be larger than 0.01

Figure \ref{fig:polish} presents the final projections found before (left) and after(right) applying the polish using the pseudo derivative optimiser. Before polishing, the four clusters are clearly shown while the edges of each cluster is relatively unshaved. The polish search start with this basis and on the right panel, the edges of the clusters are better trimmed.

## Seeing the signal in the noise 

The index function, up until this point, are all smooth, while this is not the case for all. A 1D projection function based on the Kolmogorov test, `norm_kol`, compares the projected data, $\mathbf{Y}_{n \times 1}$ to a randomly generated normal distribution, $y_n$ based on the empirical cumulated distribution function (ECDF). Let $F_{.}(u)$ be the ECDF function with the subscript $P$ and $u$ indicating the projected and randomly generated ECDF respectively,  the index is defined as: 

$$\max \left[F_{P}(u) - F_{y}(u)\right]$$
With a more complex index function, it is interesting to understand 1) if all the three optimisers can reach the optimum, 2) when a local optimum is presented, whether the simulated annealing algorithm can escape that local optimum and find the global one.


```{r noisy-better-geo, fig.env = "widefigure", fig.cap = "1D projection on 5\\-variable dataset \\code{boa5} with \\code{norm\\_kol} index. The trace plot shows the failure of pseudo derivative on non-smooth index. The PCA plot also shows that simulated annealing is searching the area close to the theoretical best while pseudo derivative can't couple with the non-continuity in the index."}
```

Figure \ref{fig:noisy-better-geo} presents the tracing plots of two optimisers: pseudo derivative and simulated annealing and as expected, the interpolated path is no longer smooth in either case. There is barely any improvement on the index value in the pseudo derivative algorithm while the simulated annealing algorithm has managed to get close to the index value of the theoretical best, indicated by the horizontal dashed line. In the PCA plot, simulated annealing is able to move gradually towards the theoretical best while the pseudo derivative method breaks after the initial few attempts.

```{r kol-result, fig.height = 6, fig.cap="1D projection of data \\code{boa6} on \\code{norm\\_kol} index,  optimised by SA and SAJO. Two different neighbourhood size of 0.5 and 0.7 is used. The finish point of each path is larger than the start point and optimums are separately labelled with * representing the global optimum and x the local optimum The color of the path is based on whether the optimiser finds the global or local optimum, after polishing."}
```

The next experiment compares the two simulated annealing optimisers, SA and SAJO,  for 20 simulation paths. Two search neighbourhood sizes, `alpha`, of 0.5 and 0.7 are also compared to understand the effect of neighbourhood size where a larger value indicating a wider search. Figure \ref{fig:kol-result} shows 80 (20 * 4) PCA projected interpolation paths faceted by optimiser and alpha. Several results can be drawn through the comparison: 1) With optimiser SA and a 0.5 neighbourhood search size, despite being the simplest and fastest, the optimiser fails to optimise for three instances where they finish neither near the local nor the global optimum. 2) With the same optimiser but a larger neighbourhood size of 0.7, more seeds have found the global optimum. 3) Comparing between the optimiser SA and SAJO with the same search size of 0.5, SAJO finishes closer to the theoretical for the ones that find the local optimum. This indicates that it is working harder to examine if there is other optimum in the space but a small search neighbourhood size makes it hard to conduct a holistic search of the whole space. 4) With optimiser SAJO and a larger neighbourhood size of 0.7, more seeds have found the global optimum. Also some finish points are getting closer to the position of the theoretical best, as comparing with the 0.5 neighbourhood size, and this would indicate fewer work for the subsequent polish.

# Implementation {#implementation}

The implementation of this projection has been divided into two packages: the data collection object is implemented in the existing CRAN package \CRANpkg{tourr} [@tourr] while the optimiser diagnostics have been implemented in a new package, \pkg{ferrn}. When a guided tour is run, the users can choose if the data from optimisation should be collected via the `verbose` argument. Once the data object has been obtained, the package, \pkg{ferrn}, can provide four diagnostic plots as shown in Section \ref{vis-diag}. The structure of package functionality has been listed below.

- Main plotting functions: 

  - \code{explore\_trace\_search()} produces summary plots, as shown in Figure \ref{fig:toy-search}
  - \code{explore\_trace\_interp()}produces trace plots for the interpolation points, as shown in Figure \ref{fig:toy-interp}
  - \code{explore\_space\_pca()} produces plots of projection basis on the reduced space by PCA, as shown in Figure \ref{fig:toy-pca}. Animated version in Figure \ref{fig:toy-pca-animated} can be turned on via the argument `animate = TRUE`
  - \code{explore\_space\_tour()} produces animated tour view on the full space of the projection bases, as shown in Figure \ref{fig:toy-tour}.

- \code{get\_*()} extracts and manipulates certain components from the existing data object.
  - \code{get\_best()} extracts the best basis found in the data object
  - \code{get\_start()} extracts the starting basis
  - \code{get\_interp()} extracts the observations in the interpolation
  - \code{get\_interp\_last()} extracts the end observations of the interpolation in each iteration
  - \code{get\_anchor()} extracts the target observations found by the optimiser
  - \code{get\_search()} extracts the search observations evaluated by the optimiser
  - \code{get\_search\_count()} produces the summary table of the number of observation in each iteration
  - \code{get\_center()} produces the center point of the basis space estimated by the starting points
  - \code{get\_space\_param()} produces the coordinates of the center and radius of the basis space
  - \code{get\_theo()} extracts the theoretical observations from the data object
  - \code{get\_interrupt()} extract the end point of the interpolation and the target point when an interruption happens
  - \code{get\_basis\_matrix()}: flattens all the bases into a matrix

- \code{bind\_*()} incorporates additional information outside the tour optimisation into the data object.
  - \code{bind\_theoretical()} incorporates the best possible basis to the existing data object with the supply of the index function and original data for producing the index value. 
  - \code{bind\_random()} generates 1000 points on the high dimensional surface of a sphere and binds it to the existing data object and output as a tibble object. 
  - \code{bind\_random\_matrix()} binds the points to the basis matrix. 

- Utilities
  - \code{add\_*()} are internal wrapper functions that facilitate the composition of PCA plot
  - \code{theme\_fern()} and \code{format\_label()} for better display of the grid lines and axis formatting
  - \code{clean\_method()} for clean up the name of the optimisers
  - \code{botanical\_palettes} is a collection of color palettes from Australian native plants. Quantitative palettes include daisy, banksia and cherry and sequential palettes contain fern and acacia.
  - \code{botanical\_pal()} as the color interpolator 
  - \code{scale\_color\_botanical()} is a ggplot2 scale for botanical palettes. 

# Conclusion

This paper has illustrated setting up a data object that can be used for diagnosing a complex optimisation procedure. The ideas were illustrated using the optimisers available for projection pursuit guided tour. Here the constraint is the orthornormality condition of the projection bases. The approach used here could be broadly applied to understand other constrained optimisers.

<!-- something about the diagnostic plots  -->
Four diagnostic plots have been introduced to investigate the progression and the projection space of an optimiser. The implementation of these visualisations is designed to be easy-to-use with each plot can be produced with a simple supply of the data object. More advanced users may decide to modify on top of the basic plots or even build their own. 

<!-- summarise what we have done and what might be done in the future. -->
Most of the work in this project has been translated into code in two packages: the collection of the data object is implemented in the existing \CRANpkg{tourr}[@tourr] package; manipulation and visualisation of the data object are implemented in the new \pkg{ferrn} package. Equipped with handy tools to diagnose the performance of optimisers, future work can extend the diagnostics to a wider range of index functions, i.e. scagnostics, association, and information index [@laa2020using] and understand how the optimisers behave for index functions with different structures.

# Acknowledgements

This article is created using \CRANpkg{knitr}[@knitr] and \CRANpkg{rmarkdown} [@rmarkdown]  in R. The source code for reproducing this paper can be found at: \url{https://github.com/huizezhang-sherry/paper-tour-vis}.

\bibliography{zhang-cook-laa-langrene-menendez}
