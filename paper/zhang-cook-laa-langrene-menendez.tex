% !TeX root = RJwrapper.tex
\title{Visual Diagnostics for Constrained Optimisation with Application to
Guided Tours}
\author{by H.Sherry Zhang, Dianne Cook, Ursula Laa, Nicolas Langrené, Patricia Menéndez}

\maketitle

\abstract{%
Guided tour searches for interesting low-dimensional views of
high-dimensional data via optimising a projection pursuit index
function. The first paper of projection pursuit by
\citet{friedman1974projection} stated that ``the technique used for
maximising the projection index strongly influences both the statistical
and the computational aspects of the procedure.'' While much work has
been done in proposing indices in the literature, less has been done on
evaluating the performance of the optimisers. In this paper, we
implement a data collection object in the optimisation of projeciton
pursuit guided tour and introduce visual diagnostics based on the data
object collected. These diagnostics and workflows can be applied to a
broad class of optimisers, to assess their performance. An R package,
\pkg{ferrn}, has been created to implement the diagnostics.
}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Visualisation is widely used in exploratory data analysis
\citep{tukey1977exploratory, unwin2015graphical, healy2018data, wilke2019fundamentals}.
Presenting information in graphics often unveils information that would
otherwise not be aware of and provides a more comprehensive
understanding of the problem at hand. Task specific tools such as
\citet{li2020visualizing} show how visualisation can be used to
understand the behaviour of neural network on classification models, but
no general visualisation tool is available for diagnosing optimisation
procedures. The work presented in this paper brings visualization tools
into optimisation problems with an aim to better understand the
performance of the optimisers in practice.

The goal of continuous optimisation is to find the best solution within
the space of all feasible solutions where typically the best solution is
decided by an objective function. Broadly speaking, optimization can be
unconstrained or constrained \citep{kelley1999iterative}. The
unconstrained problem can be formulated as a minimization (or
maximization) problem such as \(\min_{x} f(x)\) where
\(f:\mathbb{R}^n \rightarrow \mathbb{R}\) is an objective function with
certain properties defined in an \(L^p\) space. In this case, solutions
rely on gradient descent or ascent methods. In the constrained
optimization problem additional restrictions are introduced via a set of
functions that can be convex or non-convex:
\(g_i:\mathbb{R}^n \rightarrow \mathbb{R}\) for \(i = 1, \ldots k\) and
hence the problem can be written as \(\min_{x} f(x)\) \emph{subject to}
\(g_i(x) \leq 0\). Here methods such as multipliers and convex
optimization methods including linear and quadratic programming can be
used.

The focus of this paper is on the optimisation problem arising in the
projection pursuit guided tour \citep{buja2005computational}, an
exploratory data analysis tool used for detecting interesting structures
in high-dimensional data through a set of lower-dimensional projections
\citep{cook2008grand}. The goal of the optimisation is to identify the
projection, characterised by the projection basis, that gives the most
interesting low-dimensional view. The interestingness of the structure
is defined by the index function, a function of the projection basis.

The optimization challenges encountered in the projection pursuit guided
tour problem are common to those of optimization in general. Examples of
those include the existence of multiple optima (local and global), the
trade off between computational burden and proximity to the optima,
dealing with noisy objective functions that might be non-smooth and
non-differentiable \citep{jones1998efficient}. Those are not unique to
this context and therefore the visualization tools and optimization
methods presented in this paper can be easily applied to any other
optimization problems.

The remainder of the paper is organised as follows. Section \ref{optim}
provides an overview of optimisation methods, specifically line search
methods. Section \ref{tour} reviews projection pursuit guided tour,
defines the optimisation problem and outlines three existing algorithms.
Section \ref{vis-diag} presents the new visual diagnostics. A data
structure is defined to capture information during the optimisation, and
different diagnostic plots are designed with the data collected. Section
\ref{application} shows applications of how these diagnostic plots can
be used to discover interesting aspects of different optimisers and
guide improvement to the existing algorithms. Finally, Section
\ref{implementation} describes the R package: \pkg{ferrn}, that
implements the visual diagnostics.

\hypertarget{optim}{%
\section{Optimisation methods}\label{optim}}

Optimization problems are ubiquitous in many areas of study. While in
some cases analytical solutions can be found, the majority of problems
rely on numerical methods to find the optimal solution. These numerical
methods follow iterative approaches that aim at finding the optimum by
progressively improving the current solution until a desirable accuracy
is achieved. Although this principle seems uncomplicated, a number of
challenges arise such as the possible existence of multiple maxima
(local and global), constraints and noisy objective function, and the
trade-off between desirable accuracy and computational burden. In
addition, the optimization results might depend on the algorithm
starting values, affecting the consistency of results.

Optimization methods can be divided into various classes, such as global
optimisation \citep{kelley1999iterative, fletcher2013practical}, convex
optimisation \citep{boyd2004convex} or stochastic optimisation
\citep{nocedal2006numerical}. Our interest is on constrained
optimization \citep{bertsekas2014constrained} as defined in the
introduction section, and assuming it is not possible to find a solution
to the problem in the way of a closed-form. That is, the problem
consists of finding the minimum or maximum of a function \(f \in L^p\)
in the constrained \(\mathbb{A}\) space.

A large class of methods utilises the gradient information of the
objective function to perform the optimisation iterations, with the most
notable one being the gradient ascent (descent) method. Although
gradient optimization methods are popular, they rely on the availability
of the objective function derivatives and on the complexity of the
constraints. Derivative-free methods, which do not rely on the knowledge
of the gradient, are more generally applicable. Derivative-free methods
have been developed over the years, where the emphasis is on finding, in
most cases, a near optimal solution. Examples of those include response
surface methodology \citep{box1951experimental}, stochastic
approximation \citep{robbins1951stochastic}, random search
\citep{fu2015handbook} and heuristic methods
\citep{sorensen2013metaheuristics}. Later, we will present a simulated
annealing optimisation algorithm, which belongs to the class of random
search methods, for optimisation with the guided tour.

A common search scheme utilised by both derivative-free methods and
gradient methods is line search. In line search methods, users are
required to provide an initial estimate \(x_{1}\) and, at each
iteration, a search direction \(S_k\) and a step size \(\alpha_k\) are
generated. Then one moves on to the next point following
\(x_{k+1} = x_k + \alpha_kS_k\) and the process is repeated until the
desire convergence is reached. While gradient-based methods choose the
search direction by the gradient, derivative-free methods uses local
information of the objective function to determine the search direction.
The choice of step size also needs considerations, as inadequate step
sizes might prevent the optimisation method to converge to an optimum.
An ideal step size can be chosen via finding the value of
\(\alpha_k \in \mathbb{R}\) that maximises \(f(x_k + \alpha_kS_k)\) with
respect to \(\alpha_k\) at each iteration.

Several R implementations address optimization problems with both
general purpose as well as task specifics solvers. The most prominent
one within the general solvers is \code{optim()} in the \CRANpkg{stats}
\citep{stats} package, which provides both gradient-based and
derivative-free optimisation functions. Another general solver
specialised in non-linear optimisation is \CRANpkg{nloptr}
\citep{nloptr}. Specific solvers for simulated annealing includes
\code{optim(..., method  "SANN")} and package \CRANpkg{GenSA}
\citep{gensa} that deals with more complicated objective functions. For
other task specific solvers, readers are recommended to visit the
relevant sections in CRAN task review on
\ctv{optimisation and mathematical programming}
\citep{crantaskreviewoptim}.

\hypertarget{tour}{%
\section{Projection pursuit guided tour}\label{tour}}

Projection pursuit guided tour combines two different methods
(projection pursuit and guided tour) in exploratory data analysis.
Projection pursuit, coined by \citet{friedman1974projection}, detects
interesting structures (e.g.~clustering, outliers and skewness) in
multivariate data via low dimensions projection. Guided tour is one
variation of a broader class of data visualisation methods, tour, which
visualises high-dimensional data through a series of animated
projection.

Notation is first established before defining projection pursuit. Let
\(\mathbf{X}_{n \times p}\) be the data matrix with \(n\) observations
in \(p\) dimensions. A d-dimensional projection is a linear
transformation from \(\mathbb{R}^p\) into \(\mathbb{R}^d\), and defined
as \(\mathbf{Y} = \mathbf{X} \cdot \mathbf{A}\), where
\(\mathbf{Y}_{n \times d}\) is the projected data and
\(\mathbf{A}_{p\times d}\) is the projection basis Define
\(f: \mathbb{R}^{n \times d} \mapsto \mathbb{R}\) to be an index
function that maps the projected data \(\mathbf{Y}\) onto an scalar.
This is commonly known as the projection pursuit index function, or just
index function, and is used to measure the ``interestingness'' of a
given projection. A number of index functions have been proposed in the
literature to detect different data structures
\citep[friedman1974projection;][]{hall1989polynomial, cook1993projection, posse1995projection}
and to classify groups \citep{lee2005projection, lee2010projection}.

As a general visualisation method, tour produces animations of high
dimensional data via rotations low dimension planes. Different tour
types choose these planes differently: grand tour \citep{cook2008grand}
selects the planes randomly to provide a general overview; manual tour
\citep{cook1997manual} gradually phases in and out one variable to
understand the contribution of that variable in the projection. Guided
tour, the main interest of this paper, chooses the planes with the aid
of projection pursuit, to gradually reveal the most interesting
projection. Given a random start, projection pursuit iteratively finds
bases with higher index values and the guided tour constructs the
geodesic interpolation between these planes to form a tour path. Figure
\ref{fig:tour-path} shows a sketch of the tour path where the blue
frames are produced by the projection pursuit optimisation and the white
frames are interpolations between the blue frames. Mathematical details
of the geodesic interpolation can be found in
\citet{buja2005computational}. The tour method has been implemented in
the R package \CRANpkg{tourr} \citep{tourr}.

\begin{Schunk}
\begin{figure}

{\centering \includegraphics[width=0.5\linewidth,height=0.2\textheight]{/Users/hzha400/Documents/PhD/research/paper-tour-vis/img/tour-path} 

}

\caption[Each square (frame) represents the projected data with a corresponding basis]{Each square (frame) represents the projected data with a corresponding basis. Blue frames are found by an optimisation algorithm iteratively whilst the white frames are constructed between two blue frames by geodesic interpolation.}\label{fig:tour-path}
\end{figure}
\end{Schunk}

\hypertarget{tour-optim}{%
\subsection{Optimisation in the tour}\label{tour-optim}}

The optimisation problem in projection pursuit is stated as follows:
Given a randomly start basis \(\mathbf{A}_1\), projection pursuit finds
the final projection basis \(\mathbf{A}_T\) that satisfies the following
optimisation problem:

\begin{align}
&\arg \max_{\mathbf{A} \in \mathcal{A}} f(\mathbf{X} \cdot \mathbf{A}) \\
&s.t.  \mathbf{A}^{\prime} \mathbf{A} = I_d
\end{align}

\noindent where \(I_d\) is the \(d\)-dimensional identity matrix and the
constraint requires the projection bases \(\mathbf{A}\) to be
orthonormal. Several features of this optimisation are worth-noticing.
1) The optimisation is constrained and the space of the bases is a
d-dimensional sphere. 2) The objective function may not be
differentiable for a constructed index function. 3) Although finding the
global maximum is the goal of an optimisation problem, it is also
interesting to inspect local maximum in projection pursuit since it
could present unexpected interesting projections. 4) The optimisation
procedure needs to be easy to compute since the tour animation needs to
be played in real-time during the optimisation.

\hypertarget{existing-algorithms}{%
\subsection{Existing algorithms}\label{existing-algorithms}}

Three optimisers have been implemented in the \CRANpkg{tourr}
\citep{tourr} package: simulated annealing (SA), simulated annealing
with jump out (SAJO), and pseudo derivative (PD). Simulated annealing
(SA) is a random search optimiser that samples a candidate basis
\(\mathbf{A}_{l}\) in the neighbourhood of the current basis
\(\mathbf{A}_{\text{cur}}\) by
\(\mathbf{A}_{l} = (1- \alpha)\mathbf{A}_{\text{cur}} + \alpha \mathbf{A}_{\text{rand}}\)
where \(\alpha\) controls the radius of the sampling neighbourhood and
\(\mathbf{A}_{\text{rand}}\) is generated randomly. \(\mathbf{A}_{l}\)
is then orthonormalised to fulfil the basis constraint. If
\(\mathbf{A}_{l}\) has an index value higher than the current basis
\(\mathbf{A}_{\text{cur}}\), the optimiser outputs \(\mathbf{A}_{l}\)
for guided tour to construct an interpolation path. The neighbourhood
parameter \(\alpha\) is adjusted by a cooling parameter:
\(\alpha_{j+1} = \alpha_j * \text{cooling}\) before the next iteration
starts. The optimiser terminates when the maximum number of iteration
\(l_{\max}\) is reached before a better basis can be found. The
algorithm of SA is summarised in Algorithm \ref{random-search}.
\citet{posse1995projection} has proposed a slightly different cooling
scheme by introducing a halving parameter \(c\). In his proposal, the
\(\alpha\) is only adjusted if the last iteration takes more than \(c\)
times to find a better basis.

\begin{algorithm}
\SetAlgoLined
  \SetKwInOut{input}{input}
  \SetKwInOut{output}{output}
    \input{$f(.)$, $\alpha_1$, $l_{\max}$, $\text{cooling}$} 
    \output{$\mathbf{A}_{l}$}
    generate random start $\mathbf{A}_1$ and set $\mathbf{A}_{\text{cur}} \coloneqq \mathbf{A}_1$, $I_{\text{cur}} = f(\mathbf{A}_{\text{cur}})$, $j = 1$\;
  \Repeat{$\mathbf{A}_l$ is too close to $\mathbf{A}_{\text{cur}}$ in terms of geodesic distance}{
   set $l = 1$\;
  \Repeat{$l > l_{\max}$ or $I_{l} > I_{\text{cur}}$}{
    generate $\mathbf{A}_{l} = (1- \alpha_j)\mathbf{A}_{\text{cur}} + \alpha_j \mathbf{A}_{\text{rand}}$ and orthogonalise $\mathbf{A}_{l}$\;
    compute $I_{l}  = f(\mathbf{A}_{l})$\;
    update $l = l + 1$\;
  }
  update $\alpha_{j+1} = \alpha_j * \text{cooling}$\;
  construct the geodesic interpolation between $\mathbf{A}_{\text{cur}}$ and $\mathbf{A}_l$\; 
  update $\mathbf{A}_{\text{cur}} = \mathbf{A}_l$ and $j = j + 1$\;
}
  \caption{Simulated annealing (SA)}
  \label{random-search}
\end{algorithm}

Simulated annealing with jump out (SAJO)
\citep{kirkpatrick1983optimization, bertsimas1993simulated} uses the
same sampling process as SA but allows a probabilistic acceptance of a
basis with lower index value than the current. Given an initial value of
\(T_0\), the temperature at iteration \(l\) is defined as
\(T(l) = \frac{T_0}{\log(l + 1)}\). When a candidate basis fails to have
an index value larger than the current basis, SAJO gives it a second
chance to be accepted with probability
\[P= \min\left\{\exp\left[-\frac{\mid I_{\text{cur}} - I_{l} \mid}{T(l)}\right],1\right\}\]
where \(I_{(\cdot)}\) denotes the index value of a given basis. This
implementation allows the optimiser to make a move and explore the basis
space even the landing basis does have a higher index value and hence
enable the optimiser to jump out of a local optimum. The algorithm
\ref{simulated-annealing} highlights how SAJO differs from SA in the
inner loop.

\begin{algorithm}
\SetAlgoLined
\Repeat{$l > l_{\max}$ or $I_{l} > I_{\text{cur}}$ or $P > U$}{
    generate $\mathbf{A}_{l} = (1- \alpha_j)\mathbf{A}_{\text{cur}} + \alpha_j \mathbf{A}_{\text{rand}}$ and orthogonalise $\mathbf{A}_{l}$\;
    compute $I_{l}  = f(\mathbf{A}_{l})$, $T(l) = \frac{T_0}{\log(l + 1)}$ and $P= \min\left\{\exp\left[-\frac{I_{\text{cur}} -I_{l}}{T(l)}\right],1\right\}$\;
    draw $U$ from a uniform distribution: $U \sim \text{Unif(0, 1)}$\;
    update $l = l + 1$\;
  }
  \caption{Simulated annealing with jump out (SAJO)}
  \label{simulated-annealing}
\end{algorithm}

Pseudo derivative (PD) search \citep{cook1995grand} uses a different
strategy than SA and SAJO. Rather than randomly sample the basis space,
PD first computes a search direction by evaluating vases close to the
current basis. A step size is then chosen along the geodesic direction
by another optimisation over an 90 degree angle from \(-\pi/4\) to
\(\pi/4\). The resulting candidate basis \(\mathbf{A}_{**}\) is returned
for the current iteration if it has a higher index value than the
current one. Algorithm \ref{search-geodesic} summarises the inner loop
of the PD.

\begin{algorithm}
\SetAlgoLined
\Repeat{$l > l_{\max}$ or $p_{\text{diff}} > 0.001$}{
  generate $n$ random directions $\mathbf{A}_{\text{rand}}$ \;
  compute $2n$ candidate bases deviate from $\mathbf{A}_{\text{cur}}$ by an angle of $\delta$ while ensure orthogonality\;
  compute the corresponding index value for each candidate bases\;
  determine the search direction as from $\mathbf{A}_{\text{cur}}$ to the candidate bases with the largest index value\;
  determine the step size via optimising the index value on the search direction over a 90 degree window\;
  find the optima $\mathbf{A}_{**}$ and compute $I_{**} = f(\mathbf{A}_{**})$, $p_{\text{diff}} = (I_{**} - I_{\text{cur}})/I_{**}$\;
  update $l = l + 1$\;
}
\caption{Pseudo derivative (PD)}
\label{search-geodesic}
\end{algorithm}

\hypertarget{vis-diag}{%
\section{Visual diagnostics}\label{vis-diag}}

Before making diagnostic plots, information generated by the
optimisation need to be collected and a pre-defined data structure
solidifies the information being collected and streamlines the
subsequent plot-making.

\hypertarget{data-structure-for-diagnostics}{%
\subsection{Data structure for
diagnostics}\label{data-structure-for-diagnostics}}

Three main pieces of information are recorded for the projection pursuit
optimisers: 1) projection bases \(\mathbf{A}\), 2) index values \(I\),
and 3) state \(S\). For SA and SAJO, possible state include
\texttt{random\_search}, \texttt{new\_basis}, and
\texttt{interpolation}. Pseudo derivative (PD) has a wider variety of
state including \texttt{new\_basis}, \texttt{direction\_search},
\texttt{best\_direction\_search}, \texttt{best\_line\_search}, and
\texttt{interpolation}. Multiple iterators index the information
collected at different levels: \(t\) is a unique identifier prescribing
the natural ordering of each observation; \(j\) and \(l\) are the
counter of the outer and inner loop respectively. Other parameters of
interest recorded include \texttt{method} that tags the name of the
optimiser, and \texttt{alpha} that indicates the sampling neighbourhood
size for searching observations. A matrix notation of the data structure
is presented in Equation \ref{eq:data-structure}.

\begin{equation}
\renewcommand\arraystretch{2}  % default value: 1.0
\left[
\begin{array}{c|ccc|cc|cc}
t & \mathbf{A} & I & S & j &  l  & V_{1} & V_{2}\\
\hline
1 & \mathbf{A}_1 & I_1 & S_1 & 1 & 1 & V_{11} & V_{12}\\
\hline
2 & \mathbf{A}_2 & I_2 & S_2 & 2 & 1  & V_{21}  & V_{22}\\
3 & \mathbf{A}_3 & I_3 & S_3 & 2 & 2  & V_{31}  & V_{32}\\
\vdots & \vdots &\vdots &\vdots  &\vdots & \vdots &\vdots  &\vdots\\
\vdots & \vdots & \vdots &\vdots & 2 & l_2 & \vdots  & \vdots\\
\hline
\vdots &\vdots & \vdots &\vdots & 2  & 1& \vdots & \vdots\\
\vdots &\vdots &\vdots &\vdots & 2 & 2& \vdots &  \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots & \vdots  &\vdots \\
\vdots &\vdots &\vdots &\vdots & 2 & k_2 &\vdots  & \vdots\\
\hline
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
\hline
\vdots & \vdots & \vdots &\vdots  & J &  1 & \vdots & \vdots \\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
T & \mathbf{A}_T & I_T &S_T  & J &  l_{J} & V_{T1}& V_{T2}\\
\hline
\vdots &\vdots & \vdots &\vdots & J  & 1& \vdots & \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots & \vdots  &\vdots \\
\vdots &\vdots &\vdots &\vdots & J & k_J &\vdots  & \vdots\\
\hline
\vdots& \vdots & \vdots & \vdots & J+1 & 1 & \vdots& \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
T^\prime & \mathbf{A}_{T^\prime} & I_{T^\prime} &S_{T^\prime}  & J+1 &  l_{J+1} & V_{T^\prime 1}& V_{T^\prime 2}\\
\end{array}
\right]
= 
\left[
\begin{array}{c}
\text{column name} \\
\hline
\text{search (start basis)} \\
\hline
\text{search} \\
\text{search} \\
\vdots \\
\text{search (accepted basis)} \\
\hline
\text{interpolate} \\
\text{interpolate} \\
\vdots \\
\text{interpolate} \\
\hline
\vdots \\
\hline
\text{search} \\
\vdots \\
\text{search (final basis)} \\
\hline
\text{interpolate} \\
\vdots \\
\text{interpolate} \\
\hline
\text{search (no output)} \\
\vdots \\
\text{search (no output)} \\
\end{array}
\right]
\label{eq:data-structure}
\end{equation}

\noindent where \(T^{\prime} = T + k_{J}+ l_{J+1}\). Note that there is
no output in iteration \(J + 1\) since the optimiser doesn't find a
better basis in the last iteration and terminates. The final basis found
is \(A_T\) with index value \(I_T\).

The data structure constructed above meets the tidy data principle
\citep{wickham2014tidy} that requires each observation to form a row and
each variable to form a column. With tidy data structure, data wrangling
and visualisation can be significantly simplified by well-developed
packages such as \CRANpkg{dplyr} \citep{dplyr} and \CRANpkg{ggplot2}
\citep{ggplot2}.

The construction of diagnostic plots adopts the core concept of grammar
of graphics \citep{wickham2010layered} in ggplot2. In grammar of
graphics, plots are not produced by calling the commands, named by the
appearance of the plot, i.e., boxplot and histogram, but by the concept
of stacked layers. Seeing plots as stacked layers gives analysts the
freedom to composite plots with multiple elements on hands.

\hypertarget{checking-how-hard-the-optimiser-is-working}{%
\subsection{Checking how hard the optimiser is
working}\label{checking-how-hard-the-optimiser-is-working}}

A starting point of diagnosing an optimiser is to understand how many
search an optimiser has conducted. One may want to simply plot the index
value of the search points across its natural order, but a point
geometry may work well if each iteration has a similar number of points.
When some iterations have considerably more points than others, using a
point geometry over-emphasizes the iterations that have more search
points since these iterations will occupy the mast majority of the plot
space. An alternative is to summarise the search in each iteration using
boxplot and each iteration will then be spaced out equally.
Occasionally, one may still want to switch back to a point geometry if
the number of points is small in a particular iteration and this can be
achieved via the \texttt{cutoff} argument in the search plot function.
Additional annotations are added to facilitate better reading of the
plot and these includes\\
1) the number of points searched in each iteration can be added as text
label at the bottom of each iteration; 2) the anchor bases to
interpolate are connected and highlighted in a larger size; and 3) the
colour of the last iteration is in a grey scale to indicate no better
basis found in this iteration.

Figure \ref{fig:toy-search} shows an example of the search plot for SA
(left) and SAJO (right). Both optimisers quickly find better bases in
the first few iterations and then take longer to find one in the later
iterations. The anchor bases, the ones found with the highest index
value in each iteration, always have an increased index value in the
optimiser SA while this is not the case for SAJO. This feature gives SA
an advantage in this simple example to quickly find the optimum.

\begin{Schunk}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{/Users/hzha400/Documents/PhD/research/paper-tour-vis/figs/toy-search-1} 

}

\caption[A comparison of search by two otimisers]{A comparison of search by two otimisers: SA (left) and SAJO (right) on a 2D projection of a six-variable dataset, \code{boa6}, using the holes index. Both optimisers find the final basis with similar index value while it takes SAJO more iteration because the algorithm allows a probabilistic acceptance of bases with lower index value, as observed in iteration 4 and 6-8.}\label{fig:toy-search}
\end{figure}
\end{Schunk}

\hypertarget{toy-interp}{%
\subsection{Examining the optimisation progress}\label{toy-interp}}

\begin{Schunk}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{/Users/hzha400/Documents/PhD/research/paper-tour-vis/figs/toy-interp-1} 

}

\caption[Trace plots of the interpolating bases with the same optimisation routine as the previous figure]{Trace plots of the interpolating bases with the same optimisation routine as the previous figure. Both traces are smooth while the change of index value in each iteration may not be monotinic.}\label{fig:toy-interp}
\end{figure}
\end{Schunk}

Another interest of diagnostic is to examine how the index value
improves for interpolating bases since the projection on these bases is
played by the tour animation. Trace plots are created with plotting the
index value of the interpolation points against time. Figure
\ref{fig:toy-interp} presents the trace plot of the same optimisation
routine as Figure \ref{fig:toy-search}. The trace plot is smooth and an
interesting discovery is that in SAJO, the interpolation in iteration
6-8 is not monotonic. Sometimes, this non-monotonicity is acceptable
since the target basis does not necessarily have a higher index value
than the current one, while in some cases, the optimisation will be
better off if these intermediate basis can be used as the start of the
next iteration (see section \ref{monotonic} for a detailed discussion on
this).

\hypertarget{toy-pca}{%
\subsection{Understanding the optimiser's coverage of the search
space}\label{toy-pca}}

\begin{Schunk}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{/Users/hzha400/Documents/PhD/research/paper-tour-vis/figs/toy-pca-1} 

}

\caption[The PCA plot of 1D projection problem on the 5-variable dataset, \code{boa5}, using the holes index with two optimisers]{The PCA plot of 1D projection problem on the 5-variable dataset, \code{boa5}, using the holes index with two optimisers: SA and PD. All the bases in PD has been flipped and a grey dashed line has been annotated to indicate the symmetry of two starting bases.}\label{fig:toy-pca}
\end{figure}
\end{Schunk}

Apart from checking the search and progression of an optimiser, looking
at where the bases are positioned in the basis space is another
interest. Given the orthonormality constraint, the space of projection
bases \(\mathbf{A}_{p \times d}\) is a \(p \times d\) dimension sphere
and dimension reduction methods, i.e.~principal component analysis is
applied to first project all the bases onto a 2D space. In a projection
pursuit guided tour optimisation, there are various type of bases
involved: 1) The start basis; 2) The search bases that an optimiser
evaluated to produce the anchor bases; 3) The anchor bases that have the
highest index value in each iteration; 4) The interpolating bases on the
interpolation path; and finally 5) the end basis. The importance of
these basis differs and the most important ones are the start,
interpolating and end bases. Anchor and search bases can be turned on
with argument \texttt{details\ =\ TRUE}. Sometimes, two optimisers can
start with the same basis but finish with bases of opposite sign. This
happens because the projection is invariant to the sign difference of
the bases and so does the index value, however, this creates
difficulties for comparing the optimisers since the end bases will be
symmetric with respect to the origin. A sign flipping step is conducted
to flip the signs of all the bases in one routine if different
optimisations finish at opposite places.

Several annotations have been made to help understanding this plot. As
mentioned previously, the original basis space is a high dimensional
sphere and random bases on the sphere can be generated via the CRAN
package \CRANpkg{geozoo} \citep{geozoo}. Along with the bases recorded
during the optimisation and a zero basis, the parameters (centre and
radius) of the 2D space can be estimated. PCA is performed to get the
first two PC coordinates of all the bases and the centre of the circle
is the PC coordinates of the zero matrix, and radius is estimated as the
largest distance from the centre to the basis. The theoretical best
basis is known for simulated data and can be labelled to compare the how
close the end basis found by the optimisers is to the theoretical best.
Various aesthetics, i.e.~size, alpha and colour, are applicable to
emphasize critical elements and adjust for the presentation. For
example, anchor points and search points are less important and hence a
smaller size and alpha is used; Alpha can also be applied on the
interpolation paths to show the start to finish from transparent to
opaque.

Figure \ref{fig:toy-pca} shows the PCA plot of SA and PD for a 1D
projection problem. Both optimisers find the optimum but PD gets closer.
With PCA plot, one can visually appreciate the nature of these two
optimisers: PD first evaluates points in a small neighbourhood for a
promising direction, while SA evaluates points randomly in the search
space to search for the next target. There are dashed lines annotated
for SA and it describes the interruption of the interpolation, which
will be discussed in details in Section \ref{monotonic}.

\hypertarget{animating-the-diagnostic-plots}{%
\subsection{Animating the diagnostic
plots}\label{animating-the-diagnostic-plots}}

\begin{Schunk}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{/Users/hzha400/Documents/PhD/research/paper-tour-vis/figs/toy-pca-animated-1} 

}

\caption{Six selected frames from the animated PCA plots in the previous figure. With animation, the start and end position of each optimisation is easier to identify.  A full video of this aimation can be found at \url{https://vimeo.com/user132007430/review/504242845/b73f37175a}}\label{fig:toy-pca-animated}
\end{figure}
\end{Schunk}

Animation is another display to show how the search progresses from
start to finish in the space and an \texttt{animate\ =\ TRUE} argument
is used to enable the animation in the PCA plot. Figure
\ref{fig:toy-pca-animated} shows six frames from the animation of the
PCA plot in Figure \ref{fig:toy-pca}. An additional piece of information
one can learn from this animation is that SA finds its end basis quicker
than PD since SA finishes its search in the 5th frame while PD is still
making more progression.

\hypertarget{the-tour-looking-at-itself}{%
\subsection{The tour looking at
itself}\label{the-tour-looking-at-itself}}

\begin{Schunk}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{/Users/hzha400/Documents/PhD/research/paper-tour-vis/figs/toy-tour-1} 

}

\caption{Six selected frames from the tour animation for viewing the same two optmisations as previous two figures. The tour animation allows for an appreciation of the search bases in the original high dimensional space. A full video of this aimation can be found at \url{https://vimeo.com/user132007430/review/504328122/9be84db563}}\label{fig:toy-tour}
\end{figure}
\end{Schunk}

As mentioned previously, the original \(p \times d\) dimension space can
be simulated via randomly generated bases in \CRANpkg{geozoo}
\citep{geozoo} package. While the PCA plot projects the bases from the
direction that maximises the variance, the tour plot display of the
original high dimensional space from various directions using animation.
Figure \ref{fig:toy-tour} shows frames from the tour plot of the same
two optimisations in its original 5D space.

\hypertarget{forming-a-torus}{%
\subsection{Forming a torus}\label{forming-a-torus}}

Something to fill here \ldots{}

\hypertarget{application}{%
\section{Diagnosing an optimiser}\label{application}}

In this section, several examples will be presented to show how the
diagnostic plots discover something unexpected in projection pursuit
optimisation, and guide the implementation of new features.

\hypertarget{simulation-setup}{%
\subsection{Simulation setup}\label{simulation-setup}}

Random variables with different distributions have been simulated and
the distributional form of each variable is presented in Equations
\ref{eq:sim-norm} to \ref{eq:sim-x7}. Variable \texttt{x1}, \texttt{x8},
\texttt{x9} and \texttt{x10} are normal noise with zero mean and unit
variance and \texttt{x2} to \texttt{x7} are normal mixtures with varied
weights and locations. All the variables have been scaled to have an
overall unit variance before projection pursuit.

\begin{align}
x_1 \overset{d}{=} x_8 \overset{d}{=} x_9 \overset{d}{=} x_{10}& \sim \mathcal{N}(0, 1) \label{eq:sim-norm} \\
x_2 &\sim 0.5 \mathcal{N}(-3, 1) + 0.5 \mathcal{N}(3, 1)\label{eq:sim-x2}\\
\Pr(x_3) &= 
\begin{cases}
0.5 & \text{if $x_3 = -1$ or $1$}\\
0 & \text{otherwise}
\end{cases}\label{eq:sim-x3}\\
x_4 &\sim 0.25 \mathcal{N}(-3, 1) + 0.75 \mathcal{N}(3, 1) \label{eq:sim-x4}\\
x_5 &\sim \frac{1}{3} \mathcal{N}(-5, 1) + \frac{1}{3} \mathcal{N}(0, 1) + \frac{1}{3} \mathcal{N}(5, 1)\label{eq:sim-x5}\\
x_6 &\sim 0.45 \mathcal{N}(-5, 1) + 0.1 \mathcal{N}(0, 1) + 0.45 \mathcal{N}(5, 1)\label{eq:sim-x6}\\
x_7 &\sim 0.5 \mathcal{N}(-5, 1) + 0.5 \mathcal{N}(5, 1) 
\label{eq:sim-x7}
\end{align}

\hypertarget{monotonic}{%
\subsection{A problem of non-monotonicity}\label{monotonic}}

An example of non-monotonic interpolation has been given in Figure
\ref{fig:toy-interp}: a path that passes bases with higher index value
than the target one. For SAJO, a non-monotonic interpolation is
justified since target bases do not necessarily have a higher index
value than the current one, while this is not the case for SA. The
original trace plot for a 2D projection problem, optimised by SA, is
shown on the left panel of Figure \ref{fig:interruption} and one can
observe clearly that the non-monotonic interpolation has undermined the
optimiser to realise its full potential. Hence, an interruption is
implemented to stop at the best basis found in the interpolation. The
right panel of Figure \ref{fig:interruption} shows the trace plot after
implementing the interruption and while the first two interpolations are
identical, the basis at Time 61 has a higher index value than the target
in the third interpolations. Rather than starting the next iteration
from the target basis on Time 65, SA starts the next iteration at Time
61 on the right panel and reaches a better final basis.

\begin{Schunk}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{/Users/hzha400/Documents/PhD/research/paper-tour-vis/figs/interruption-1} 

}

\caption[Trace plot of 2D projection on \code{boa6} data with holes index, optimised by optimiser SA]{Trace plot of 2D projection on \code{boa6} data with holes index, optimised by optimiser SA.}\label{fig:interruption}
\end{figure}
\end{Schunk}

\hypertarget{close-but-not-close-enough}{%
\subsection{Close but not close
enough}\label{close-but-not-close-enough}}

\begin{Schunk}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{/Users/hzha400/Documents/PhD/research/paper-tour-vis/figs/polish-1} 

}

\caption[Trace plot of 2D projection on \code{boa6} data with holes index, optimised first by SA and then by polish search]{Trace plot of 2D projection on \code{boa6} data with holes index, optimised first by SA and then by polish search. The polish step results in clearer cut on the edges of the four clusters in the data structure.}\label{fig:polish}
\end{figure}
\end{Schunk}

Once the final basis has been found by an optimiser, one may want to
push further in the close neighbourhood to see if an even better basis
can be found. A polish search takes the final basis of an optimiser as
the start of a new guided tour to search for local breakthrough. The
polish algorithm is similar to the SA but with three distinctions: 1) a
hundred rather than one candidate bases are generated each time in the
inner loop; 2) the neighbourhood size is reduced in the inner loop,
rather than in the outer loop; and 3) three more termination conditions
have been added to ensure the new basis generated is distinguishable
from the current one in terms of the distance in the space, percentage
change in the index value, and neighbourhood size:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  the distance between the basis found and the current basis needs to be
  larger than 1e-3;
\item
  the percentage change of the index value need to be larger than 1e-5;
  and
\item
  the alpha parameter on itself needs to be larger than 0.01
\end{enumerate}

Figure \ref{fig:polish} presents the trace plot of a 2D projection,
optimised by SA and followed by the polish. The projection by the final
basis of each algorithm is also shown. The end basis found by SA reveals
the four clusters in the data, but the edges of each cluster are not
clean-cut. Polish works with this end basis and further pushes the index
value to produce much clear edges of the cluster, especially along the
vertical axis.

\hypertarget{reconciling-the-orientation}{%
\subsection{Reconciling the
orientation}\label{reconciling-the-orientation}}

Something to fill here \ldots{}

\begin{Schunk}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{/Users/hzha400/Documents/PhD/research/paper-tour-vis/figs/flip-sign-1} 

}

\caption[PCA plot of 1D projection of boa6 data, optimised by SA, with seed 2463]{PCA plot of 1D projection of boa6 data, optimised by SA, with seed 2463.}\label{fig:flip-sign}
\end{figure}
\end{Schunk}

\hypertarget{seeing-the-signal-in-the-noise}{%
\subsection{Seeing the signal in the
noise}\label{seeing-the-signal-in-the-noise}}

The holes index function used for all the examples before this section
produces a smooth interpolation, while this is not the case for all the
indices. A 1D projection function, \(I^{nk}(n)\), compares the projected
data, \(\mathbf{Y}_{n \times 1}\), to a randomly generated normal
distribution, \(\mathcal{N}_{n \times 1}\), based on the Kolmogorov
test. Let \(F_{.}(n)\) be the ECDF function, with two possible
subscripts \(Y\) and \(\mathcal{N}\) representing the projected and
randomly generated data, and \(n\) denoting the number of observation,
the Normal Kolmogorov index, \(I^{nk}(n)\), is defined as:

\[I^{nk}(n) = \max \left[F_{Y}(n) - F_{\mathcal{N}}(n)\right]\] With a
non-smooth index function, two research questions are raised:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  whether any optimiser fails to optimise this non-smooth index; and
\item
  whether the optimisers can find the global optimum despite the
  presence of a local optimum
\end{enumerate}

\begin{Schunk}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{/Users/hzha400/Documents/PhD/research/paper-tour-vis/figs/noisy-better-geo-1} 

}

\caption[Trace and PCA plot of 1D projection on 5\-variable dataset, \code{boa5}, with \code{norm\_kol} index, optimised by all three optimiers]{Trace and PCA plot of 1D projection on 5\-variable dataset, \code{boa5}, with \code{norm\_kol} index, optimised by all three optimiers. Optimiser PD fails to optimise the non-smooth index while although both SA and SAJO success in optimise the index, SAJO takes much longer than SA and finishes off closer to the theoretial best.}\label{fig:noisy-better-geo}
\end{figure}
\end{Schunk}

Figure \ref{fig:noisy-better-geo} presents the trace and PCA plots of
all three optimisers and as expected, none of the interpolated path is
smooth. There is barely any improvement made by PD, indicating its
failure in optimising non-smooth index. While SA and SAJO have managed
to get close to the index value of the theoretical best, trace plot
shows that it takes SAJO much longer to interpolate towards the final
basis. This long interpolation path is partially due to the fluctuation
in the early iterations, where the SAJO tends to generously accept
inferior bases before concentrating near the optimum. The PCA plot shows
the interpolation path and search points excluding the last termination
iteration. {[}A sentence for PD{]}. Comparing the amount of random
search done by SA and SAJO, it is surprising that SAJO doesn't carry
that many of sampling as SA. Combine the insight from both the trace and
PCA plot, one can learn the two different search strategies by SA and
SAJO: SA frequently samples in the space and only make a move when an
improvement is guaranteed to be made, while SAJO first broadly accepts
bases in the space and then starts the extensive sampling in a narrowed
subspace. The specific decision of which optimiser to use will depend on
the index curve in the basis space but if the basis space is non-smooth,
accepting inferior bases at first, as what SAJO has done, can lead to a
more efficient search, in terms of the overall number of points
evaluated.

\begin{Schunk}
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{/Users/hzha400/Documents/PhD/research/paper-tour-vis/figs/kol-result-1} 

}

\caption[PCA plot of 1D projection on the 6\-variable data, \code{boa6}, with normal kolmogorov index]{PCA plot of 1D projection on the 6\-variable data, \code{boa6}, with normal kolmogorov index. Two optimisers: SA and SAJO and two search sizes: 0.5 and 0.7 are used in the simulation for 20 replicates. Local optimum (x), is presented in this experiment, along with the global optimum (*). The final basis is larger and more opaque than the start basis for each path and color represents whether the global or local optimum is found, after polishing.}\label{fig:kol-result}
\end{figure}
\end{Schunk}

The next experiment compares the performance of SA and SAJO when a local
maximum exists. Two search neighbourhood sizes: 0.5 and 0.7 are compared
to understand how a large search neighbourhood would affect the final
basis and the length of the search. Figure \ref{fig:kol-result} shows 80
paths simulated using 20 seeds in the PCA plot, faceted by optimiser and
search size. With SA and a search size of 0.5, despite being the
simplest and fastest, the optimiser fails in three instances where the
final basis lands neither near the local nor the global optimum. With a
larger search size of 0.7, more seeds have found the global maximum.
Comparing SA and SAJO for a search size of 0.5, SAJO doesn't seem to
improve the final basis found, despite having longer interpolation
paths. However, the denser paths near the local maximum is an indicator
that SAJO is working hard to examine if there is any other optimum in
the basis space but the relative small search size has diminished its
ability to reach the global maximum. With a larger search size, almost
all the seeds (16 out of 20) have found the global maximum and some
final bases are much closer to the theoretical best, as compared to the
three other cases. This indicates that SAJO, with a reasonable large
search size, is able to overcome the local optimum and optimise close
towards the global optimum.

\hypertarget{implementation}{%
\section{Implementation}\label{implementation}}

The implementation of this projection has been divided into two
packages: the data collection object is implemented in the existing CRAN
package \CRANpkg{tourr} \citep{tourr} while the optimiser diagnostics
have been implemented in a new package, \pkg{ferrn}. When a guided tour
is run, the users can choose to collect the data by binding
\texttt{animate\_*()} to a variable. Once the data object has been
obtained, the \pkg{ferrn} package provides diagnostic plots shown in
Section \ref{vis-diag}. The main functions of the \pkg{ferrn} package
functionality is listed below.

\begin{itemize}
\item
  Main plotting functions:

  \begin{itemize}
  \tightlist
  \item
    \code{explore\_trace\_search()} produces summary plots in Figure
    \ref{fig:toy-search}
  \item
    \code{explore\_trace\_interp()}produces trace plots for the
    interpolation points in Figure \ref{fig:toy-interp}
  \item
    \code{explore\_space\_pca()} produces the PCA plot of projection
    bases on the reduced space in Figure \ref{fig:toy-pca}. Animated
    version in Figure \ref{fig:toy-pca-animated} can be turned on via
    the argument \texttt{animate\ =\ TRUE}.
  \item
    \code{explore\_space\_tour()} produces animated tour view on the
    full space of the projection bases in Figure \ref{fig:toy-tour}.
  \end{itemize}
\item
  \code{get\_*()} extracts and manipulates certain components from the
  existing data object.

  \begin{itemize}
  \tightlist
  \item
    \code{get\_anchor()} extracts target observations
  \item
    \code{get\_basis\_matrix()} flattens all the bases into a matrix
  \item
    \code{get\_best()} extracts the observation with the highest index
    value in the data object
  \item
    \code{get\_dir\_search()} extracts directional search observations
    for PD search
  \item
    \code{get\_interp()} extracts interpolated observations
  \item
    \code{get\_interp\_last()} extracts the ending interpolated
    observations in each iteration
  \item
    \code{get\_interrupt()} extracts the ending interpolated
    observations and the target observations if the interpolation is
    interrupted
  \item
    \code{get\_search()} extracts search observations
  \item
    \code{get\_search\_count()} extracts the count of search
    observations
  \item
    \code{get\_space\_param()} produces the coordinates of the centre
    and radius of the basis space
  \item
    \code{get\_start()} extracts the starting observation
  \item
    \code{get\_theo()} extracts the theoretical best observations, if
    presented
  \end{itemize}
\item
  \code{bind\_*()} incorporates additional information outside the tour
  optimisation into the data object.

  \begin{itemize}
  \tightlist
  \item
    \code{bind\_theoretical()} binds the theoretical best observation in
    simulated experiment
  \item
    \code{bind\_random()} binds randomly generated bases in the
    projection bases space to the data object
  \item
    \code{bind\_random\_matrix()} binds randomly generated bases and
    outputs in a matrix format
  \end{itemize}
\item
  \code{add\_*()} create ggprotos for different components for the PCA
  plot

  \begin{itemize}
  \tightlist
  \item
    \code{add\_anchor()} is a wrapper for plotting anchor bases
  \item
    \code{add\_anno()} is a wrapper for annotating the symmetry of start
    bases
  \item
    \code{add\_dir\_search()} is a wrapper for plotting the directional
    search bases with magnified distance
  \item
    \code{add\_end()} is a wrapper for plotting end bases
  \item
    \code{add\_interp()} is a wrapper for plotting the interpolation
    path
  \item
    \code{add\_interp\_last()} is a wrapper for plotting the last
    interpolation bases for comparing with target bases when
    interruption is carried
  \item
    \code{add\_interrupt()} is a wrapper for linking the last
    interpolation bases with target ones when interruption is carried
  \item
    \code{add\_search()} is a wrapper for plotting search bases
  \item
    \code{add\_space()} is a wrapper for plotting the circular space
  \item
    \code{add\_start()} is a wrapper for plotting start bases
  \item
    \code{add\_theo()} is a wrapper for plotting theoretical best bases,
    if applicable
  \end{itemize}
\item
  Utilities

  \begin{itemize}
  \tightlist
  \item
    \code{theme\_fern()} and \code{format\_label()} for better display
    of the grid lines and axis formatting
  \item
    \code{clean\_method()} for clean up the name of the optimisers
  \item
    \code{botanical\_palettes} is a collection of colour palettes from
    Australian native plants Quantitative palettes include daisy,
    banksia and cherry and sequential palettes contain fern and acacia
  \item
    \code{botanical\_pal()} as the colour interpolator
  \item
    \code{scale\_color\_*()} and \code{scale\_fill\_*()} for scaling the
    colour and fill of the plot
  \end{itemize}
\end{itemize}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

This paper has illustrated setting up a data object that can be used for
diagnosing a complex optimisation procedure. The ideas were illustrated
using the optimisers available for projection pursuit guided tour. Here
the constraint is the orthonormality condition of the projection bases.
The approach used here could be broadly applied to understand other
constrained optimisers.

Four diagnostic plots have been introduced to investigate the
progression and the projection space of an optimiser. The implementation
of these visualisations is designed to be easy-to-use with each plot can
be produced with a simple supply of the data object. More advanced users
may decide to modify on top of the basic plots or even build their own.

Most of the work in this project has been translated into code in two
packages: the collection of the data object is implemented in the
existing \CRANpkg{tourr}\citep{tourr} package; manipulation and
visualisation of the data object are implemented in the new \pkg{ferrn}
package. Equipped with handy tools to diagnose the performance of
optimisers, future work can extend the diagnostics to a wider range of
index functions, i.e.~scagnostics, association, and information index
\citep{laa2020using} and understand how the optimisers behave for index
functions with different structures.

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

This article is created using \CRANpkg{knitr}\citep{knitr} and
\CRANpkg{rmarkdown} \citep{rmarkdown} in R. The source code for
reproducing this paper can be found at:
\url{https://github.com/huizezhang-sherry/paper-tour-vis}.

\bibliography{zhang-cook-laa-langrene-menendez}


\address{%
H.Sherry Zhang\\
Monash University\\
Department of Econometrics and Business Statistics\\
}
\href{mailto:huize.zhang@monash.edu}{\nolinkurl{huize.zhang@monash.edu}}

\address{%
Dianne Cook\\
Monash University\\
Department of Econometrics and Business Statistics\\
}
\href{mailto:dicook@monash.edu}{\nolinkurl{dicook@monash.edu}}

\address{%
Ursula Laa\\
University of Natural Resources and Life Sciences\\
Institute of Statistics\\
}
\href{mailto:ursula.laa@boku.ac.at}{\nolinkurl{ursula.laa@boku.ac.at}}

\address{%
Nicolas Langrené\\
CSIRO Data61\\
34 Village Street, Docklands VIC 3008 Australia\\
}
\href{mailto:nicolas.langrene@csiro.au}{\nolinkurl{nicolas.langrene@csiro.au}}

\address{%
Patricia Menéndez\\
Monash University\\
Department of Econometrics and Business Statistics\\
}
\href{mailto:patricia.menendez@monash.edu}{\nolinkurl{patricia.menendez@monash.edu}}

