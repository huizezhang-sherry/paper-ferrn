---
title: Title here
blinded: 0
authors: 
  
- name: Author 1
  thanks: The authors gratefully acknowledge ...
  affiliation: Department of YYY, University of XXX
- name: Author 2
  affiliation: Department of ZZZ, University of WWW
keywords:
- optimisation
- projection pursuit 
- guided tourr
- visual 
- diagnostics 
- R
abstract: Friedman & Tukey commented on their intial paper on projection pursuit in 1974 that "the  technique  use  for  maximising  the  projection index strongly influences both the statistical and the computational aspects of the procedure." While many projection pursuit indices have been proposed in the literature, few concerns the optimisation procedure. In this paper, we developed a system of diagnostics aiming to visually learn how the optimisation procedures find its way towards the optimum. This diagnostic system can be applied to more general to help practitioner to unveil the black-box in randomised iteartive (optimisation) algorithms. An R package, ferrn, has been created to implement this diagnostic system.
bibliography: biblio.bib
preamble: >
  \usepackage{amssymb}
  \usepackage[ruled,vlined, linesnumbered]{algorithm2e}
output: rticles::asa_article
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(tidyverse)
library(ferrn)
library(patchwork)
library(ggrepel)
```

```{r load-data}
files <- paste0("data/", list.files(here::here("data")))
purrr::walk(.x = files, ~load(here::here(.x), env = globalenv()))

source(here::here("source/indice.r"))
source(here::here("source/sim_data.r"))
```


# Introduction

In an optimization problem the goal is to find the best solution within the space of all feasible solutions which typically is represented by a set of constraints. The problem consists on optimizing an objective function $f: S \rightarrow \Re$ with $S \in \Re^n$ in a reduced spaced given by the problem constraints to either minimize or maximize a function.... Gradient based optimization has been typically used to solve such problems. However, there are many situations where derivatives of an objective function are unavailable or unreliable and therefore traditional methods based on derivatives are not the best option to solve an optimization problem.

Derivative free methods provides another option to optimise the objective function without evaluating any gradient or Hessian information and a particular class of methods: direct search, has gained its popularity through its conceptual simplicity.  However, the whole searching process in the algorithm remains a black-box. Plots are usually used to evaluate and compare the performance of different algorithms but it can easily become tedious because the code will have to be modified significantly when comparing different parameters in the algorithms. For example, a categorical variable with 5 levels can be easily mapped onto color while mapping another categorical variable with 30 levels will not make the plot informative. Thus the plot needs to be re-designed to better suits the characteristics of the parameter (whether the parameter is a scalar or a matrix? whether the parameter is quantitative or categorical? If categorical, how many levels does the parameter have?). This motivates the  design of a visual diagnostic framework for optimisation algorithms based on the idea of a *global object*. 

The paper is organised as follows. Section \ref{DFO} gives a general literature review of optimisation, specifically derivative free optimisation. Section \ref{vis-diag} presents the new idea of constructing a
systematic visual framework that diagnoses the components of an optimisation procedure (parameters, searching path, etc). The rest of the paper serves as a comprehensive example of using the visual diagnostics on one particular problem: *projection pursuit guided tour*. Some background knowledge of projection pursuit guided tour is provided in Section \ref{tour}. Section \ref{apply} applies the concepts proposed in section \ref{vis-diag} in the tour problem and sets up the data. The last section, Section \ref{plots}, presents the visual diagnostic plots and explains how they can help to understand different aspects of the optimisation in tour.


# Derivative free optimisation {#DFO}

Given an objective function $f$, one way of optimising it is to equate its gradient to zero. In modern optimisation problems, gradient information can be hard to evaluate or sometimes even impossible and Derivative-Free Optimisation (DFO) methods can be useful to approach these problems. One common class of methods in DFO is *Direct-search methods*. Coined by @hooke1961direct, direct search methods don't require any gradient or Hessian information and has gained its popularity through its simplicity in use and reliability in solving complicated practical problems. Depends on whether a random sample is used in the search, this class of methods can be further classified as *stochastic* or *deterministic*.  The stochastic version of the direct-search methods will be the main optimisation procedure analysed in this paper.

[How about adding more details into derivative free methods? ppp]


<!-- The two most common methods in DFO are *direct-search methods* and *model-based methods* and this paper dedicates to the discussion of direct-search methods, which .  -->


<!-- A well-known example of it is the Nelder-Mead algorithm [@nelder1965simplex] and it enjoys the popularity due to its simplicity and reliability. [feel like this sentence can be expanded to include more information. xxx]. -->







## Difficulties

[Are we using  projection pursuit/guided tour to better understand the convergence of optimization algorithms visually in combination with the algorithms discussed below? Or we are focusing on the optimisation problem only within the project pursuit context? Some of the problems listed below are also applicable to optimization problem in general too. ppp] 

Below listed several issues in projection pursuit optimisation. Some are general optimisation problems, while others are more specific for PP optimisation. 

- *Finding global maximum*: Although finding local maximum is relatively easy with developed algorithms, it is generally hard to guarantee global maximum in a problem where the objective function is complex or the number of decision variables is large. Also, there are discussions on how to avoid getting trapped in a local optimal in the literature.

- *optimising non-smooth function*: When the objective function is non-differentiable, derivative information can not be obtained, which means traditional gradient- or Hessian- based methods are not feasible. Stochastic optimisation method could be an alternative to solve these problems.

- *computation speed*: The optimisation procedure needs to be fast to compute since tours produces real-time animation of the projected data.

- *consistency result in stochastic optimisation*: In stochastic algorithm, researchers usually set a seed to ensure the algorithm producing the same result for every run. This practice supports reproducibility, while less efforts has been made to guarantee different seeds will provide the same result. 

- *high-dimensional decision variable*: In projection pursuit, the decision variable includes all the entries in the projection matrix, which is high-dimensional. Researcher would be better off if they can understand the relative position of different projection matrix in the high-dimensional space. 

- *role of interpolation in PP optimisation*: An optimisation procedure usually involves iteratively finding projection bases that maximises the index function, while tour requires geodesic interpolation between these bases to produce a continuous view for the users. It would be interesting to see if the interpolated bases could, in reverse, help the optimisation reach faster convergence.

<!-- the interpolation can be seen as giving information on the direction of the derivative. -->


<!-- - is `search_geodesic` widely known in the research community - doesn't seem to find paper introducing this except (Cook, 1995). Any tools available to  better understand its searching process? -->

*Think about how does your package help people to understand optimisation*
 
 - diagnostic on stochastic optim
 - vis the progression of multi-parameter decision variable 
 - understanding learning rate - neighbourhood parameter
 - understand where the local & global maximum is found - trace plot - see if noisy function


# Projection pursuit guided tour {#tour}

From Section \ref{tour}, we presents a comprehensive case study on how to use the visual diagnostics to explore the optimisation in a specific problem: projection pursuit guided tour. Section \ref{tour} aims to provide non-experts with an overview of the problem content and the existing optimisation procedures used in projection pursuit guided tour. For those who are already familiar with the techniques, feel free to skip this section. 

The optimisation problem we're interested in is in the context of projection pursuit. Coined by @friedman1974projection, projection pursuit is a method that detects the interesting structure (i.e. clustering, outliers and skewness) of multivariate data via projecting it in lower dimensions. Let $D_{n \times p}$ be a data matrix, an n-d projection can be seen as a linear transformation: $T: \mathbb{R}^p \mapsto \mathbb{R}^d$ defined by $P = D \cdot A$, where $A_{p\times d}$ is the projection basis and $P_{n \times p}$ is the projected data. Define $f: \mathbb{R}^{p \times d} \mapsto \mathbb{R}$ be an index function that maps the projection basis $A$ onto a scalar value $I$, this function is commonly known as the projection pursuit index (PPI) function, or indice function,  that can be used to measure the "interestingness" of the projection. A number of indice functions have been proposed in the literature and this includes Legendre index [@friedman1974projection], hermite index [@hall1989polynomial], natural hermite index [@cook1993projection], chi-square index [@posse1995projection], LDA index [@lee2005projection] and PDA index [@lee2010projection]. [Any literature on holes index? xxx] 

 As @friedman1974projection noted "..., the technique use for maximising the projection index strongly influences both the statistical and the computational aspects of the procedure." A suitable optimisation procedure is needed to find the projection angle that maximises the PPI and the quality of the optimisation largely affect the interesting projections one could possibly observe.

Projection pursuit is usually used in conjunction with a tour method called *guided tour*. Tour explores the multivariate data *interactively* via playing a series of projections, that form a *tour path* and guided tour uses the path that is geodesically the shortest. Details of the mathematical construction of a tour path can be found in @buja2005computational. Modified from @buja2005computational, Figure \ref{tour-path} vividly depicts the tour path in guided tour. Guided tour, along with other types of tour, has been implemented in the *tourr* package in R, available on the Comprehensive R Archive Network at [https://cran.r-project.org/web/packages/tourr/](https://cran.r-project.org/web/packages/tourr/) [@wickham2011tourrpackage]. 

```{r tour-path, out.width="100%", out.height="60%", fig.cap="\\label{tour-path}An illustration of the tour path"}
knitr::include_graphics(path = here::here("figures/tour_path_increasing.png"))
```
 



[doubt if I need the following. xxx - maybe an algorithmatic version of guide tour would be helpful for non-expert to understand the method?]
<!-- A tour path includes two major components: a *generator* that generating the target basis that has higher index value than the current basis and an *interpolator* that performing geodesic interpolation between the current and target basis. An illustration modified from [@buja2005computational] vividly depicts the tour path in guided tour. -->


<!-- The pseudo-code below illustrates the implementation of guided tour in the tourr package. Given an projection pursuit index function and a randomly generated projection basis (current basis), the optimisation procedure produces a target basis inside `generator()`  Both the current basis and the target basis will be supplied to `tour_path()` to prepare information needed for constructing a geodesic path. This information is then used to compute a series of interpolating bases inside the `tour()` function. All the basis will be sent to create animation for visualising the tour in the `animate()` function.   -->


<!-- ```{r algo,eval = FALSE, echo = TRUE} -->
<!-- animation <- function(){ -->

<!--   # compute projection basis  -->
<!--   tour <- function(){ -->

<!--     # construct bases on the tour path -->
<!--     new_geodesic_path <- function(){ -->
<!--       tour_path <- function(){ -->

<!--         # GENERATOR: generate projection basis via projection pursuit -->
<!--         guided_tour <- function(){ -->
<!--           generator <- function(){ -->

<!--             # define projection pursuit index -->
<!--             # generate the target basis from the current basis via optimisation -->
<!--           } -->
<!--         } -->

<!--         # prepare geodesic information needed for interpolating along the tour path -->
<!--       } -->
<!--     } -->

<!--     # INTERPOLATOR: interpolate between the current and target basis  -->
<!--     function(){ -->
<!--       # generate interpolating bases on the geodesic path -->
<!--     } -->
<!--   } -->

<!--   # animate according to different display methods -->
<!-- } -->
<!-- ``` -->



## Optimisation in projection pursuit

[thinking if the following three should be presented as algorithms or plain description is fine. xxx]

Now we formulate the problem in mathmetical notation. Projection pursuit finds the projection basis $A^J = [\mathbf{a}_1, \cdots, \mathbf{a}_d]$, where $\mathbf{a}_i$ is a $p \times 1$ vector, satisfying the following optimisation problem: 


\begin{align}
I &= \arg \max_{A \in \mathcal{A}^{p \times d}} f(A) \\
s.t. & \mathcal{A} = \{ \forall \mathbf{a}_i \in A: \lVert \mathbf{a}_i \rVert = 1 \}
\end{align}

via an iterative search: $\{A^1, A^2, \cdots \cdots A^J\}$. Guided tour interpolates geodesically between each basis found by projection pursuit $A^j$ to form a tour path: $$\{A^1, A^1_1, A^1_2,\cdots, A^1_{k_1}, A^2, A^2_1, A^2_2, \cdots A^2_{k_2}, A^3, \cdots, A^{J-1}, A^{J-1}_{1}, \cdots A^{J-1}_{k_m} ,A^J  \}$$

There are three existing methods for optimisating PPI function and we review them below. 

@posse1995projection used a stochastic direct search method, a random search algorithm to sample a candidate bases $A^{(j +1)}$ in the neighbourhood of the current basis $A^j$ defined by the radius of the p-dimensional sphere, $\alpha$. If the candidate basis has a higher index value than the current basis, it is outputed as the target basis $A^{j + 1}$ along with other metadata and the algorithm stops. If no basis is found to have higher index value after the maximum number of tries $i_{max}$, the algorithm stops with nothing outputted. $h$ is the halfing parameter meaning if $h > 30$, the nieghbourhood parameter $\alpha$ will be reduced by half in the next iteration of the random search. The algorithm for a random search is summarised in Algorithm \ref{random-search}.

\newpage
\begin{algorithm}[H]
\SetAlgoLined
  initialisation: $h = 0$, $loop = 1, i_{max}$\;
  \While{$i < i_{max}$}{
    $A^{(j + 1)} = A^{j} + \alpha A_{rand}$\;
    $I^{(j + 1)} = f(A^{(j+1)})$\;
    \eIf{$I^{(j+1)} > I^{j}$}{
      $A^{j+1} = A^{(j+1)}$\;
      }{
      $h = h + 1$\;
      }
    $i = i + 1$\;
  }
  \caption{random search: my first algorithm}
  \label{random-search}
\end{algorithm}

@cook1995grand explained the use of a gradient ascent optimisation with the assumption that the index function is continuous and differentiable. Since some indices could be non-differentiable, the computation of derivative is replaced by a pseudo-derivative of evaluating five randomly generated directions in a tiny nearby neighbourhood. Taking a step on the straight derivative direction has been modified to maximise the projection pursuit index along the geodesic direction. 

\newpage
\begin{algorithm}[H]
\SetAlgoLined
  initialisation: $loop = 1, i_{max}$\;
  \While{$i < i_{max}$}{
    Generate $\{A^{(j_1)}: A^{(1)}, \cdots A^{(5)}\}$\;
    Find $A^{(J_1)}$ where $J_1 \in j_1$ such that $\{A^{(J_1)}: \arg \max I(A^{(j_1)})\}$\;
    Denote $F_a = A^j$ and $F_z = A^{(J_1)}$\;
    Conduct singular value decomposition (SVD) for $F_a$ and $F_z$ such that $F_{a}^{T}F_z = V_a\Lambda V_z^T$\;
    Compute $G_a = F_aV_a$, $G_z = F_zV_z$ and $\tau = \arccos(\Lambda)$\;
    ...\;
    Find $A^{(J_2)}$ satisfying $A^{J_2}$ on the interpolation path such that $\{A^{(J_2)}: \arg \max I(A^{(j_2)})\}$\;
    Compute $I^{(J_2)} = f(A^{(J_2)})$, $p_{diff} = (I^{(J_2)} - I^{j})/I^j$\;
      \If{$p_{diff} > 0.001$}{
        $A^{j+1} = A^{(J_2)}$\;
        $I^{j+1} = I^{(J_2)}$\;
        \KwOut{$A^{j+1}$, $I^{j+1}$, $i$ and relevant metadata}
      }
    $i = i + 1$\;
  }
  \caption{searchgeodesic}
  \label{search_geodesic}
\end{algorithm}



Simulated annealing [@bertsimas1993simulated, @kirkpatrick1983optimization] is a non-derivative procedure based on a non-increasing cooling scheme $T(i)$. Given an initial $T_0$, the temperature at iteration $i$ is defined as $T(i) = \frac{T_0}{log(i + 1)}$. The simulated annealing algorithm works as follows. Given a neighbourhood parameter $\alpha$ and a randomly generated orthonormal basis $A_{rand}$, a candidate basis is constructed as $A^{(j+1)} = (1 - \alpha)A^{j} + \alpha A_{rand}$. If the index value of the candidate basis is larger than the one of the current basis, the candidate basis becomes the target basis. If it is smaller, the candidate is accepted with probability $\text{prob} = \min\left\{\exp\left[\frac{I^j - I^{(j+1)}}{T(i)}\right],1\right\}$ where $I^j$ and $I^{(j+1)}$ are the index value of the current and candidate basis respectively. The algorithm can be summarised as in Algorithm \ref{simulated_annealing}.

\newpage
\begin{algorithm}[H]
\SetAlgoLined
  initialisation: $T_0$, $\alpha$\;
  \While{$i < i_{max}$}{
    $A^{(j+1)} = (1 - \alpha)A^{j} + \alpha A_{rand}$ s.t. $\lVert\mathbf{a_i} \rVert = 1$\;
    $I^{(j+1)} = f(A^{(j+1)})$\;
    $T(i) = \frac{T_0}{\log(i + 1)}$\;
      \eIf{$I^{(j+1)} > I^{j}$}{
        $A^{j+1} = A^{(j+1)}$\;
        $I^{j+1} = I^{(j+1)}$\;
        \KwOut{$A^{j+1}$, $I^{j+1}$,$i$ and relevant metadata}
      }{
        $\text{prob} = \min\left\{\exp\left[\frac{I^j - I^{(j+1)}}{T(i)}\right],1\right\}$\;
        Draw $\text{rand} \sim \text{Unif(0, 1)}$\;
        \If{$\text{prob} > \text{rand}$}{
          \KwOut{$A^{j+1}$, $I^{j+1}$, $i$ and relevant metadata}
        }
      }
    $i = i + 1$\;
  }
  \caption{simulated annealing}
  \label{simulated_annealing}
\end{algorithm}

\newpage

Below listed several features characterise the optimisation procedures needed in projection pursuit

- *Being able to handle non-differentiable PPI function*: The PPI function could be non-differentiable, thus derivative free methods are preferred. 

- *Being able to optimise with constraints*: The constraint comes from projection matrix being an orthonormal matrix. 

- *Being able to find both local and global maximum*: Although the primary interest is to find the global maximum, local maximum could also reveal structures in the data that are of our interest. 


# Visual diagnostic system {#vis-diag}

[I would expand this section more as the core contribution. ppp]

## Origin idea
<!-- Visual diagnostics can be real-time or post-run. Real time diagnostic directly uses the data produced in the algorithm to produce visual representation and thus doesn't need to store the data.   This section focuses on the definition and production of post-run diagnostics and the next section discusses real-time diagnostics. -->


<!-- Visualisation has been widely used for exploring and understanding data. Presenting information in a graphical manner often allows people to see information they would otherwise not seen. -->


Random search methods has a black-box mechanism and focuses solely on finding the global maximum point, while the projection pursuit problem we have aims at *exploring* the data and thus is interested in how the algorithm finds its maximum. This motivates us **to develop a visual diagnostic system for exploring the optimisation searching path**. 

The necessity of developing such a system rather than simply producing different diagnostic plots is because the diagnostics of each variable requires a different function and these functions can't be scaled to other problems. Thus we want to establish a set of rules that can generalise the diagnostic of iterative algorithms. 

The idea of generalising all the diagnostic plots under one framework is inspired by the concept of *grammar of graphic*[@wickham2010layered], which powers the primary graphical system in R, ggplot2 [@ggplot2]. In grammar of graphic, plots are not defined by its appearance (i.e. boxplot, histogram, scatter plot etc) but by "stacked layers". By this design, ggplot doesn't need to develop a gazillion of functions that each produces a different type of plot. Instead, it aesthetically maps the variables to the geometric objects and builds the plot through different layers.

<!-- [Do I need this paragraph to details the grammar of graphic? xxx - don't think so; not particularly relevant - assume most people know it roughly] -->
<!-- A layer includes 1) the data that powers the plot; 2) a geometric object that represents the visual shape of the data and 3) relevant statistical transformation that transform the information in the data to the information used to draw the geometric object. An important concept in the grammar of graphic is *aesthetic mapping*. Aesthetic mapping links the variable in a dataset to information needed to produce a geometric object. For example, we map one variable on the x-axis and another on the y-axis to create a scatterplot. Another example of creating a boxplot involves 1) mapping one variable on the x-axis and 2) transforming another variable to its five-point-summary and mapping the five-point-summary to the y-axis.  -->


## Global object

Ggplot requires a data frame that contains all the variables to plot and a *global object* is constructed as the data frame supplied to the visual diagnostic plots to better suit the characters of iterative optimisation algorithms. Given an optimisation algorithm, two primary variables of interest are the *decision variable* and the *value of the objective function*. *iterators* indexes the data collected and this order has time series feature that prescribes the progression of the optimisation. Given the complexity of the algorithm, there could be multiple iterators. There are other parameters that can't be classified as one of the three categories but are also of our interest. They are defined under the fourth category: *other parameter of interest*. This could include variables that are specific for one particular algorithm. Below we show an example of defining the global object of projection pursuit guided tour.

An optimisation procedure can easily generate hundreds or thousands sampling points and the question becomes what are the points we want to explore... [the classification of searching or updating points goes here and people should be clear what part of the points goes to the plot - this is the data part not the plot part. ]



In tour, all the points recorded in the global object can be divided into two broad categories: searching points and interpolating points

- *Searching points* include the observations recorded in the searching algorithm to find the target basis. The points for target bases is also included in the searching points and there is one such point per `tries`.

- *interpolating points* exist in the guided tour to produce continuous animated view from one target basis to another and it doesn't have `loop` value.

<!-- The concept of grammar of graphic requires a dataset to be supplied in making a plot, thus a global object needs to be created in the iterative algorithm. The variables will be mapped into element of a graphic to explore and thus should contain all the parameters of interest. The next section shows how a global object is created in projection pursuit guided tour. -->


<!-- In the current implementation of the `tourr` package, while the target basis generated by the projection pursuit can be accessed later via `save_history()`, interpolating bases and those randomly nearby bases generated in the optimisation are not stored. This creates difficulties for fully understand the behaviour of the optimisation and interpolation of tour in complex scenario **[need a rephrase this part].**  -->

In the projection pursuit guided tour, the decision variable is the projection matrix, the value of objective function is the index value. There are three iterators: `id` is the smallest ordering unit that increases by one for each observation; `tries` is updated once a search-and-interpolate step is finished. A third iterator `loop` is used to record the number of repetition in the search step and starts over from one at the beginning of a new `tries`. Three other parameters of our interest includes `method`, `alpha` and `info`.  `method` identifies the name of the searching method used and we are interested in comparing the performance between different algorithms under direct search. The neighbourhood parameter `alpha` controls the size of the sampling space and we are interested to understand how the searching space shrinks as the algorithm progresses. `info` labels different stages in the searching process. A sketch of the global object for projection pursuit guided tour is presented in Figure \ref{fig:glb-obj}. 


```{r glb-obj, out.width= "100%", out.height="20%", fig.cap="\\label{glb-obj}The global object in projection pursuit guided tour."}
knitr::include_graphics(path = here::here("figures/global_obj.png"))
```


### Simulated data

[I would add the maths here to describe the simulations. ppp]

Two set of simulated data are used in the demo of the visual diagnostics in projection pursuit guided tour. A small dataset consists of 1000 randomly simulated observations of five variables (`x1`, `x2`, `x8`, `x9`, `x10`). `x2` is the informative variable simulated from two bi-modal normal distribution centred at -3 and 3 with variance of one and the other four are simulated from non-informative standard random normal distributions. The data has been scaled to ensure `x2` has variance of 1. The goal is to find the 1D projection of bi-modal shape, which corresponds to the projection matrix (vector) of `[0, 1, 0, 0, 0]`. 

A larger dataset contains more informative variables (`x3` to `x7`) of different types. The distribution of all the variables except `x3` is plotted in Figure \ref{origin-data} and `x3` takes 500 positive one and 500 negative one. [should I introduce the dist for each var? xxx] [also feel like we don't really use `x3` to `x6`, should we not mention about these? xxx]

```{r origin-data, fig.cap="\\label{origin-data} The distribution of simulated data except x3"}
origin_dt_bi <- data_mult %>% 
  dplyr::select(-x3) %>% 
  gather(names, values) %>%
  mutate(names = as_factor(names))
  
origin_dt_bi %>%
  ggplot(aes(x = values)) +
  geom_histogram(binwidth = 0.3) +
  geom_density(aes(y = 0.3 * ..count..)) +
  facet_wrap(vars(names), ncol = 3)
```

In tour, when the optimisation ends the global object will be stored and printed and can be turned off via `print = FALSE`. Additional messages during the optimisation can be displayed via `verbose = TRUE`. Below presented the global object of the 1D projection using the small dataset. [the printing is not ideal as it doesn't show all the columns. xxx]

```{r code, eval = FALSE}
set.seed(123456)
holes_1d_geo <- animate_dist(data, tour_path = guided_tour(holes(), d = 1,
                                           search_f =  search_geodesic),
                  rescale = FALSE, verbose = TRUE)

#save(temp, file = here::here("vignettes", "data", "temp.rda"))
```

```{r glb-obj-1}
holes_1d_geo %>% head(5)
```

<!-- `tries` has an increment of one once the generator is called (equivalently a new target basis is generated); `info` records the stage the basis is in. This would include the `interpolation` stage and the detailed stage in the optimisation i.e. `direction_search`, `best_direction_search`, `line_search`and `best_line_search` for geodesic searching (`search_geodesic`); `random_search` and `new_basis` for simulating annealing (`search_better`). `loop` is the counter used for the optimisation procedure and thus will be `NA` for interpolation steps. `id` creates a sequential order of the basis.  -->

<!--  Another examples is a 2D projection of the larger dataset with two informative variable (`x2` and `x7`) using search_better method. Notice in this example, the dimension of the bases becomes 6 by 2. -->


<!-- ```{r glb-obj-2, echo = TRUE} -->
<!-- holes_2d_better %>% head(5)  -->
<!-- ``` -->




## Visual diagnostics

Below we will present several examples to diagnose different aspect of the tour optimisation. We will first provide examples on producing 1) static plots to explore the value of objective function and 2) animated plots to explore the searching space and then a more sophisticated example that combines both to understand how to optimise a noisy and complex index function. In both section, we will first provide a toy example that is easy to grasp and one or two examples that help us understand the optimisation algorithm and parameter choice better.


Follow the research question we raised earlier, the purpose of visual diagnostics is to understand 

- Whether the procedure has provided an optimum and how the value of the objective function changes through the optimisation.  

- How does the searching space look like, that is, geometrically, where are the points located in the space.

Overall, the first question can be answered using a static plot with x-axis showing the progression of the optimisation and y-axis showing the value of the objective function. The second question can be addressed via visualising the rotating high dimensional space. This seems to be a hard task to see beyond 3D, but the fact is that we never really see all the six faces of a cube at one time. The way humans understand the dimension of a 3D object is either through rotating the physical 3D object or using shade and line type to annotate the 3D object in a paper or electronically. Thus we can do a similar rotation on the screen to precieve even higher dimension. [there is likely to be a learning curve - remember when learning geometry in school, it takes a while to get used to see 3D cube on a paper]. Thus animated visualisation is needed to preceive the optimisation path in the searching space.

The plot design of visual diagnostics depends on the characteristics of the variables to plot. If the searching points are of interest [point geom is not good; need summarise. While updating points are more manageable -> point geom]. However, one must realise that with hundreds or thousands of searching points, exploring the sample space in animation could be slow and this is because of the time it takes to render hundreds point. [stratefy may help? - may need more work here.]

### Explore the value of objective function {#static}

#### Toy example: exploring searching points 

The largest difficulties of exploring searching points is its unknown number of observations per `tries`. Mapping `id` on the x-axis will leave the `tries` with few observations a small space in the plot, while those `tries` with large number of search points (usually towards the end) occupy the vast majority of the plot space. This motivates to summarise the points in each `tries`. At each iteration, rather than knowing the index value of *every* points, we are more interested to know a general summary of all the points and more importantly, the point with the largest `index_val` since it prescribes the geodesic interpolation and future searches. 

Boxplot is a suitable candidate that provides five points summary of the data, however it has one drawback: it doesn't report the number of point in each box. We may risk losing information on how many points it takes to find the target basis by displaying the boxplot alone for all `tries`. Thus, the number of point for each `tries` is displayed at the bottom of each box and we provide options to switch `tries` with small number of points to a point geometry, which is achieved via the `cutoff` argument. A line geometry is also added to link the points with the largest index value in each `tries`. This helps to visualise the improvement made by each `tries`. 

*Example: exploring searching points* The larger dataset is used for 2D projection with `search_better` and `max.tries = 500`. In Figure \ref{points}, the searching points with `id` and `tries` on the x-axis, colored by `tries`. Label at the bottom indicates the number of observations in each tries and facilitates the choice of cutoff to switch from point geometry to boxplot geometry (`cutoff = 15`). The line geometry suggests the largest improvement happens at `tries = 5`. 

```{r points-tries,fig.asp=0.5, fig.cap="\\label{points}A comparison of plotting the same search points with different plot designs. The left plot doesn't efficiently use the plot space to convey information from the plot while the right plot provides good summarisation of data and number of points in each tries."}

dt <- holes_2d_better_max_tries %>%
  filter(info != "interpolation") %>% 
  mutate(id = row_number()) 

dt %>% explore_trace_search()

```

#### A more complex example: adding an interruption in the interpolation stage

```{r interpolation}
#points is always included in the newest version
# p1 <- holes_2d_better %>%
#   filter(info == "interpolation") %>%
#   ggplot(aes(x = tries, y = index_val, col = as.factor(tries))) +
#   geom_point() +
#   geom_line(aes(group = 1))
# 
# p2 <- holes_2d_better %>% explore_trace_interp() + geom_point(aes(col = as.factor(tries)))
# 
# (p1 | p2)  &
#   theme(legend.position = "bottom")
# 
```

*Example: Interruption* This examples uses `search_better` for a 2D projection on the larger dataset using the `holes` index. In the interpolation stage, continuous frames will be constructed to bridge the project on the current basis to the target basis.  The target basis will become the current basis in the next iteration and the searching stage continues to find the next target basis. In the left panel of Figure \ref{interruption}, we observe that  there are interpolating bases with higher index values than the target basis and these bases could be used to search for new basis in the next searching stage. 

An interruption is then constructed to accept the interpolating bases up to the one with the largest index value and use that basis as the current basis for the next searching stage. After implementing this interruption, the tracing plot with the same configuration is shown on the right panel of Figure \ref{interruption}. In the third `tries`, rather than interpolating fully to the target basis at `id = 62`, it stops before the index value starts to decrease at `id = 60`. This implementation results in a higher index value in the end with fewer steps.

```{r interruption,fig.asp=0.5, fig.cap = "\\label{interruption}Trace plots of the interpolated basis with and without the interruption. The interruption stops the interpolation when the index value starts to decrease at id = 60. The implementation of the interuption finds an ending basis with higher index value using fewer steps. "}
p1_anno <- interrupt_no %>% filter(info == "interpolation") %>% mutate(id = row_number()) %>% filter(id %in% c(44, 60, 62)) %>% 
  mutate(anno = c("current basis", "interpolated basis", "target basis"))

p1 <- interrupt_no %>% mutate(id = row_number() - 1) %>% explore_trace_interp() + ggtitle("without interruption") + 
  geom_point(data = p1_anno) + 
  geom_label_repel(data = p1_anno, aes(label = anno), box.padding = 0.5) + ylim(0.8, 0.9) + xlim(0, 80) + 
  theme(legend.position = "none")
  
p2 <- interrupt_yes %>% explore_trace_interp() + ggtitle("with interruption") + ylim(0.8, 0.9) + xlim(0, 80) + 
  theme(legend.position = "none")

p1 | p2
#xxx up and down plot allows for displaying time series features while left and right display makes it easier for comparison? The same problem for plot of comparing between different searching methods
```

#### Another example: polishing the final projection basis

<!-- In the previous two sections, only the iterator and the index value are mapped onto the x and y aesthetics of the plot; while more aesthetics i.e. color, could be added to compare other parameters in the global object. -->

<!-- Two examples are shown below to explore and compare different searching methods and neighbourhood parameter alpha.  -->

*Example: Polish* In principle, all the optimisation routines should result in the same output on the same problem while this may not be true in real application. This motivates the creation of a polishing search that polishes the ending basis and achieves unity on different methods. 

`search_polish` takes the ending basis of a given search as the current basis and uses a brutal-force approach to sample a large number of basis (`n_sample`) in the neighbourhood, whose radius is controlled by `polish_alpha`. Among the `n_sample` basis, the one with the largest index value becomes the candidate basis. If its index value of the candidate basis is larger than that of the current basis, it becomes the current basis in the next iteration. If no basis is found to have larger index value than the current basis, the searching neighbourhood will be shrunk and the search continues. The polishing search ends when one of the four stopping criteria is satisfied:

1) the two basis can't be too close 
2) the percentage improvement of the index function can't be too small 
3) the searching neighbourhood can't be too small
4) the number of iteration can't exceed the `max.tries`

[should the stopping criteria be more detailed? xxx]

The usage of search_polish is as follows. After the first tour, the final basis from the interpolation is extracted and supplied into a new tour with the `start` argument and `search_polish` as the searching function in the guided_tour. All the other arguments should remain the same. 

```{r sample-code, eval = FALSE, echo = TRUE}
set.seed(123456)
holes_2d_geo <- animate_xy(data_mult[,c(1,2, 7:10)],tour_path = 
                             guided_tour(holes(), d = 2, 
                                         search_f = tourr:::search_geodesic),
                           rescale = FALSE, verbose = TRUE)

last_basis <- holes_2d_geo %>% filter(info == "interpolation") %>% 
  tail(1) %>% pull(basis) %>% .[[1]]

set.seed(123456)
holes_2d_geo_polish <- animate_xy(data_mult[,c(1,2, 7:10)], tour_path = 
                                    guided_tour(holes(), d = 2, 
                                                search_f = tourr:::search_polish),
                                  rescale = FALSE, verbose = TRUE, 
                                  start = last_basis)
```

The following example conducted a 2D projection on the larger dataset using search better with different configurations. `max.tries` is a hyperparameter that controls the maximum number of try without improvement and its default value is 25. As shown in Figure \ref{trace-compare}, both trials attain the same index value after polishing while a small `max.tries` of 25 is not sufficient for `search_better` to find the global maximum. The `max.tries` argument needs to be adjusted to ensure it is sufficient to explore the parameter space.

```{r polish, fig.asp=0.5,fig.cap="\\label{trace-compare}Breakdown of index value when using different max.tries in search better in conjunction with search polish. Both attain the same final index value after the polishing while using a max.tries 25 is not sufficient to find the ture maximum."}

p1 <- bind_rows(holes_2d_better, holes_2d_better_polish) %>% 
  explore_trace_interp(col = method) + ggtitle("max.tries = 25")

p2 <- bind_rows(holes_2d_better_max_tries, holes_2d_better_max_tries_polish) %>% 
  explore_trace_interp(col = method) + ggtitle("max.tries = 500")

(p1 | p2) &
  theme(legend.position = "none") 
```


<!-- *Example: The neighbourhood parameter alpha* Add an example on comparing the neighbourhood parameter in search_better & search_posse. -->


<!-- ```{r nrow, echo = TRUE} -->
<!-- # nrow(holes_2d_better_max_tries) -->
<!-- # nrow(holes_2d_pos) -->
<!-- ``` -->


<!-- ```{r alpha, fig.asp=0.5,eval = FALSE} -->
<!-- p1 <- holes_2d_better_max_tries %>% -->
<!--   filter(info != "interpolation") %>% -->
<!--   mutate(id = row_number()) %>% -->
<!--   explore_trace_parameter(var = alpha) + -->
<!--   ylim(0.46, 0.505) + ggtitle("search_better") -->

<!-- p3 <- holes_2d_better_max_tries %>% explore_trace_interp(col = alpha) + ylim(0.8, 0.96) -->

<!-- p2 <- holes_2d_pos %>% -->
<!--   filter(info != "interpolation") %>% -->
<!--   mutate(id = row_number()) %>% -->
<!--   explore_trace_parameter(var = alpha) + ylim(0.46, 0.505) + ggtitle("search_posse") -->

<!-- p4 <- holes_2d_pos %>% explore_trace_interp(col = alpha) + ylim(0.8, 0.96) -->

<!-- (p1 | p2) / (p3 | p4) + plot_layout(guides = "collect") -->
<!-- ``` -->



Exploring the value of an objective function is cna be a simple task. Because the iterator has a time series ordering feature, it goes onto the x-axis as convention. The value of hte objective function goes to the y-axis. Other parameter of interest can be represented using color to present more information. [This is the case for our purpose with method, but when the level goes higher or the parameter is quantitative, color may not be a good option. ]

An option to explore of the searching space is to explore a reduced projection 2D space via principle component analysis or other dimensional reduction methods. In this way, the first two principle components will take up the x and y axis and all the other information will be mapped using other aesthetic attribution. [need to think further about how different type of variables can be shown]. While explore the reduced space is an initial attempt to understand the searching space, there are existing technology for rotating a higher dimensional space for visualisation. Geozoo is an option. It generates random points on the high dimensional space and we can overlay it with the points on the optimisation path to visualise the spread of it on the high-D sphere.







### Explore searching space {#animated}

The parameter to explore can be a vector or matrix instead of scalar, for example, the projection basis in tour. This imposes difficulties in visualisation since humans are bounded to perceive at most three dimensions in a plot. Thus, principal component analysis is used to reduce the dimension of projection bases and the first two principal components are mapped to the x and y axis of the plot. Additional variable of interest could be mapped to the color aesthetics for exploration. 

#### A toy example: understand different stage of search_geodesic

*Example: understand search_geodesic* [feel like this example is merely explaining search geodesic algorithm, so maybe introduce the animated plot here? xxx] `search_geodesic` is a two-stage ascending algorithm with four different stages in the search and a PCA plot useful to understand how the algorithm progresses and the relative position of each basis in the PCA projected 2D space. Starting from the start basis, a directional search is conducted in a narrow neighbourhood on five random directions. The best one is picked and a line search is then run on the geodesic direction to find the target basis. The starting and target bases are then interpolated. In the next iteration, the target basis becomes the current basis and then procedures continues. 

```{r pca, fig.cap = "\\label{pca}PCA plot of search geodesic Coloring by info allows for better understanding of each stage in the geodesic search"}

holes_1d_geo %>% explore_proj_pca(col = info) + 
  theme(legend.position = "bottom")

#xxx the info level need to be reorder to show the start and directional search points better.
```

#### A more complex example: Choosing the initial value for polishing parameter


*Example: initial value for polishing alpha* `search_polish` is a brutal-force algorithm that evaluate 1000 points in the neighbourhood at each loop. Setting an appropriate initial value for polish_alpha would avoid wasting search on large vector space that are not likely to produce higher index value. The default initial value for polishing step is 0.5 and we are interested in whether this is an appropriate initial value to use after `search_geodesic`. The problem is a 1D projection of the small dataset using `search_geodesic` and followed by `search_polish`. The top-left panel of Figure \ref{polish-alpha} displays all the projection bases on the first two principal components, colored by the `polish_alpha`. We can observe that rather than concentrating on the ending basis from `search_geodesic` as what polishing step is designed, `search_polish` searches a much larger vector space, which is unnecessary. Thus a customised smaller initial value for `polish_alpha` would be ideal. One way to do this is to initialised `polish_alpha` as the projection distance between the last two target bases. The top-right panel of Figure \ref{polish-alpha} shows a more desirable concentrated searching space near the ending basis. Both specifications of initial value allow the searches to reach the same ending index values. 


```{r polish-alpha, fig.cap = "\\label{polish-alpha}PCA plot of two different polish alpha initialisations. A default polish alpha = 0.5 searches a larger space that is unncessary while a small customised initial value of polish alpha will search near the ending basis. Both intialisations reach the same ending index values. "}
p1 <- bind_rows(holes_1d_geo, holes_1d_geo_polish_default_alpha) %>% 
  explore_proj_pca(col = as.factor(alpha))

p2 <- bind_rows(holes_1d_geo, holes_1d_geo_polish) %>% 
  explore_proj_pca(col = as.factor(alpha))

p3 <- bind_rows(holes_1d_geo, holes_1d_geo_polish_default_alpha) %>%
  explore_trace_interp(col = method) + ylim(0.75, 0.95)

p4 <- bind_rows(holes_1d_geo, holes_1d_geo_polish) %>% 
  explore_trace_interp(col = method) +  ylim(0.75, 0.95)

((p1 | p2) /(p3 | p4)) +  plot_layout(heights = c(2, 1)) & theme(legend.position = "none") 

```


<!-- # Animated diagnostic plots -->

<!-- ```{r pca-animated, echo = TRUE, eval = FALSE} -->
<!-- holes_1d_geo %>% explore_proj_pca(animate = TRUE, col = info) +  -->
<!--   theme(legend.position = "bottom") -->
<!-- ``` -->

### A comprehensive example of diagnosing a noisy index function

*Example: Noisy index function* The interpolation path of holes index, as seen in Figure \ref{interruption}, is smooth, while this may not be the case for more complicated index functions. `kol_cdf` index, an index function based on Kolmogorov test, compares the difference between a projection matrix and a randomly generated normal distribution based on cumulated distribution function (CDF). Several diagnostic visualisations below show the characteristic of this index function. 

Figure \ref{kol-cdf} compares the tracing plot of the interpolating points for `search_geodesic` and `search_better` on 1D projection for `kol_cdf` index and the interpolation path shows a zig-zag pattern. Polishing step has done much more work to reach the final index value for `search_geodesic` than `search_better` and this indicates that noisy index's favour of a random search method than ascent method [the order of the examples needs to be rearranged since I haven't introduced polishing here. xxx].

```{r kol-cdf,fig.asp=0.5, fig.cap = "\\label{kol-cdf}Comparison of two different searching methods: search_geodesic and search_better on 1D projection problem for a noisier index: kol_cdf. The geodesic search rely heavily on the polishing step to find the final index value while search better works well."}
p1 <- bind_rows(kol_cdf_1d_geodesic, kol_cdf_1d_geodesic_polish) %>% explore_trace_interp(col = method)  + ggtitle("search_geodesic")

p2 <- bind_rows(kol_cdf_1d_better, kol_cdf_1d_better_polish) %>% explore_trace_interp(col = method) + ggtitle("search_better")

(p1 | p2 ) & theme(legend.position = "bottom") & ylim(0, 0.23)
```


The second example for `kol_cdf` index uses 1D projection on the larger dataset. Since there are two informative variables and the projection is 1D, there are two local maximum  when the projection matrix at $[0, 1, 0, 0, 0, 0]$ and $[0, 0, 1 ,0, 0, 0]$. As in Figure \ref{1d-2var-different-seeds}, different local maximum can be found using `search_better` with different seeds while `search_better_random` can always find the global maximum, as in Figure \ref{1d-2var-better-random}, although at a high cost of number of points evaluated. 

```{r 1d-2var-different-seeds,  fig.asp=0.5, fig.cap="\\label{1d-2var-different-seeds}The trace plot search better in a 1D projection problem with two informative variables using different seeds (without polishing). Since there are two informative variables, setting different value for seed will lead search better to find either of the local maximum."}
p5 <- kol_cdf_1d_2var_better %>% explore_trace_interp() + ggtitle("search_better, seed 123456")
p6 <- kol_cdf_1d_2var_better_2 %>% explore_trace_interp() +  ggtitle("search_better, seed 12345")

(p5 | p6) & theme(legend.position = "none") & ylim(0, 0.25)
```

```{r 1d-2var-better-random, fig.asp=0.5, fig.cap = "\\label{1d-2var-better-random}Using search better random for the problem above will result in finding the global maximum but much larger number of iteration is needed."}
kol_cdf_1d_2var_better_random %>% 
  explore_trace_interp() + 
  ylim(0, 0.25) + 
  theme(legend.position = "none") + 
  ggtitle("search_better_random")
```



# Implementation: Ferrn pacakge

Everything is coded up in a package. Package structure

# Conclusion

\clearpage
