---
title: "Visual diagnostics for constrained optimisation with application to guided tours"
blinded: 0
authors: 
  
- name: Author 1
  thanks: The authors gratefully acknowledge ...
  affiliation: Department of YYY, University of XXX
- name: Author 2
  affiliation: Department of ZZZ, University of WWW
keywords:
- optimisation
- projection pursuit 
- guided tourr
- visual 
- diagnostics 
- R
abstract: Friedman & Tukey commented on their initial paper on projection pursuit in 1974 that "the  technique  used  for  maximising  the  projection index strongly influences both the statistical and the computational aspects of the procedure." While many projection pursuit indices have been proposed in the literature, few concerns the optimisation procedure. In this paper, we developed a system of diagnostics aiming to visually learn how the optimisation procedures find its way towards the optimum. This diagnostic system can be applied more generally to help practitioner to unveil the black-box in randomised iterative (optimisation) algorithms. An R package, ferrn, has been created to implement this diagnostic system.
bibliography: biblio.bib
preamble: >
  \usepackage{amssymb, amsmath, mathtools, dsfont, bbm, array, booktabs}
  \usepackage[ruled,vlined, linesnumbered]{algorithm2e}
output:
  bookdown::pdf_book:
    base_format: rticles::asa_article
    fig_height: 4
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,root.dir = here::here())

library(tidyverse)
library(ferrn)
library(patchwork)
library(ggrepel)
library(tourr)
library(gganimate)
```

```{r load-data}
files <- paste0("data/", list.files(here::here("data")))
purrr::walk(.x = files, ~load(here::here(.x), env = globalenv()))

source(here::here("source/indice.r"))
source(here::here("source/sim_data.r"))
```


# Introduction

<!-- We can see from Figure \ref{tour-path} that only the data projected using the accepted and interpolated bases are animated by tour; none of the searching bases sampled by the optimsiation algorithms are presented in the tour animation. Although these algorithms have been demonstrated to find the global maximum in various literature as we cited above, they focus solely on finding the global maximum and fewer tools are available to explore how each algorithm searches the parameter space and compare the results between different algorithms.  -->

<!-- This study is meaningful because **as more complex index functions are proposed based on statistical theory, only computationally effective optimisation algorithms allow us to find the projection basis that presents interesting structure in the data**. -->


<!-- Hence the study **presents a gallery of diagnostic plots applicable to optimisation algorithms.** -->

Visualisation has been widely used in exploratory data analysis. Presenting information in a graphical format often allows people to see information they would otherwise not see. This motivates our work of creating plots to diagnose optimisation algorithms in the context of projection pursuit guided tour, with the aim to understand and compare features of different existing algorithms. 

In an optimization problem the goal is to find the best solution within the space of all feasible solutions which typically is represented by a set of constraints. The problem consists in optimizing an objective function $f: S \rightarrow \mathbb{R}$ with $S \in \mathbb{R}^n$ in a reduced space given by the problem constraints to either minimize or maximize a function. 

Projection pursuit and guided tour are exploratory data analysis tools that detect interesting structure of high dimensional data through projection on low dimensional space. Optimisation is applied here to search for the low dimensional space that finds the most interesting projection.

The remainder of the paper is organised as follows. 
Section \ref{optim} provides a literature review of optimisation methods, specifically the line search methods used in projection pursuit guided tour. 
Section \ref{tour} reviews projection pursuit guided tour, forms the optimisation problem, and introduces three main existing algorithms.
Section \ref{vis-diag} presents the new visual diagnostics design, from forming the data object to the definition of different diagnostic plots with some small examples. 
Section \ref{application} shows the application of how the diagnostic plots designed in section \ref{vis-diag} can be used to understand and compare different algorithms and how they contribute to modifications that improve the algorithms.
Finally, Section \ref{implementation} describes the R package: ferrn, that implements all the visual diagnostics above.


# Optimisation Methods {#optim}

Given an optimisation problem, two basic approaches find the optimum based on different thinking.
An analytical approach aims to find the optimal solution in a finite number of steps, but a potential issue with it is that the closed-form solution may not be available when the problem starts to become complex. 
An iterative approach, on the other hand, finds the optimum based on the idea of making progressive improvement to the current solution. An iterative method may end up finding a local optimum but the progressive nature of the algorithm allows the practitioner to decide when to stop if a desirable accuracy has been achieved.

A traditional while often used in practice is an iterative method called *line search method* [@fletcher2013practical]. In a simple one-dimensional problem of finding the $x$ that minimises $f(x)$, line search achieves the goal via an iterative algorithm in the form of Equation \ref{eq:line-search}.

\begin{equation}
x^{(j + 1)} = x^{(j)} + \alpha_k* d^{(j)}
\label{eq:line-search}
\end{equation}

where $d^{(j)}$ is the searching direction in iteration $j$, and $\alpha_j$ is the step-size. Strictly speaking, $\alpha_k$ is chosen by another minimisation of $f(x^{(j)} + \alpha* d^{(j)})$ with respect to $\alpha$ and theoretical results have demonstrated the global convergence of the algorithm when the exact minimisation of $\alpha_j$ is attained [@curry1944method]. In practice, this second minimisation is rarely implemented due to its computational demanding or even the existence of such a minimisation. A more realistic approach is to impose a mandatory decrease in the objective function for each iteration: $f^{(j+1)}> f^{(j)}$ and despite we lose the guarantee on global convergence, this approach turns out to be efficient in practical problems. 

[Are we using  projection pursuit/guided tour to better understand the convergence of optimization algorithms visually in combination with the algorithms discussed below? Or we are focusing on the optimisation problem only within the projection pursuit context? Some of the problems listed below are also applicable to optimization problem in general too. ppp] 


# Projection pursuit guided tour {#tour}

Modern development of the line search methods focuses on proposing different computation on the searching direction: $d^{(j)}$ and various approximations on the step size: $\alpha_j$ catered for practical optimisation problems. The specific problem context we are interested in is called projection pursuit guided tour. 
Projection pursuit and guided tour are two separate methods in exporatory data analysis focusing on different aspects: coined by @friedman1974projection, projection pursuit detects interesting structures (i.e. clustering, outliers and skewness) in multivariate data via low dimensions projection; whilst guided tour is a particular variation in a broader class of data visualisation method called tour.

Let $\mathbf{X}_{n \times p}$ be the data matrix, an n-d projection can be seen as a linear transformation $T: \mathbb{R}^p \mapsto \mathbb{R}^d$ defined by $\mathbf{P} = \mathbf{X} \cdot \mathbf{A}$, where $\mathbf{P}_{n \times d}$ is the projected data and $\mathbf{A}_{p\times d}$ is the projection basis. Define $f: \mathbb{R}^{n \times d} \mapsto \mathbb{R}$ to be an index function that maps the projection basis $\mathbf{A}$ onto an index value $I$, this function is commonly known as the projection pursuit index (PPI) function, or the index function and is used to measure the "interestingness" of a projection. A number of index functions have been proposed in the literature to detect different data structures, including Legendre index [@friedman1974projection], Hermite index [@hall1989polynomial], natural Hermite index [@cook1993projection], chi-square index [@posse1995projection], LDA index [@lee2005projection] and PDA index [@lee2010projection]. 

In their initial paper, @friedman1974projection noted that "..., the technique used for maximising the projection index strongly influences both the statistical and the computational aspects of the procedure." Hence, effective optimisation algorithms are necessary for projection pursuit to find the bases that give interesting projections. While we leave the formal construction of the optimisation problem and existing algorithms to section \ref{tour-optim}, we outline the general idea here. Given a random starting (current) basis, projection pursuit repeatedly searches for candidate bases nearby until it finds one with higher index value than the current basis. In the second round, that basis becomes the current basis and the repetitive sampling  continues. The process ends until no better basis can be found or one of the termination criteria is reached.

Before introducing the guided tour, we shall be familiar with the general tour method [@cook2008grand]. A tour produces animated visualisation of the high dimensional data via rotating low dimension planes. The smoothness of the animation is ensured by computing a series of intermediate planes between two low dimension planes via geodesic interpolation and we refer readers to @buja2005computational for the mathematical details. Iteratively choosing different low dimension planes and interpolating between them forms a tour path. Different types of tour methods choose the low dimensional planes differently and we mention two other types of tour that are commonly used. A grand tour selects the planes randomly in the high dimensional space and hence serves as an initial exploration of the data. Manual control allows researches to fine-tuning an existing projection by gradually phase in and out one variable. 

Guided tour chooses the planes produced by optimising the projection pursuit index function. Figure \ref{tour-path} shows a sketch of the tour path consisting of the blue frames produced by the projection pursuit optimisation algorithm iteratively and the white frames, which are the interpolations between two blue frames. The tour method has been implemented in the *tourr* package in R, available on the Comprehensive R Archive Network at [https://cran.r-project.org/web/packages/tourr/](https://cran.r-project.org/web/packages/tourr/) [@wickham2011tourrpackage]. 

```{r tour-path, out.width="60%", out.height="40%", fig.align='center', fig.cap="\\label{tour-path}An illustration of the tour path"}
knitr::include_graphics(path = here::here("figures/tour_path_keynote/tour_path_keynote.001.png"),)
```

<!-- The pseudo-code below illustrates the implementation of guided tour in the tourr package. Given an projection pursuit index function and a randomly generated projection basis (current basis), the optimisation procedure produces a target basis inside `generator()`  Both the current basis and the target basis will be supplied to `tour_path()` to prepare information needed for constructing a geodesic path. This information is then used to compute a series of interpolating bases inside the `tour()` function. All the basis will be sent to create animation for visualising the tour in the `animate()` function.   -->


<!-- ```{r algo,eval = FALSE, echo = TRUE} -->
<!-- animation <- function(){ -->

<!--   # compute projection basis  -->
<!--   tour <- function(){ -->

<!--     # construct bases on the tour path -->
<!--     new_geodesic_path <- function(){ -->
<!--       tour_path <- function(){ -->

<!--         # GENERATOR: generate projection basis via projection pursuit -->
<!--         guided_tour <- function(){ -->
<!--           generator <- function(){ -->

<!--             # define projection pursuit index -->
<!--             # generate the target basis from the current basis via optimisation -->
<!--           } -->
<!--         } -->

<!--         # prepare geodesic information needed for interpolating along the tour path -->
<!--       } -->
<!--     } -->

<!--     # INTERPOLATOR: interpolate between the current and target basis  -->
<!--     function(){ -->
<!--       # generate interpolating bases on the geodesic path -->
<!--     } -->
<!--   } -->

<!--   # animate according to different display methods -->
<!-- } -->
<!-- ``` -->


## Optimisation in the tour {#tour-optim}


Now we begin to formulate the optimisation problem. Given a randomly generated starting basis $\mathbf{A}_1$, projection pursuit finds the final projection basis $\mathbf{A}_T$ that satisfies the following optimisation problem: 


\begin{align}
&\arg \max_{\mathbf{A} \in \mathcal{A}} f(\mathbf{X} \cdot \mathbf{A}) \\
&s.t.  \mathbf{A}^{\prime} \mathbf{A} = I_d
\end{align}

where $I_d$ is the d-dimensional identity matrix. The constraint requires the projection basis $\mathbf{A}$ to be an orthogonal matrix with each column vector being orthonormal.

There are several features of this optimisation that are worth noticing. First of all, it is a multivariate constraint optimisation problem. Since the decision variables are the entries of a projection basis, it is required to be orthonormal. It is also likely that the objective function is non-differentiable or the gradient information is simply not available. In this case, we will need to either use some approximation of the gradient or turn to derivative free methods. Given the goal of projection pursuit as finding the basis with the largest index value, the optimisation problem needs to be able to find the global maximum. Along the way, local maximum may also be of our interest since they could present unexpected interesting projections. There is also one computational consideration: the optimisation procedure needs to be easy to compute since the tour animation is played in real-time. 

<!-- - *constraint optimisation*: our problem requires the project basis to be orthonormal -->

<!-- - *Multivariate optimisation*: In projection pursuit, the decision variable includes all the entries in the projection matrix, which is high-dimensional. Researchers would be better off if they could understand the relative position of different projection matrix in the high-dimensional space. -->

<!-- - *optimising non-smooth function*: When the objective function is non-differentiable, derivative information can not be obtained, which means traditional gradient- or Hessian- based methods are not feasible. Stochastic optimisation method could be an alternative to solve these problems. (derivative information is not available) -->

<!-- - *Finding global maximum*: Although finding local maximum is relatively easy with developed algorithms, it is generally hard to guarantee global maximum in a problem where the objective function is complex or the number of decision variables is large. Also, there are discussions on how to avoid getting trapped in a local optimum in the literature. (the exact $\alpha$ that minimised $f(x^{(k)}+ \alpha d^{(k)})$ can be hard to find or sometiems even impossible, imposing certain decrease, how every this doesn't guarantee global convergence) -->

<!-- - *computation speed*: The optimisation procedure needs to be fast to compute since the tour produces real-time animation of the projected data. -->

<!-- - *consistency result in stochastic optimisation*: In stochastic algorithm, researchers usually set a seed to ensure the algorithm produces the same result for every run. This practice supports reproducibility, while less efforts has been made to guarantee different seeds will provide the same result.  -->

<!-- - *role of interpolation in PP optimisation*: An optimisation procedure usually involves iteratively finding projection bases that maximise the index function, while tour requires geodesic interpolation between these bases to produce a continuous view for the users. It would be interesting to see if the interpolated bases could, in reverse, help the optimisation reach faster convergence. -->

<!-- the interpolation can be seen as giving information on the direction of the derivative. --> 

<!-- *Think about how does your package help people to understand optimisation* -->

<!--  - diagnostic on stochastic optim -->
<!--  - vis the progression of multi-parameter decision variable  -->
<!--  - understanding learning rate - neighbourhood parameter -->
<!--  - understand where the local & global maximum is found - trace plot - see if noisy function -->

## Existing algorithms

Below we introduce three possible algorithms: `search_better` and `search_better_random` are derivative free methods that sample candidate bases in the neighbourhood, whilst `search_geodesic` is an analogue of gradient ascent on the manifold of the projection basis.

`search_better` is a random search device that samples a candidate basis $\mathbf{A}_{l}$ in the neighbourhood of the current basis $\mathbf{A}_{\text{cur}}$ by $\mathbf{A}_{l} = (1- \alpha)\mathbf{A}_{\text{cur}} + \alpha \mathbf{A}_{\text{rand}}$ where $\alpha$ controls the radius of the sampling neighbourhood and $\mathbf{A}_{\text{rand}}$ is a randomly generated matrix with the same dimension as $\mathbf{A}_{\text{cur}}$. $\mathbf{A}_{l}$ is then orthogonalised to ensure the orthonormal constraint is fulfilled. When a basis is found with index value higher than the current basis $\mathbf{A}_{\text{cur}}$, the search terminates and outputs the basis for guided tour to construct an interpolation path. The next iteration of search begins after adjusting $\alpha$ by a cooling parameter: $\alpha_{j+1} = \alpha_j * \text{cooling}$. Another termination condition is when the maximum number of iteration $l_{\max}$ is reached. A slightly different cooling scheme has been proposed by @posse1995projection to include a halving parameter $c$. Rather than reducing the radius of the searching neighbourhood, $\alpha$, at each iteration, Posse's design only adjust $\alpha$ if the last search takes more than $c$ times to find an accepted basis to avoid the searching space being reduced too fast. The algorithm of `search_better` is summarised in Algorithm \ref{random-search}.  [mention orthonormalise to ensure the constraint is fulfilled; don't use derivative information but a random search]



\begin{algorithm}
\SetAlgoLined
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
    \Input{$\mathbf{A}_{\text{cur}}$, $f$, $\alpha$, $l_{\max}$} 
    \Output{$\mathbf{A}_{l}$}
  initialisation\;
  Set $l = 1$\;
  \While{$l < l_{\max}$}{
    Generate $\mathbf{A}_{l} = (1- \alpha)\mathbf{A}_{\text{cur}} + \alpha \mathbf{A}_{\text{rand}}$ and orthogonise $\mathbf{A}_{l}$\;
    Compute $I_{l}  = f(\mathbf{A}_{l})$\;
    \If{$I_{l} > I_{\text{cur}}$}{
      \KwRet{$\mathbf{A}_{l}$} \;
      }
    $l = l + 1$\;
  }
  \caption{random search}
  \label{random-search}
\end{algorithm}

Simulated annealing [@kirkpatrick1983optimization, @bertsimas1993simulated] uses the same sampling process as `search_better` but allow a probabilistic acceptance of a basis with lower index value based on a cooling scheme $T(l)$. Given an initial $T_0$, the temperature at iteration $l$ is defined as $T(l) = \frac{T_0}{\log(l + 1)}$. When a candidate basis fails to have an index value larger than the current basis, simulated annealing gives it a second chance to be accepted with probability $$P= \min\left\{\exp\left[-\frac{I_{\text{cur}} - I_{l}}{T(l)}\right],1\right\}$$ where $I_{(\cdot)}$ denotes the index value of a given basis. This implementation allows the algorithm to jump out of a local maximum and enables a more holistic search of the whole parameter space. This feature is particularly useful when the dimension of the projected space is smaller than the number of informative variables in the dataset (i.e. a one dimensional projection of the dataset with two informative variables). The algorithm can be written as replacing line 5-8 of Algorithm \ref{random-search} with Algorithm \ref{simulated_annealing}.

\begin{algorithm}
\SetAlgoLined
    Compute $I_{l} = f(\mathbf{A}_{l})$ and $T(l) = \frac{T_0}{\log(l + 1)}$\;
      \eIf{$I_{l} > I_{\text{cur}}$}{
        \KwRet{$\mathbf{A}_{l}$} \;
      }{
        Compute $P= \min\left\{\exp\left[-\frac{I_{\text{cur}} -I_{l}}{T(l)}\right],1\right\}$\;
        Draw $U$ from a uniform distribution: $U \sim \text{Unif(0, 1)}$\;
        \If{$P > U$}{
           \KwRet{$\mathbf{A}_{l}$} \;
        }
      }
  \caption{simulated annealing}
  \label{simulated_annealing}
\end{algorithm}



@cook1995grand used a gradient ascent algorithm on the manifold of the projection bases. In gradient ascent, one first find the direction for improvment via computing the gradient information. In `search_geodesic`, $2n$ bases are first generated in a tiny neighbourhood of the current basis, controlled by the neighbourhood parameter $\delta$. A geodesic is then constructed using the current basis and one of the $2n$ bases that have the highest index value. If the neighbourhood parameter $\delta$ is tiny, the geodesic constructed is an analogue of the gradient information in the curved space and can work as the searching direction. In gradient ascent, the next step is to conduct a line search to find the best improvement along the gradient direction. In `search_geodesic`, this step is replaced by optimising the index value along the geodesic direction constructed before over an 90 degree angle from $-\pi/4$ to $\pi/4$. The optima $\mathbf{A}_{**}$ is outputted for the current iteration if the percentage change in the index value between $\mathbf{A}_{**}$ and $\mathbf{A}_{\text{cur}}$ is greater than a threshold value. As above, another termination condition is when $l_{\max}$ is reached.  Algorithm \ref{search-geodesic} summarises the steps in geodesic search.

\begin{algorithm}
\SetAlgoLined
\SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
    \Input{$\mathbf{A}_{\text{cur}}$, $f$, $l_{\max}$, $n = 5$, $\delta$}
    \Output{$\mathbf{A}_{**}$}
  initialisation\;
  Set $l = 1$\;
  \While{$l < l_{\max}$}{
    Generate $2n$ bases in a small neighbourhood, $\delta$, of $\mathbf{A}_{\text{cur}}$ and ensure orthogonality \;
    Find the one with the largest index value: $\mathbf{A}_{*}$\;
    Construct the geodesic $\mathcal{G}$ from $\mathbf{A}_{\text{cur}}$ to $\mathbf{A}_{*}$\;
    Optimise the index value on the geodesic $\mathcal{G}$ over a 90 degree window to produce the optima $\mathbf{A}_{**}$  \;
    Compute $I_{**} = f(\mathbf{A}_{**})$, $p_{\text{diff}} = (I_{**} - I_{\text{cur}})/I_{**}$\;
      \If{$p_{\text{diff}} > 0.001$}{
         \KwRet{$\mathbf{A}_{**}$} \;
      }
    $l = l + 1$\;
  }
  \caption{search geodesic}
  \label{search-geodesic}
\end{algorithm}


# Visual diagnostics {#vis-diag}

To be able to make diagnostic plot, the optimisation algorithm should populate a data structure that contains the key elements of the algorithm. When the algorithm runs, key information regarding the decision variable, objective function and hyper-parameters needs to be recorded and stored as a data object for future analysis.

## Data structure for diagnostics 

<!-- [Do I need this paragraph to details the grammar of graphic? xxx - don't think so; not particularly relevant - assume most people know it roughly] -->
<!-- A layer includes 1) the data that powers the plot; 2) a geometric object that represents the visual shape of the data and 3) relevant statistical transformation that transform the information in the data to the information used to draw the geometric object. An important concept in the grammar of graphic is *aesthetic mapping*. Aesthetic mapping links the variable in a dataset to information needed to produce a geometric object. For example, we map one variable on the x-axis and another on the y-axis to create a scatterplot. Another example of creating a boxplot involves 1) mapping one variable on the x-axis and 2) transforming another variable to its five-point-summary and mapping the five-point-summary to the y-axis.  -->

In the optimisation algorithms for projection pursuit, three main elements to record are 1) projection bases: $\mathbf{A}$, 2) index values: $I$, and 3) State: $S$, which labels the observation with detailed stage in the optimisation. Multiple iterators are also  needed to index the data collected at different levels. $t$ is a unique identifier that prescribes the natural ordering of each observation; $j$ is the counter for each search-and-interpolate round, which remains the same within one round and has an increment of one once a new round starts. $l$ is the counter for each search/interpolation allowing us to know how many basis the algorithm has searched before finding one to output. There are other parameters that are of our interest and we denote them as *$V_{p}$*. In projection pursuit, this includes $V_1 = \text{method}$, which tags the name of the algorithm used and $V_2 = \text{alpha}$, the neighbourhood parameter that controls the size in sampling candidate bases.  A matrix notation of the data structure is presented in Equation \ref{eq:data-structure}.


<!-- # ```{r iterators, out.width = "100%", out.height = "25%", fig.cap = "\\label{iterators} A sketch of the design of iterators in iterative algorithms."} -->
<!-- # knitr::include_graphics(here::here("figures", "iterators.png")) -->
<!-- # ``` -->



\begin{equation}
\left[
\begin{array}{c|ccc|cc|cc}
t & \mathbf{A} & I & S & j &  l  & V_{1} & V_{2}\\
\hline
1 & \mathbf{A}_1 & I_1 & S_1 & 1 & 1 & V_{11} & V_{12}\\
\hline
2 & \mathbf{A}_2 & I_2 & S_2 & 2 & 1  & V_{21}  & V_{22}\\
3 & \mathbf{A}_3 & I_3 & S_3 & 2 & 2  & V_{31}  & V_{32}\\
\vdots & \vdots &\vdots &\vdots  &\vdots & \vdots &\vdots  &\vdots\\
\vdots & \vdots & \vdots &\vdots & 2 & l_2 & \vdots  & \vdots\\
\hline
\vdots &\vdots & \vdots &\vdots & 2  & 1& \vdots & \vdots\\
\vdots &\vdots &\vdots &\vdots & 2 & 2& \vdots &  \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots & \vdots  &\vdots \\
\vdots &\vdots &\vdots &\vdots & 2 & k_2 &\vdots  & \vdots\\
\hline
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
\hline
\vdots & \vdots & \vdots &\vdots  & J &  1 & \vdots & \vdots \\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
T & \mathbf{A}_T & I_T &S_T  & J &  l_{J} & V_{T1}& V_{T2}\\
\hline
\vdots &\vdots & \vdots &\vdots & J  & 1& \vdots & \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots & \vdots  &\vdots \\
\vdots &\vdots &\vdots &\vdots & J & k_J &\vdots  & \vdots\\
\hline
\vdots& \vdots & \vdots & \vdots & J+1 & 1 & \vdots& \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
T^\prime & \mathbf{A}_{T^\prime} & I_{T^\prime} &S_{T^\prime}  & J+1 &  l_{J+1} & V_{T^\prime 1}& V_{T^\prime 2}\\
\end{array}
\right]
= 
\left[
\begin{array}{c}
\text{column name} \\
\hline
\text{search (start basis)} \\
\hline
\text{search} \\
\text{search} \\
\vdots \\
\text{search (accepted basis)} \\
\hline
\text{interpolate} \\
\text{interpolate} \\
\vdots \\
\text{interpolate} \\
\hline
\vdots \\
\hline
\text{search} \\
\vdots \\
\text{search (final basis)} \\
\hline
\text{interpolate} \\
\vdots \\
\text{interpolate} \\
\hline
\text{search (no output)} \\
\vdots \\
\text{search (no output)} \\
\end{array}
\right]
\label{eq:data-structure}
\end{equation}

where $T^{\prime} = T + k_{J}+ l_{J+1}$. Note that we deliberately denote the last round of search as $j = J+1$ and in that round there is no output/interpolation basis and the algorithm terminates. This notation allows us to denote the last complete search-and-interpolate round as round $J$ and hence the final basis is $A_T$ and highest index value found is $I_T$.

[outside the paper: I find the notation of current/target basis is confusing because the target basis in round $j$ becomes the current basis in round $j+1$. Also, when we start to have polish, the target basis may not be the current basis in the next round... The place where current/target is most appropriate is probably when describing the interpolation where the first one is always the current basis and the last is always the target basis. I think it is better to leave this language in the code]


```{r glb-obj, out.width= "100%", out.height="20%", fig.cap="\\label{glb-obj}The data structure in projection pursuit guided tour."}
knitr::include_graphics(path = here::here("figures/global_obj.png"))
```

It is worth noticing that the data structure constructed above meets the tidy data principle [@wickham2014tidy] that states

1) each observation forms a row, 
2) each variable forms a column, and 
3) each type of observational unit forms a table

The wrangling and visualisation of tidy data have been greatly simplified by the well-known dplyr[@dplyr] and ggplot2[@ggplot2] package.


With a constructed data object, the construction of diagnostic plots is inspired by the concept of grammar of graphic [@wickham2010layered], which powers the primary graphical system in R, ggplot2 [@ggplot2]. In grammar of graphic, plots are not defined by its appearance (i.e. boxplot, histogram, scatter plot etc) but by "stacked layers". Using this design, ggplot does not have to develop a gazillion of functions that each produces a different type of plot from a different data structure. Instead, it aesthetically maps variables (and its statistical transformation) in a dataset to different geometric objects (points, lines, box-and-whisker etc) and builds the plot through overlaying different layers. 

<!-- The plot design of visual diagnostics depends on the characteristics of the variables to plot. If the searching points are of interest [point geom is not good; need summarise. While updating points are more manageable -> point geom]. However, one must realise that with hundreds or thousands of searching points, exploring the sample space in animation could be slow and this is because of the time it takes to render hundreds point. [stratefy may help? - may need more work here.] -->

<!-- ### Explore the value of objective function {#static} -->

<!-- In tour, all the points recorded in the global object can be roughly divided into two broad categories: searching points and interpolating points -->

<!-- - *Searching points* include the observations recorded in the searching algorithm to find the target basis. The points for target bases is also included in the searching points and there is one such point per `tries`. -->

<!-- - *interpolating points* exist in the guided tour to produce continuous animated view from one target basis to another and it doesn't have `loop` value. -->


<!-- In the current implementation of the `tourr` package, while the target basis generated by the projection pursuit can be accessed later via `save_history()`, interpolating bases and those randomly nearby bases generated in the optimisation are not stored. This creates difficulties for fully understand the behaviour of the optimisation and interpolation of tour in complex scenario **[need a rephrase this part].**  -->

## Check how hard the optimiser is working

A primary interest of diagnosing an optimisation algorithm is to study how it progressively finds its optimum. One way of doing it is to plot the index value across its natural ordering $t$, however, it usually takes the algorithm much longer to find a better basis than the current one towards the end of the search. 
When plotting the searching observations as points in a plot, the space each iteration takes will be proportional to the number of points in that iteration.

<!-- different iterations may have different number of points and, towards the end of the search there could easily be hundreds of bases being tested before the target basis is found. In the plot, points from those iterations towards the end will occupy the vast majority of the plot space. This motivates to use summarisation.  Rather than knowing the index value of *every* basis, we are more interested to have a general summary of all the index value in that iteration and more importantly, the basis with the largest index value (since it prescribes the next geodesic interpolation and future searches).  -->


<!-- . We may risk losing information on how many points it takes to find the target basis by displaying the boxplot alone for all `tries`. Thus, the number of point in each iteration is displayed at the bottom of each box and we provide options to switch iteration with small number of points to a point geometry, which is achieved via a `cutoff` argument. A line geometry is also added to link the points with the largest index value in each iteration. This helps to visualise the improvement made in each iteration. Using the concept of *gramma of graphics* [@wickham2010layered], the plot for exploring index value can be defined in three layers as: -->

Another option is to use summarisation for each iteration. Boxplot is a suitable candidate that provides five points summary of the data, however, there are two pieces of information missing from the boxplot: 1) It does not report the number of points, and 2) the position of the last basis. This could be remedied by adding more layers using the concept of grammar of graphics. A label geometry is added at the bottom of the plot to show the number of points in each iteration and a line geometry links the last basis in each iteration. Further, an option to switch between displaying points and boxplot geometry is helpful since point geometry can be more intuitive for the iteration with few observations. This is achieved via a `cutoff` parameter. Below we define the searching point plot based on four different component layers 


- Layer 1: boxplot geom
  - data: group by $j$ and filter the observations in the groups that have count greater than `cutoff = 15`.
  - x: $j$ is mapped to the x-axis
  - y: the statistical transformed index value: $Q_{I^{\prime}_t}(q)$ is mapped to the y-axis where $Q_X(q)$, $q = 0, 25, 50, 75, 100$ finds the qth-quantile of $X$ and $I^{\prime}_t$ denotes the index value of all the searching bases defined in Matrix \ref{eq:data-structure}.

- Layer 2: point geom
  - data: group by $j$ and filter the observations in the group that have count less than `cutoff = 15`.
  - x: $j$ is mapped to the x-axis
  - y: $I$ is mapped to the y-axis 

- Layer 3: line geom
  - data: filter the points with the highest index value in group $j$
  - x: $j$ is mapped to the x-axis
  - y: $I$ is mapped to the y-axis 
  
- Layer 4: label geom
  - data: A count table of the number of observation in each iteration
  - x: $j$ is mapped to the x-axis
  - y: the y-axis for each label is $0.99 * \text{MIN}$, where $\text{MIN}$ is the smallest index value in the whole data object
  - label: the number of observation is mapped to the label


<!-- *Example: exploring searching points* We choose variables `x1`, `x2`, `x3`, `x8`, `x9` and `x10` to perform a 2D projection with tour. Parameter `search_f = tour::search_better` and `max.tries = 500` is used. The index value of the searching points are shown in Figure \ref{toy-search}. The label at the bottom indicates the number of observations in each iteration and facilitates the choice of `cutoff` argument (by default `cutoff = 15`). We learn that the `search_better` quickly finds better projection basis with higher index value at first and then takes longer to find a better one later. -->


Figure \ref{toy-search} presents a comparison between the two plotting options discussed above. Using the natural order to plot searching points distorts the point from the first few iterations and over-emphasizes the searches in the last few iterations. The second plot design evenly presents the summarised information for each iteration while allowing for a switch to the full information for the iteration with small number of observations.  

```{r toy-search, fig.height=5.5, fig.cap="\\label{toy-search}A comparison of plotting the same search points with different plot designs. In the upper plot, points from the last three iterations span the vast majority of the plotting space leaving the first few iterations being squeezed together whilst the lower plot spaces each iteration evenly and presents summary information easy to read."}

dt <- holes_2d_better_max_tries %>%
  filter(info != "interpolation") %>% 
  mutate(id = row_number()) 

p1 <- dt %>% explore_trace_search() + 
  ggtitle("satisfactory") + 
  scale_color_botanical(palette = "daisy")

p2 <- holes_2d_better %>% explore_trace_search() + 
  ggtitle("not satisfactory") + 
  scale_color_botanical(palette = "daisy")

p1 / p2

```


## Examining the optimisation progress

Sometimes, we may be interested in exploring the points on the interpolation path since these are the points that will be played by the tour animation. The plot definition is as follows:

- Layer 1: point geom
  - data: filter the observations with $S = \text{interpolation}$ and mutate $t$ to be the row number of the subsetted tibble
  - x: $t$ is mapped to the x-axis
  - y: $I$ is mapped to the y-axis 

- Layer 2: line geom
  - using line geometry for the same data and aesthetics

Figure \ref{toy-interp} presents the interpolation of three different tour paths. The upper plot shows a desirable interpolation in each iteration with the index value being progressively and monotonically increasing.  While in the middle plot, the increases towards the target basis is not monotonical in the last two iterations and interpolated basis with higher index value can be found on the tour path. The lower plot is constructed using `search_better_random`, where a basis with smaller index value  has a probabilistic chance of being accepted and we can observe a much more involved pattern. In iteration three, the probabilistic acceptance produces a monotonically decreasing interpolation whilst in iteration five, six and seven, with also a probabilistic acceptance, the interpolation is now non-monotonical. When the target basis has a higher index value, iteration eight reaches this basis by first decreasing the index value. While the interpolation plot shows different possibilities of the change in index value on the tour path, the diagnosis of the validity of each pattern and the related optimisation algorithms will be postponed to Section \ref{application}.

```{r toy-interp, fig.cap = "\\label{toy-interp} This is a toy example of plotting interpolated points"}
p1 <- holes_2d_better_max_tries %>% explore_trace_interp() + theme(legend.position = "none") + 
  scale_color_botanical(palette = "fern")

p2 <- interrupt_no %>% explore_trace_interp() + theme(legend.position = "none") + 
  scale_color_botanical(palette = "fern")

p3 <- holes_2d_better_random %>% explore_trace_interp() + theme(legend.position = "none") + 
  scale_color_botanical(palette = "fern")

p1 / p2 / p3
```

\ 

## Understanding the optimiser's coverage of the search space

Apart from checking the progression of an optimiser, another interesting aspect is to visualise how the search looks like in its parameter space. Given the orthonormality constraint, the projection bases $\mathbf{A}_{p \times d}$ lives on the surface of a $p \times d$ dimension sphere, where the dimension can easily go above five (5). Visualising the search paths on the original high diemnsional sphere would require skills from the viewer to preceive rotation of geometry in higher dimensional space (d > 3) while an easier alternative is to view the reduced space via some dimension reduction methods i.e. Principal component analysis. To better visual the serach path as an embedding of a hollow sphere, random points on the high dimensional sphere is generated using package `geozoo` and PCA is conducted on both the bases and the points on the surface of the sphere.

The visualisation can thus be defined as

<!-- We first flat each projection bases $\mathbf{A}$ to a row vector and stack them by rows. This gives a matrix of dimension $n$ by $(p*d)$. Centering and scaling are usually done before performing PCA and we denotes the pre-processed matrix as $\tilde{\mathbf{A}}_{n \times (p \times d)}$. By definition, the principal components maximise the variance of the projection bases in the reduced space. Using lagrange multipler, this is equivalent to find the eigenvector of the variance-covariance matrix: $\Sigma = \tilde{\mathbf{A}}^T\tilde{\mathbf{A}}$. Decomposing $\tilde{\mathbf{A}}$ using singular value decomposition (SVD) gives $$\tilde{\mathbf{A}} = \mathbf{U} \mathbf{\lambda} \mathbf{V}$$ and hence the variance-covariance matrix -->

<!-- $$\Sigma = \tilde{\mathbf{A}}^T\tilde{\mathbf{A}} = (\mathbf{V}^T \mathbf{\lambda}^T \mathbf{U}^T) (\mathbf{U} \mathbf{\lambda} \mathbf{V}) = \mathbf{V}^T \mathbf{\lambda}^2 \mathbf{V}$$ -->

<!-- has eigenvector $\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_{p\times d}]$. The projection mapping to the x- and y-axis uses the first two principal components and hence $$\tilde{\mathbf{A}} \mathbf{v}_1 \text{ and } \tilde{\mathbf{A}} \mathbf{v}_2$$ -->

<!-- The resulting two vectors are of dimension $n \times 1$, which is compatible with the dimension of the global object. THis means all the iterators and interesting other variables defined in the global object can be bound with the these two projections and can be mapped to other aesthetics (i.e. colour). -->

- Layer 1: point geom
  - data: subset the basis of interest and arrange into a matrix format; perform PCA on the basis matrix and compute the projected basis on the first two principal components; bind the variables from the original global object and form a tibble
  - x: the projected basis on the first principal component
  - y: the projected basis on the second principal component
  - colour: $V$ is mapped to the colour aesthetic
  
Figure \ref{toy-pca} shows the first two principal components of two search paths. Starting from the same position, two searching methods take different paths to get its final bases. There are two theoretical bases because they correspond to the same data projection $Y$ with a 180 degree rotation. What differentiate the two optimisers is that `search_better()` also involve random evaluation of points in the sphere during its sampling process while `search_geodesic()` doesn't and this stochastic rather than deterministic approach can be useful in complex scenario.


```{r toy-pca, fig.height = 4, fig.cap = "\\label{toy-pca}PCA plot of search geodesic colouring by info allows for better understanding of each stage in the geodesic search"}
# path1 <- holes_1d_geo$basis %>% 
#   flatten_dbl() %>% 
#   matrix(ncol = 5, byrow = TRUE)
# path2 <- holes_1d_better$basis %>% 
#   flatten_dbl() %>% 
#   matrix(ncol = 5, byrow = TRUE)
# set.seed(1)
# sphere <- geozoo::sphere.hollow(p = 5, n = 1000)$point
# theoretical <- matrix(c(0, 1, 0, 0, 0,
#                         0, -1, 0, 0, 0), nrow = 2, byrow = TRUE)
# 
# dt <- rbind(sphere, path1, path2, theoretical)
# pca <- dt %>% prcomp(scale. = TRUE)
# #summary(pca)
# pca_pred <- predict(pca) %>%
#   as_tibble() %>%
#   mutate(method = c(rep("sphere", nrow(sphere)),
#                     holes_1d_geo$method,
#                     holes_1d_better$method,
#                     rep("theoretical", 2))) %>%
#   group_by(method) %>%
#   mutate(id = row_number(),
#          id = ifelse(method == "sphere", 1, id)) %>%
#   ungroup()
# 
# p <- pca_pred %>%
#   ggplot(aes(x = PC1, y = PC2, color = method))+
#   geom_point() +
#   geom_point(data = pca_pred %>% filter(method == "theoretical"), 
#              size = 10) + 
#   geom_point(data = pca_pred %>% filter(method %in% c("search_better", "search_geodesic") & id == 1), size = 10) + 
#   theme(aspect.ratio = 1) + 
#   scale_color_botanical(palette = "cherry")
# 
# p

p <- bind_rows(holes_1d_geo, holes_1d_better) %>% 
  bind_theoretical(matrix(c(0, 1, 0, 0, 0), nrow = 5), 
                   index = tourr::holes(), raw_data = boa5) %>% 
  bind_theoretical(matrix(c(0, -1, 0, 0,0), nrow = 5), 
                   index = tourr::holes(), raw_data  = boa5)%>%
  explore_space_pca(col = method)  + 
  scale_color_botanical(palette = "cherry") + 
  theme(legend.position = "bottom")

p
```


## Animating the diagnostic plots

Animating the plots introduced above is useful, especially in the case of PCA polot since it shows the bases found by the optimiser in its natural order.

<!-- all the plots above can be animated if wanted - importance is how the data is moving around  -->

<!-- show an example of some snap shots  of the animation and provide a link... for animated pca plot -->

<!-- This is particularly useful when plotting the PCA plot since it allways us to tract the progress of the search. -->

<!-- The projection bases live in a high dimensional sphere, whch cna be simulated using `geozoo` package. -->

<!-- The reason to simulate random points on the hihg dimensiaon sphere is to emphasize  taht the projection bases lives on the surface of the sphere. -->

```{r fig.cap = "\\label{toy-pca-aniamted}A selected number of frames from the animated PCA plot. With animation, it is easier to track the progrssion from the start to finish in each algorithm.", fig.height=4, fig.align="center"}
# ani <- p + theme(legend.position = "none") +
#   transition_states(id) +
#   shadow_mark()
# 
# ani <- bind_rows(holes_1d_geo, holes_1d_better) %>% 
#   bind_theoretical(matrix(c(0, 1, 0, 0, 0), nrow = 5), 
#                    index = tourr::holes(), raw_data = data) %>% 
#   bind_theoretical(matrix(c(0, -1, 0, 0,0), nrow = 5), 
#                    index = tourr::holes(), raw_data  = data)%>%
#   explore_space_pca(col = method, animate = TRUE)  + 
#   scale_color_botanical(palette = "cherry") 
# 
# animate(ani, nframes = 350, device = "png",
#         renderer = file_renderer("figures/pca/",
#                                  prefix = "pca", overwrite = TRUE))

frames <- c("0002", "0058", "0078", "0117", "0172", "0350")
ani <- paste0(here::here("figures/"), "pca/", "pca", frames, ".png")
rl <- lapply(ani, png::readPNG)
gl <-  lapply(rl, grid::rasterGrob)
wrap_plots(gl) 
```


## The tour looking at itself

Viewing the bases on the reduced space via PCA shed some lights on the space the optimisers have explored, the visualisation on the original $p \times d$ dimension enables a stereoscopic view of the search. To view a high dimensional ($d \ge 3$) object on a screen, an approach is to play the rotation of the object in animation. This can be done via a regular grand tour or a slice tour [@laa2020slice], which emphasizes the points that are close to a section of the sphere.

Compare to the PCA plot, the animated rotation (tour) display, in Figure \ref{toy-tour}, gives a more holistic view of the search of the optimiser and these additional information gained from the animation is cruicial. This is because in essense, PCA presents one reduced space that maximises the variance and it inherently amplifies the spread of the randomly evaluated bases in the search since they have larger variance than the bases on the interpolation path. Hence when the interest is to view the interpolation path in the space, an animated rotation display provides the view of high dimension from different angles.

```{r toy-tour, fig.cap="\\label{toy-tour}A selected number of frames from the tour animation for viewing the 5D space of all the projection bases. The second frame on the top row view the space from a direction that is close to the one in PCA plot. The tour animation allows for a more holistic view of the full space in high dimensions from different angles."}
set.seed(1)
sphere <- geozoo::sphere.hollow(p = 5, n = 1000)$point
path1 <- holes_1d_better$basis %>% 
  flatten_dbl() %>% matrix(ncol = 5, byrow = TRUE)
path2 <- holes_1d_geo$basis %>% 
  flatten_dbl() %>% matrix(ncol = 5, byrow = TRUE)
theoretical <- matrix(c(0, 1, 0, 0, 0,
                        0, -1, 0, 0, 0), nrow = 2, byrow = TRUE)
start <- get_start(holes_1d_better) %>% pull(basis) %>% .[[1]] %>% matrix(nrow = 1)
dt <- rbind(sphere, path1, path2, theoretical, start)
colnames(dt) <- c(map_chr(1:5, ~paste0("x", .x)))
pal <- c("#D3D3D3",c("#524340",  #orchre
                     "#B4B754",  # green
                     "#F3B422" # yellow
                     ))
color <- c(rep(pal[1], nrow(sphere)),
           rep(pal[2], nrow(path1)),
           rep(pal[3], nrow(path2)),
           rep(pal[4], 2), 
           pal[2] # for start
           )
cex <- c(rep(1, nrow(sphere)),
         rep(1.5, nrow(path1)), 
         rep(1.5, nrow(path2)), 
         rep(5, 2), 
         5 # for start
         )


#animate_xy(dt, col = color, cex = cex, tour_path = grand_tour()) # trial


# set.seed(123)
# render(
#   dt,
#   tour_path = grand_tour(),
#   dev = "png",
#   display = display_xy(col=color,cex = cex, axes = "bottomleft"),
#   rescale = FALSE,
#   frames = 500,
#   here::here("figures","tour", "tour%03d.png")
# )

frames <- c("001", "143", "078", "117", "172", "350")
ani <- paste0(here::here("figures/"), "tour/", "tour", frames, ".png")
rl <- lapply(ani, png::readPNG)
gl <-  lapply(rl, grid::rasterGrob)
wrap_plots(gl) 
```



# Diagnosing an optimiser {#application}

For a particular index function, the best algorithm to optimise relates to the character of the index and the data. If the index function is smooth and has single maximum, all of the three algorithms introduced above can find the maximum.  When multiple optima are presented, `search_better` may stuck in the local maximum and in the case where the index function is non-smooth, `search_geodesic` may even fail to find the maximum. In this section, examples will be presented  to outline how the diagnostic plots can be used to compare the performance of the algorithms in different scenarios.

## Simulation setup

Random variables with different structures has been simulated and the distribution of each is presented in Equation \ref{eq:sim-norm} to \ref{eq:sim-x7}. Variable `x1`, `x8`, `x9` and `x10` are normal distributed with zero mean and unit variance and `x2` to `x7` are mixtures of normal distributions with varied weights and locations. The mixture variables have been scaled to have an overall unit variance before running the projection pursuit.

\begin{align}
x_1 \overset{d}{=} x_8 \overset{d}{=} x_9 \overset{d}{=} x_{10}& \sim \mathcal{N}(0, 1) \label{eq:sim-norm} \\
x_2 &\sim 0.5 \mathcal{N}(-3, 1) + 0.5 \mathcal{N}(3, 1)\label{eq:sim-x2}\\
\Pr(x_3) &= 
\begin{cases}
0.5 & \text{if $x_3 = -1$ or $1$}\\
0 & \text{otherwise}
\end{cases}\label{eq:sim-x3}\\
x_4 &\sim 0.25 \mathcal{N}(-3, 1) + 0.75 \mathcal{N}(3, 1) \label{eq:sim-x4}\\
x_5 &\sim \frac{1}{3} \mathcal{N}(-5, 1) + \frac{1}{3} \mathcal{N}(0, 1) + \frac{1}{3} \mathcal{N}(5, 1)\label{eq:sim-x5}\\
x_6 &\sim 0.45 \mathcal{N}(-5, 1) + 0.1 \mathcal{N}(0, 1) + 0.45 \mathcal{N}(5, 1)\label{eq:sim-x6}\\
x_7 &\sim 0.5 \mathcal{N}(-5, 1) + 0.5 \mathcal{N}(5, 1) 
\label{eq:sim-x7}
\end{align}

## A problem of not monotonic

We use the same dataset as the toy example above to explore the search function `search_better` and we want to learn how the index value changes on the interpolation path for the `holes` index. From the left panel of Figure \ref{interruption}, we observe that when interpolating from the current basis to the target basis, the index value may not be monotone: we could reach a basis with a higher index value than the target basis on the interpolation path. In this sense, we would be better off using the basis with the highest index value on the interpolation path as the current basis for the next iteration (rather than using the target basis). Hence, an interruption is constructed to accept the interpolating bases only up to the one with the largest index value. After implementing this interruption, the search finds higher final index value with fewer steps as shown in the right panel of Figure \ref{interruption}.

```{r interruption,fig.asp=0.5, fig.cap = "\\label{interruption}Trace plots of the interpolated basis with and without interruption. The interruption stops the interpolation when the index value starts to decrease at id = 60. The implementation of the interuption finds an ending basis with higher index value using fewer steps. "}
p1_anno <- interrupt_no %>% filter(info == "interpolation") %>% mutate(id = row_number()) %>% filter(id %in% c(44, 60, 62)) %>% 
  mutate(anno = c("current basis", "interpolated basis", "target basis"))

p1 <- interrupt_no %>% mutate(id = row_number() - 1) %>% explore_trace_interp() + ggtitle("without interruption") + 
  geom_point(data = p1_anno) + 
  geom_label_repel(data = p1_anno, aes(label = anno), box.padding = 0.5) + ylim(0.8, 0.9) + xlim(0, 80) + 
  theme(legend.position = "none")
  
p2 <- interrupt_yes %>% explore_trace_interp() + ggtitle("with interruption") + ylim(0.8, 0.9) + xlim(0, 80) + 
  theme(legend.position = "none")

p1 | p2

```


## Close but not close enough

<!-- In the previous two sections, only the iterator and the index value are mapped onto the x and y aesthetics of the plot; while more aesthetics i.e. colour, could be added to compare other parameters in the global object. -->

<!-- Two examples are shown below to explore and compare different searching methods and neighbourhood parameter alpha.  -->

Once the final basis has been found by an algorithm, one may want to push further to investigate whether there's an even better basis in the close neighbourhood. This motivates the polish search where the final basis is supplied to a new guided tour to search for any local breakthrough. The following code demonstrates the usage of `search_polish`:

```{r sample-code, eval = FALSE, echo = TRUE}
library(tourr)
library(ferrn)

set.seed(123456)
holes_2d_geo <- 
  animate_xy(boa6,
             tour_path = guided_tour(holes(), d = 2, 
                                     search_f = search_geodesic),
             rescale = FALSE, verbose = TRUE)

last_basis <- get_best(holes_2d_geo) %>% pull(basis) %>% .[[1]]

set.seed(123456)
holes_2d_geo_polish <- 
  animate_xy(boa6, 
             tour_path = guided_tour(holes(), d = 2, 
                                     search_f = search_polish),
             rescale = FALSE, verbose = TRUE, start = last_basis)
```

Similar to `search_better` as a stochastic random search, `search_polish` has a different scheme of reducing the search neighbourhood. In each search-interpolation iteration, `search_better` has a fixed neighbourhood parameter alpha and this alpha is reduced by another cooling parameter only after an iteration finishes. On the contrary, `search_polish` allows alpha to be reduced during each iteration to exploit the search in the neighbourhood. Further, to avoid the case where alpha becomes too small and the further search is meaningless, three more stopping criteria have been added, on top of the original `max.tries`. These include:

1) the distance between the candidate basis and the current basis needs to be larger than 1e-3;
2) the percentage change of the index value need to be larger than 1e-5; and
3) the alpha parameter on itself need to be larger than 0.01

Figure \ref{geo-polish} presents the final projections found before and after applying `search_polish` using the demo code. Polish search improves the index value from `r get_best(holes_2d_geo)$index_val %>% format(digits = 4)`  to `r get_best(holes_2d_geo_polish)$index_val %>% format(digits = 4)` with reduction of weights on the non-informative variables, namely `V1`, `V8`, `V9` and `V10`. When appears in the projected data in Figure \ref{geo-polish}, polish works to sharpen the edges of each cluster.




```{r geo-polish, fig.height= 2.5, fig.cap = "\\label{geo-polish}Projected data before and after polishing. Weights on axis other than V2 and V7 have been further reduced and the edges of each cluster have become more obvious."}
# set.seed(123456)
# render(
#   boa6,
#   tour_path = guided_tour(holes(), d = 2, search_f = search_geodesic),
#   dev = "png",
#   display = display_xy(axes = "bottomleft", verbose = TRUE),
#   rescale = FALSE,
#   frames = 100,
#   file = here::here("figures","geo_polish", "before%03d.png")
# )
# 
# last_basis <- get_best(holes_2d_geo)$basis %>% .[[1]]
# 
# set.seed(123456)
# render(
#   boa6,
#   tour_path =  guided_tour(holes(), d = 2, search_f = search_polish, alpha = 0.1),
#   dev = "png",
#   display = display_xy(axes = "bottomleft", verbose = TRUE),
#   start = last_basis,
#   rescale = FALSE,
#   frames = 100,
#   file = here::here("figures","geo_polish", "after%03d.png")
# )

# set.seed(123456)
# holes_2d_geo <- animate_xy(boa6,tour_path =
#                              guided_tour(holes(), d = 2,
#                                          search_f = search_geodesic),
#                            rescale = FALSE, verbose = TRUE)
# last_basis <- get_best(holes_2d_geo)$basis %>% .[[1]]
# 
# set.seed(123456)
# holes_2d_geo_polish <- animate_xy(boa6, tour_path =
#                                     guided_tour(holes(), d = 2,
#                                                 search_f = search_polish, alpha = 0.1),
#                                   rescale = FALSE, verbose = TRUE,
#                                   start = last_basis)
# 
# save(holes_2d_geo, file = here::here("data", "holes_2d_geo.rda"))
# save(holes_2d_geo_polish, file = here::here("data", "holes_2d_geo_polish.rda"))


before <- png::readPNG(here::here("figures","geo_polish", "before100.png"))
after <- png::readPNG(here::here("figures","geo_polish", "after004.png"))
gl <-  lapply(list(before, after), grid::rasterGrob)
wrap_plots(gl) 
```



## Seeing the signal in the noise 

Up until this point, the index functions are smooth, meaning the trace of interpolation points is smooth, while this is not the case for all the index functions. `norm_kol`, a 1D projection function based on Kolmogorov test, compares the difference between the 1D projected data, $\mathbf{P}_{n \times 1}$ and a randomly generated normal distribution, $y_n$ based on the empirical cumulated distribution function (ECDF). Denotes the ECDF function as $F_{.}(u)$ with the subscript indicating the projection or the random normal variable,  the `norm_kol` index is defined by

$$\max \left[F_{\mathbf{P}}(u) - F_{y}(u)\right]$$

Figure \ref{kol-cdf} compares the tracing plot of two algorithms: `search_geodesic` and `search_better`. This time, the interpolated path is no longer smooth when using either algorithm. It is also obvious that `search_geodesic` fails to optimise this rough index since there is barely improvment of the index value. On the other hand, `search_better` is doing a relatively good job on the optimisation for the 5-variable dataset `boa5`. The theoretical best basis [`r matrix(c(0, 1, 0, 0, 0))`] produces an index value of `r kol_1d_better %>% bind_theoretical(matrix = matrix(c(0, 1, 0, 0, 0), nrow = 5, ncol = 1), index = tourr::norm_kol(), raw_data = boa5) %>% tail(1) %>% pull(index_val)` and `search_better` finds the final basis [`r get_best(kol_1d_better)$basis %>% .[[1]] %>% format(digits = 4)`] with an index value of `r get_best(kol_1d_better)$index_val %>% format(digits = 4)`. A further polish step will give a marginal improvement of index value to `r get_best(kol_1d_better_polish)$index_val` with a basis of [`r get_best(kol_1d_better_polish)$basis %>% .[[1]] %>% format(digits = 4)`]. At this stage, the difference between the theoretical best and what has been found is likely due to simulation error since the best possible basis for a simulated data will be slightly off the theoretical best basis, which is derived based on the distributional assumption in Equation \ref{eq:sim-norm} to \ref{eq:sim-x7}. 

```{r kol-cdf,fig.asp=0.5, fig.cap = "\\label{kol-cdf}Trace plot of search_geodesic and search_better on 1D projection problem for a noisy index: norm_kol. Search_geodesic has varely made progression while search_better has successfully found the informative "}
# set.seed(123456)
# kol_1d_geo <- 
#   animate_dist(boa5, tour_path = guided_tour(norm_kol(), d = 1,
#                                              search_f =  search_geodesic, max.tries = 100),
#                rescale = FALSE, verbose = TRUE)
# 
# set.seed(123456)
# kol_1d_better <- 
#   animate_dist(boa5, tour_path = guided_tour(norm_kol(), d = 1,
#                                              search_f =  search_better, max.tries = 100),
#                rescale = FALSE, verbose = TRUE)
# save(kol_1d_geo, file = here::here("data", "kol_1d_geo.rda"))
# save(kol_1d_better, file = here::here("data", "kol_1d_better.rda"))

index <- tourr::norm_kol()
theo_best_index_val <- index(as.matrix(boa5) %*% matrix(c(0, 1, 0, 0, 0), nrow = 5, ncol = 1))

bind_rows(kol_1d_geo, kol_1d_better) %>% 
  explore_trace_interp(col = method, group = method) +
  facet_wrap(vars(method), ncol = 2) + 
  geom_hline(yintercept = theo_best_index_val, alpha = 0.5, linetype = 2) + 
  scale_color_botanical() +
  theme(legend.position = "none", aspect.ratio = 1)

```

The second experiment with the noisy index is to understand how the optimisers perform when there are multiple informative variables in the case of 1D projection. The dataset used is `boa6` with `x2` and `x7` being informative. The two possible theoretical best bases are [`r matrix(c(0, 1, 0, 0, 0, 0), nrow = 6)`] and [`r matrix(c(0, 0, 1, 0, 0, 0), nrow = 6)`] with index value `r index <- tourr::norm_kol(); proj <- as.matrix(boa6) %*% matrix(c(0, 1, 0, 0, 0, 0), nrow = 6); index(proj)` and `r index <- tourr::norm_kol(); proj <- as.matrix(boa6) %*% matrix(c(0, 0, 1, 0, 0, 0), nrow = 6); index(proj)` repectively and hence, the global maximum happens when variable `x7` is found.

A simulation with 20 different seeds shows that `search_better` can find either one of the informative variable but there's no guarantee on which seed finds which variable. `search_better_random` manages to find the global maximum via a metropolis-hasting algorithm without being affected by the seed. To ensure the algorithm finish the search in a timely manner, it is necessary to set a reasonable initial temperature $t_0$ so that the probability of accepting an inferior basis is not too high.  Figure \ref{kol-pca-sim}, .... Figure \ref{kol-pca} shows the comparison, in terms of the trace plot and PCA plot, between `search_better_random` and two `search_better` algorithms where one finds `x2` and another finds `x7`.  The trace plot shows that the global maximum has been found by `search_better_random` and one `search_better`, but not the other. The PCA plot further illustrate how the two algorithms finds the global maximum: `search_better_random` travels along the top right corner on the projected 2D circle while the path of `search_better` is much shorter and only goes pass in between the two theoretical bases.

(there's more things going on about both algorithms experimenting with different seeds.)


```{r fig.height = 8}
set.seed(123)
seed <- sample.int(10000, 20)
# kol_1d_2var_better_sim <- list()
# kol_1d_2var_better_sim_polish <- list()
# for (i in 1: length(seed)){
#   set.seed(seed[i])
#   kol_1d_2var_better_sim[[i]] <-
#     animate_dist(boa6,
#                  tour_path = guided_tour(norm_kol(), d = 1,
#                                          search_f = search_better, max.tries = 100),
#                  rescale = FALSE, verbose = TRUE)
# 
# 
#   last_basis <- get_best(kol_1d_2var_better_sim[[i]])$basis %>% .[[1]]
# 
#   set.seed(1842)
#   kol_1d_2var_better_sim_polish[[i]] <-
#     animate_dist(boa6,
#                  tour_path = guided_tour(norm_kol(), d = 1,
#                                          search_f = search_polish, max.tries = 100),
#                  rescale = FALSE, verbose = TRUE,
#                  start = last_basis)
# }


# save(kol_1d_2var_better_sim, file = here::here("data", "kol_1d_2var_better_sim.rda"))
# save(kol_1d_2var_better_sim_polish, file = here::here("data", "kol_1d_2var_better_sim_polish.rda"))

polish_list <- purrr::map2(kol_1d_2var_better_sim_polish,seed,  ~.x %>% dplyr::mutate(seed = .y)) 
better_list <- purrr::map2(kol_1d_2var_better_sim,seed,  ~.x %>% dplyr::mutate(seed = .y)) 
polish_tidy <- do.call(dplyr::bind_rows, polish_list)
better_tidy <- do.call(dplyr::bind_rows, better_list)
all_tidy_better <- bind_rows(better_tidy, polish_tidy)

best <- get_best(all_tidy_better, group = seed) %>% arrange(index_val)

all_summary <- best$basis %>% 
  purrr::flatten_dbl() %>% matrix(ncol = 6, byrow = TRUE) %>% 
  as_tibble() %>% 
  rename("V7" = "V3", "V8" = "V4", "V9" = "V5", "V10" = "V6") %>% 
  mutate(IndexVal = best$index_val, seed = as.factor(best$seed)) %>% 
  pivot_longer(-c(seed, IndexVal), names_to = "VarFound", values_to = "value") %>% 
  group_by(seed, IndexVal) %>% 
  filter(abs(value) == max(abs(value))) %>% 
  arrange(IndexVal) %>% ungroup()

pca <- all_tidy_better%>% 
  mutate(seed = as.factor(seed)) %>% 
  left_join(all_summary %>% select(seed, VarFound, IndexVal), by = "seed") %>%
  bind_theoretical(matrix = matrix(c(0, 1, 0, 0, 0, 0), nrow = 6),
                                        index = tourr::norm_kol(),
                                        raw_data = boa6) %>% 
   bind_theoretical(matrix = matrix(c(0, 0, 1, 0, 0, 0), nrow = 6),
                                        index = tourr::norm_kol(),
                                        raw_data = boa6) %>% 
  
  compute_pca()

random <- pca$aug %>% filter(method == "randomly_generated") %>% select(-seed)
theoretical <- pca$aug %>% filter(info == "theoretical") %>% select(-seed)

pca$aug %>% filter(method != "randomly_generated"& info != "theoretical") %>% 
  group_by(seed) %>% arrange(IndexVal) %>% 
  ggplot(aes(x = PC1, y = PC2, col = VarFound)) + 
  geom_point(data = random, col = "grey", size = 0.3) +
  geom_point(aes(col = VarFound), alpha = 0.1) + 
  geom_point(data = theoretical, size = 5, col = botanical_palettes$cherry[3]) + 
  geom_path(data = pca$aug %>% filter(info == "interpolation"), size = 2) + 
  geom_point(data = pca$aug %>% get_best(group = seed), size = 5) +
  geom_point(data = pca$aug %>% get_start() %>% filter(method == "search_better_random"), size = 3) +
  facet_wrap(vars(fct_reorder(seed, IndexVal))) +
  theme_void() + 
  theme(aspect.ratio = 1, legend.position = "bottom") + 
  scale_color_botanical(palette = "fern")  
```


```{r fig.height= 8}
set.seed(123)
seed <- sample.int(10000, 20)
# kol_1d_2var_better_random_sim <- list()
# kol_1d_2var_better_random_sim_polish <- list()
# for (i in 1: length(seed)){
#   cat("i = ", i, "seed = ", seed[i], "\n")
#   set.seed(seed[i])
#   kol_1d_2var_better_random_sim[[i]] <-
#     animate_dist(boa6,
#                  tour_path = guided_tour(norm_kol(), d = 1,
#                                          search_f = search_better, max.tries = 500, t0 = 1e-6, alpha = 0.7),
#                  rescale = FALSE, verbose = TRUE)
# 
# 
#   last_basis <- get_best(kol_1d_2var_better_random_sim[[i]])$basis %>% .[[1]]
# 
#   set.seed(seed[i])
#   kol_1d_2var_better_random_sim_polish[[i]] <-
#     animate_dist(boa6,
#                  tour_path = guided_tour(norm_kol(), d = 1,
#                                          search_f = search_polish, max.tries = 200),
#                  rescale = FALSE, verbose = TRUE,
#                  start = last_basis)
#   
# }


#save(kol_1d_2var_better_random_sim, file = here::here("data", "kol_1d_2var_better_random_sim.rda"))
#save(kol_1d_2var_better_random_sim_polish, file = here::here("data", "kol_1d_2var_better_random_sim_polish.rda"))


polish_list <- purrr::map2(kol_1d_2var_better_random_sim_polish,seed,  ~.x %>% dplyr::mutate(seed = .y)) 
better_list <- purrr::map2(kol_1d_2var_better_random_sim,seed,  ~.x %>% dplyr::mutate(seed = .y)) 
polish_tidy <- do.call(dplyr::bind_rows, polish_list)
better_tidy <- do.call(dplyr::bind_rows, better_list)
all_tidy <- bind_rows(better_tidy, polish_tidy)

best <- get_best(polish_tidy, group = seed) %>% arrange(index_val)
best <- get_best(better_tidy, group = seed) %>% arrange(index_val)
best <- get_best(all_tidy, group = seed) %>% arrange(index_val)

all_summary <- best$basis %>% 
  purrr::flatten_dbl() %>% matrix(ncol = 6, byrow = TRUE) %>% 
  as_tibble() %>% 
  rename("V7" = "V3", "V8" = "V4", "V9" = "V5", "V10" = "V6") %>% 
  mutate(IndexVal = best$index_val, seed = as.factor(best$seed)) %>% 
  pivot_longer(-c(seed, IndexVal), names_to = "VarFound", values_to = "value") %>% 
  group_by(seed, IndexVal) %>% 
  filter(abs(value) == max(abs(value))) %>% 
  arrange(IndexVal) %>% ungroup()

pca <- all_tidy %>% 
  mutate(seed = as.factor(seed)) %>% 
  left_join(all_summary %>% select(seed, VarFound, IndexVal), by = "seed") %>%
  bind_theoretical(matrix = matrix(c(0, 1, 0, 0, 0, 0), nrow = 6),
                                        index = tourr::norm_kol(),
                                        raw_data = boa6) %>% 
   bind_theoretical(matrix = matrix(c(0, 0, 1, 0, 0, 0), nrow = 6),
                                        index = tourr::norm_kol(),
                                        raw_data = boa6) %>% 
  
  compute_pca()

random <- pca$aug %>% filter(method == "randomly_generated") %>% select(-seed)
theoretical <- pca$aug %>% filter(info == "theoretical") %>% select(-seed)

pca$aug %>% filter(method != "randomly_generated"& info != "theoretical") %>% 
  group_by(seed) %>% arrange(IndexVal) %>% 
  ggplot(aes(x = PC1, y = PC2, col = VarFound)) + 
  geom_point(data = random, col = "grey", size = 0.3) +
  geom_point(aes(col = VarFound), alpha = 0.05) + 
  geom_point(data = theoretical, size = 5, col = botanical_palettes$cherry[3]) + 
  geom_path(data = pca$aug %>% filter(info == "interpolation"), size = 2) + 
  geom_point(data = pca$aug %>% get_best(group = seed), size = 5) +
  facet_wrap(vars(fct_reorder(seed, IndexVal))) +
  theme_void() + 
  theme(aspect.ratio = 1, legend.position = "bottom") + 
  scale_color_botanical(palette = "fern") 
```



```{r kol-pca, fig.height = 5, fig.cap = "\\label{kol-pca}Comparison between search_better_random and two different search_better. In the trace plot, the two dashed horizontal lines are the index values of the two theoretical bases. serach_better_random and one of the serach_better have found the global maximum but the other fails. The lower panel shows the PCA plot for each of the three algorithms. The two yellow dots, again, correspond to the theoretical bases with the one on the right being the global maximum. The paths of two search_better are relatively short and simple while the search_better_random has a more sophisticated path. This is due to the fact that the interpolation at each iteration are longer, which shows its ability to find new bases at a distance."}

# set.seed(123456)
# kol_1d_2var_better <- 
#   animate_dist(boa6, 
#                tour_path = guided_tour(norm_kol(), d = 1,
#                                        search_f = search_better, max.tries = 100),
#                rescale = FALSE, verbose = TRUE)
# 
# 
# set.seed(1234567)
# kol_1d_2var_better_2 <- 
#   animate_dist(boa6, 
#                tour_path = guided_tour(norm_kol(), d = 1,
#                                        search_f = search_better, max.tries = 100),
#                rescale = FALSE, verbose = TRUE)
# 
# set.seed(12345678)
# kol_1d_2var_better_2 <- 
#   animate_dist(boa6, 
#                tour_path = guided_tour(norm_kol(), d = 1,
#                                        search_f = search_better, max.tries = 100),
#                rescale = FALSE, verbose = TRUE)
# 
# 
# 
# 
# set.seed(123456)
# kol_1d_2var_better_random <- 
#   animate_dist(boa6, 
#                tour_path = guided_tour(norm_kol(), d = 1,
#                                        search_f = search_better_random,
#                                        max.tries = 2000, t0 = 1e-5),
#                rescale = FALSE, verbose = TRUE)
# 
# save(kol_1d_2var_better, file = here::here("data", "kol_1d_2var_better.rda"))
# save(kol_1d_2var_better_2, file = here::here("data", "kol_1d_2var_better_2.rda"))
# save(kol_1d_2var_better_random, file = here::here("data", "kol_1d_2var_better_random.rda"))
# 
# p1 <- bind_rows(kol_1d_2var_better %>% mutate(algo = "better 1"),
#           kol_1d_2var_better_2 %>% mutate(algo = "better 2"),
#           kol_1d_2var_better_random %>% mutate(algo = "better random")) %>% 
#   explore_trace_interp(group = algo, col = algo) + 
#   facet_wrap(vars(algo), ncol = 3)  + 
#   scale_color_botanical(palette = "banksia")  + 
#   theme(legend.position = "none", aspect.ratio = 1)
# 
# pca <- bind_rows(kol_1d_2var_better %>% mutate(algo = "better 1"),
#           kol_1d_2var_better_2 %>% mutate(algo = "better 2"),
#           kol_1d_2var_better_random %>% mutate(algo = "better random")) %>% 
#   bind_theoretical(matrix = matrix(c(0, 1, 0, 0, 0, 0), nrow = 6),
#                                         index = tourr::norm_kol(),
#                                         raw_data = boa6) %>% 
#    bind_theoretical(matrix = matrix(c(0, 0, 1, 0, 0, 0), nrow = 6),
#                                         index = tourr::norm_kol(),
#                                         raw_data = boa6) %>% 
#   compute_pca()
  
dt <- bind_rows(all_tidy_better %>% filter(seed == 1842) %>% mutate(algo = "better V2"),
          all_tidy_better %>% filter(seed == 9982) %>% mutate(algo = "better V7"),
          all_tidy %>% filter(seed == 4761) %>% mutate(algo = "better random"))

index <- tourr::norm_kol() 
theo1 <- index(as.matrix(boa6) %*% matrix(c(0, 1, 0, 0, 0, 0), nrow = 6))
theo2 <- index(as.matrix(boa6) %*% matrix(c(0, 0, 1, 0, 0, 0), nrow = 6))
p1 <- dt %>% 
  explore_trace_interp(group = algo, col = algo) + 
  facet_wrap(vars(algo), ncol = 3)  + 
  geom_hline(yintercept = theo1, alpha = 0.5, linetype = 2) + 
  geom_hline(yintercept = theo2, alpha = 0.5, linetype = 2) + 
  scale_color_botanical(palette = "banksia")  + 
  theme(legend.position = "none", aspect.ratio = 1)

pca <- dt %>% 
  bind_theoretical(matrix = matrix(c(0, 1, 0, 0, 0, 0), nrow = 6),
                                        index = tourr::norm_kol(),
                                        raw_data = boa6) %>% 
   bind_theoretical(matrix = matrix(c(0, 0, 1, 0, 0, 0), nrow = 6),
                                        index = tourr::norm_kol(),
                                        raw_data = boa6) %>% 
  compute_pca()
random <- pca$aug %>% filter(method == "randomly_generated") %>% select(-algo)
theoretical <- pca$aug %>% filter(info == "theoretical") %>% select(-algo)

p2 <- pca$aug %>% filter(method != "randomly_generated" & info != "theoretical") %>% 
  ggplot(aes(x = PC1, y = PC2, col = algo)) + 
  geom_point(data = random, col = "grey") +
  geom_point(aes(col = algo), alpha = 0.05) + 
  geom_point(data = theoretical, size = 5, col = botanical_palettes$cherry[3]) + 
  geom_path(data = pca$aug %>% filter(info == "interpolation"), size = 2) + 
  geom_point(data = pca$aug %>% get_best(group = algo), size = 5) +
  geom_point(data = pca$aug %>% get_start() %>% filter(method %in% c("search_better_random", "search_better")),  size = 3) +
  facet_wrap(vars(algo)) +
  theme_void() + 
  theme(legend.position = "none", aspect.ratio = 1) + 
  scale_color_botanical(palette = "banksia")  

wrap_plots(p1, p2, ncol = 1) 

```


# Implementation {#implementation}

(further notice on the implementation in the tour package will be discussed in the tour developer meeting.)
The implementation of this projection has been divided into two pacakges, where the data collection happens when a tour is running and hence implemented in the existing `tourr` package. The diagnostics of the algorithm via plots and other auxiliary functions have been implemented in a new pacakge, `ferrn`. 

When the optimisation ends, the data object will be stored and printed (it can be turned off by supplying argument `print = FALSE`). Additional messages during the optimisation can be displayed by argument `verbose = TRUE`.  Notice that the tibble object allows the list-column `basis` to be printed out nicely with the dimension of the projection basis readily available. Below presents a sample of the data object. 

```{r code, eval = FALSE}
set.seed(123456)
holes_1d_better <- animate_dist(data, tour_path = guided_tour(holes(), d = 1,
                                           search_f =  search_better),
                  rescale = FALSE, verbose = TRUE)

#save(temp, file = here::here("vignettes", "data", "temp.rda"))
```


```{r glb-obj-1}
holes_1d_better %>% 
  dplyr::select(id, basis, index_val, info, tries, loop, method, alpha) %>% 
  head(10) 
```


Once the data object has been obtained, the package, `ferrn`, provides automatic diagnostic plots as described in Section \ref{vis-diag}. The structure of package functionality has been listed below.

**Main functions**

- `explore_trace_search()`: produce summary plots, as shown in Figure \ref{toy-search}
- `explore_trace_interp()`: produce trace plots for the interpolation points, as shown in Figure \ref{toy-interp}
- `explore_space_pca() `: produce plots of projection basis on the reduced space by PCA, as shown in Figure \ref{toy-pca}. Animated version in Figure \ref{toy-pca-aniamted} can be acquired via the argument `animate = TRUE`
- `explore_space_tour()`: produce animated tour view on the full space of the projection bases, as shown in Figure \ref{toy-tour}

**Auxiliary functions**

-  `get_*()` extract and, in some cases, manipulate certain components from the existing data object.
    - `get_best()`: extract the best basis found by the algorithm
    - `get_start()`: extract the starting basis
    - `get_interp()`: extract the observations in the interpolation and prepare to be supplied to `explore_trace_interp()`. 
    - `get_search_count()`: produce the summary table of the number of observation in each iteration and prepare to be supplied to `explore_trace_search()`
    - `get_basis_matrix()`: flattern all the bases into a row and stack them to form a large matrix for `explore_space_tour()`
    
- `bind_*()` incorporate additional information that is outside the tour optimisation.

  - `bind_theoretical()`: incorporate the best possible basis to the existing data object with the supply of the index function and original data for producing the index value. 
  - `bind_random()`: generates a large number of points on the surface of a high dimensional sphere and attaches it to the existing data object and output as a tibble object. `bind_random_matrix()` returns a matrix. 
  
  <!-- This binding is used in the space plots to compare the position of the theoretical best basis to the ones being searched by the algorithm. The following chunk presents its usage -->
<!-- These are also useful in the space plots since it preserves the spherical structure of the spaces of the projection bases, in the computation of PCA for `explore_space_pca()` and animation of `explore_space_tour()`.  -->
  

- Color and relevel
  - `botanical_palettes`:  a collection of color palettes from Australian native plants Quantitative palettes include daisy, banksia and cherry and sequantial palettes cotain fern and acacia.
  - `botanical_pal()`: a color interpolator useful for `scale_color_botanical()`
  - `scale_color_botanical()`: a ggplot construction for using botanical palettes. 
  - `relevel_geo()`: manipulate the level in the `info` column in the data object when geodesic search is used
  - `relevel_better()`: manipulate the level in the `info` column in the data object when search_better related search is used


```{r eval = FALSE}
# Below are codes that generate the plots in Section 4 
# using functions from ferrn package
bind_rows(holes_2d_better_max_tries %>% mutate(max_tries = 500), 
          holes_2d_better %>% mutate(max_tries = 25)) %>% 
  explore_trace_search(group = max_tries)
```

```{r eval = FALSE}
bind_rows(holes_2d_better_max_tries %>% mutate(group = "max_tries: 500"), 
          interrupt_no %>% mutate(group = "no interruption"), 
          holes_2d_better_random %>% mutate(group = "better_random")) %>% 
  mutate(group = fct_relevel(group, c("max_tries: 500", 
                                      "no interruption", 
                                      "better_random"))) %>% 
  explore_trace_interp(group = group) + 
  theme(legend.position = "none")
```

```{r eval = FALSE}
bind_rows(holes_1d_geo, holes_1d_better) %>% 
  bind_theoretical(matrix(c(0, 1, 0, 0, 0), nrow = 5), 
                   index = tourr::holes(), raw_data = data) %>% 
  bind_theoretical(matrix(c(0, -1, 0, 0,0), nrow = 5), 
                   index = tourr::holes(), raw_data  = data)%>%
  explore_space_pca(col = method)  + 
  scale_color_botanical(palette = "cherry")
```

```{r eval = FALSE}
bind_rows(holes_1d_geo, holes_1d_better) %>% 
  explore_space_tour(color = method)
```

# Conclusion

This paper has illustrated setting up a data object that can be used for diagnosing a complex optimisation procedure. The ideas were illustrated using the optimisers available for projection pursuit guided tour. Here the constraint is the orthornormality condition of the projection bases. The approach used here could be broadly applied to understand other constrained optimisers.

<!-- something about the diagnostic plots  -->
Four diagnostic plots have been introduced in the paper to investigate the progression and the projection space of an optimser, ranging from simple summary plot to advanced high dimensional animation. The implementation of these visualisations are designed to be easy-to-use with each plot can be produced with a simple supply of the data object and few additional arguments. More advanced users may decide to modify on top of the basic diagnostic pltos or even build their own. 

<!-- summarise what we have done and what might be done in the future. -->
Most of the work in this project has been translated into code in two pacakges: the collection of the data object is implemented in the existing `tourr` package; manipulation and visualisation of the data object are implemented in the new `ferrn` package. Equipped with handy tools to diagnose the performance of optimisers, future work can extend the diagnostics to a wider range of index functions i.e. scagnostics, association, and information index [@laa2020using] and understand how the optimisers behave for index functions with different structures.


\clearpage


