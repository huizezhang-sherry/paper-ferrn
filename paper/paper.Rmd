---
title: Title here
blinded: 0
authors: 
  
- name: Author 1
  thanks: The authors gratefully acknowledge ...
  affiliation: Department of YYY, University of XXX
- name: Author 2
  affiliation: Department of ZZZ, University of WWW
keywords:
- optimisation
- projection pursuit 
- guided tourr
- visual 
- diagnostics 
- R
abstract: Friedman & Tukey commented on their initial paper on projection pursuit in 1974 that "the  technique  used  for  maximising  the  projection index strongly influences both the statistical and the computational aspects of the procedure." While many projection pursuit indices have been proposed in the literature, few concerns the optimisation procedure. In this paper, we developed a system of diagnostics aiming to visually learn how the optimisation procedures find its way towards the optimum. This diagnostic system can be applied to more generally to help practitioner to unveil the black-box in randomised iterative (optimisation) algorithms. An R package, ferrn, has been created to implement this diagnostic system.
bibliography: biblio.bib
preamble: >
  \usepackage{amssymb, amsmath, mathtools, dsfont, bbm}
  \usepackage[ruled,vlined, linesnumbered]{algorithm2e}
output: rticles::asa_article
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(tidyverse)
library(ferrn)
library(patchwork)
library(ggrepel)
```

```{r load-data}
files <- paste0("data/", list.files(here::here("data")))
purrr::walk(.x = files, ~load(here::here(.x), env = globalenv()))

source(here::here("source/indice.r"))
source(here::here("source/sim_data.r"))
```


# Introduction

In an optimization problem the goal is to find the best solution within the space of all feasible solutions which typically is represented by a set of constraints. The problem consists in optimizing an objective function $f: S \rightarrow \Re$ with $S \in \Re^n$ in a reduced spaced given by the problem constraints to either minimize or maximize a function.... Gradient based optimization has been typically used to solve such problems. However, there are many situations where derivatives of an objective function are unavailable or unreliable and therefore traditional methods based on derivatives are not the best option to solve an optimization problem.

Derivative free methods provide another option to optimise the objective function without evaluating any gradient or Hessian information and a particular class of methods, direct search, has gained popularity through its conceptual simplicity.  However, the whole searching process in the algorithm remains a black-box. Plots are usually used to evaluate and compare the performance of different algorithms but it can easily become tedious because the code will have to be modified significantly when comparing different parameters in the algorithms. For example, a categorical variable with 5 levels can be easily mapped onto colour while mapping another categorical variable with 30 levels will not make the plot informative. Thus the plot needs to be re-designed to better suit the characteristics of the parameter (whether the parameter is a scalar or a matrix? whether the parameter is quantitative or categorical? If categorical, how many levels does the parameter have?). This motivates the  design of a visual diagnostic framework for optimisation algorithms based on the idea of a *global object*. 

The remainder of the paper is organised as follows. Section \ref{DFO} gives a general overview of optimisation methods, specifically derivative free optimisation. 
Section \ref{tour} introduces the optimisation problem in a specific problem: projection pursuit guided tour and existing derivative free algorithms for solving it. 
Section \ref{vis-diag} and \ref{vis-diag-plots} together present the new construction of a visual diagnostic framework that unveil the black-box of optimisation. Section \ref{vis-diag} focuses on motivational ideas and data structure and Section \ref{vis-diag-plots} provides definitions of different diagnostic plots using grammar of graphic and examples. 
Finally, Section \ref{implementation} mentions the R package: ferrn that implements the visual diagnostics described above.

# Derivative free optimisation {#DFO}

Given an objective function $f$, one way of optimising it is to equate its gradient to zero. In modern optimisation problems, gradient information can be hard to evaluate or sometimes even impossible and Derivative-Free Optimisation (DFO) methods can be useful to approach these problems. One common class of methods in DFO is *Direct-search methods*. Coined by @hooke1961direct, direct search methods don't require any gradient or Hessian information and has gained its popularity through its simplicity in use and reliability in solving complicated practical problems. Depending on whether a random sample is used in the search, this class of methods can be further classified as *stochastic* or *deterministic*.  The stochastic version of the direct-search methods will be the main optimisation procedure analysed in this paper.

[How about adding more details into derivative free methods? ppp]


<!-- The two most common methods in DFO are *direct-search methods* and *model-based methods* and this paper dedicates to the discussion of direct-search methods, which .  -->


<!-- A well-known example of it is the Nelder-Mead algorithm [@nelder1965simplex] and it enjoys the popularity due to its simplicity and reliability. [feel like this sentence can be expanded to include more information. xxx]. -->







## Difficulties

[Are we using  projection pursuit/guided tour to better understand the convergence of optimization algorithms visually in combination with the algorithms discussed below? Or we are focusing on the optimisation problem only within the projection pursuit context? Some of the problems listed below are also applicable to optimization problem in general too. ppp] 

Below are listed several issues in projection pursuit optimisation. Some are general optimisation problems, while others are more specific too PP optimisation. 

- *Finding global maximum*: Although finding local maximum is relatively easy with developed algorithms, it is generally hard to guarantee global maximum in a problem where the objective function is complex or the number of decision variables is large. Also, there are discussions on how to avoid getting trapped in a local optimum in the literature.

- *optimising non-smooth function*: When the objective function is non-differentiable, derivative information can not be obtained, which means traditional gradient- or Hessian- based methods are not feasible. Stochastic optimisation method could be an alternative to solve these problems.

- *computation speed*: The optimisation procedure needs to be fast to compute since the tour produces real-time animation of the projected data.

- *consistency result in stochastic optimisation*: In stochastic algorithm, researchers usually set a seed to ensure the algorithm produces the same result for every run. This practice supports reproducibility, while less efforts has been made to guarantee different seeds will provide the same result. 

- *high-dimensional decision variable*: In projection pursuit, the decision variable includes all the entries in the projection matrix, which is high-dimensional. Researchers would be better off if they could understand the relative position of different projection matrix in the high-dimensional space. 

- *role of interpolation in PP optimisation*: An optimisation procedure usually involves iteratively finding projection bases that maximise the index function, while tour requires geodesic interpolation between these bases to produce a continuous view for the users. It would be interesting to see if the interpolated bases could, in reverse, help the optimisation reach faster convergence.

<!-- the interpolation can be seen as giving information on the direction of the derivative. -->


<!-- - is `search_geodesic` widely known in the research community - doesn't seem to find paper introducing this except (Cook, 1995). Any tools available to  better understand its searching process? -->

*Think about how does your package help people to understand optimisation*
 
 - diagnostic on stochastic optim
 - vis the progression of multi-parameter decision variable 
 - understanding learning rate - neighbourhood parameter
 - understand where the local & global maximum is found - trace plot - see if noisy function


\newpage

# Projection pursuit guided tour {#tour}

Coined by @friedman1974projection, projection pursuit detects interesting structures (i.e. clustering, outliers and skewness) in multivariate data via projecting onto lower dimensions. Let $\mathbf{X}_{n \times p}$ be the data matrix, an n-d projection can be seen as a linear transformation $T: \mathbb{R}^p \mapsto \mathbb{R}^d$ defined by $\mathbf{P} = \mathbf{X} \cdot \mathbf{A}$, where $\mathbf{P}_{n \times d}$ is the projected data and $\mathbf{A}_{p\times d}$ is the projection basis. Define $f: \mathbb{R}^{p \times d} \mapsto \mathbb{R}$ to be an index function that maps the projection basis $\mathbf{A}$ onto an index value $I$, this function is commonly known as the projection pursuit index (PPI) function, or the index function and is used to measure the "interestingness" of a projection. A number of index functions have been proposed in the literature to detect different data structures, including Legendre index [@friedman1974projection], Hermite index [@hall1989polynomial], natural Hermite index [@cook1993projection], chi-square index [@posse1995projection], LDA index [@lee2005projection] and PDA index [@lee2010projection]. 

In their initial paper, @friedman1974projection noted that "..., the technique used for maximising the projection index strongly influences both the statistical and the computational aspects of the procedure." Hence, effective optimisation algorithms are necessary for projection pursuit to find the bases that give interesting projections. While we leave the formal construction of the optimisation problem and existing algorithms to section \ref{tour-optim}, we outline the general idea here. Given a random starting (current) basis, projection pursuit repeatedly searches for candidate bases nearby until it finds one with higher index value than the current basis. In the second round, that basis becomes the current basis and the repetitive sampling  continues. The process ends until no better basis can be found or one of the termination criteria is reached.

A tour method, guided tour is usually used in conjunction with projection pursuit to interactively explore multivariate data. Tour plays an animation on the projected data which is constructed using the bases outputted by projection pursuit and their interpolations. Guided tour constructs the interpolation using the shortest geodesic path between two outputted bases and we refer readers to @buja2005computational for the mathematical details. Figure \ref{tour-path} shows a sketch of the tour path consisting of the blue frames produced by the projection pursuit optimisation algorithm and the white frames, which are the interpolations between two blue frames. The tour method has been implemented in the *tourr* package in R, available on the Comprehensive R Archive Network at [https://cran.r-project.org/web/packages/tourr/](https://cran.r-project.org/web/packages/tourr/) [@wickham2011tourrpackage]. 


<!-- A tour path includes two major components: a *generator* that generating the target basis that has higher index value than the current basis and an *interpolator* that performing geodesic interpolation between the current and target basis. An illustration modified from [@buja2005computational] vividly depicts the tour path in guided tour. -->


<!-- The pseudo-code below illustrates the implementation of guided tour in the tourr package. Given an projection pursuit index function and a randomly generated projection basis (current basis), the optimisation procedure produces a target basis inside `generator()`  Both the current basis and the target basis will be supplied to `tour_path()` to prepare information needed for constructing a geodesic path. This information is then used to compute a series of interpolating bases inside the `tour()` function. All the basis will be sent to create animation for visualising the tour in the `animate()` function.   -->


<!-- ```{r algo,eval = FALSE, echo = TRUE} -->
<!-- animation <- function(){ -->

<!--   # compute projection basis  -->
<!--   tour <- function(){ -->

<!--     # construct bases on the tour path -->
<!--     new_geodesic_path <- function(){ -->
<!--       tour_path <- function(){ -->

<!--         # GENERATOR: generate projection basis via projection pursuit -->
<!--         guided_tour <- function(){ -->
<!--           generator <- function(){ -->

<!--             # define projection pursuit index -->
<!--             # generate the target basis from the current basis via optimisation -->
<!--           } -->
<!--         } -->

<!--         # prepare geodesic information needed for interpolating along the tour path -->
<!--       } -->
<!--     } -->

<!--     # INTERPOLATOR: interpolate between the current and target basis  -->
<!--     function(){ -->
<!--       # generate interpolating bases on the geodesic path -->
<!--     } -->
<!--   } -->

<!--   # animate according to different display methods -->
<!-- } -->
<!-- ``` -->



```{r tour-path, out.width="100%", out.height="60%", fig.cap="\\label{tour-path}An illustration of the tour path"}
knitr::include_graphics(path = here::here("figures/tour_path_keynote/tour_path_keynote.001.jpeg"))
```


## Optimisation problem and existing algorithms {#tour-optim}


Now we begin to formulate the optimisation problem. Given a randomly generated starting basis $\mathbf{A}_1$, projection pursuit finds the final projection basis $\mathbf{A}_T = [\mathbf{a}_1, \cdots, \mathbf{a}_d]$, where $\mathbf{a}_i \in \mathbb{R}^{p}$, satisfies the following optimisation problem: 


\begin{align}
&\arg \max_{\mathbf{A} \in \mathcal{A}} f(\mathbf{X} \cdot \mathbf{A}) \\
s.t. & \mathcal{A} = \{ \forall \mathbf{a}_i, \mathbf{a}_j \in \mathbf{A}: \lVert \mathbf{a}_i \rVert = 1 \text{ and } \mathbf{a}_i \mathbf{a}_j = 0 \}
\end{align}

There are three existing algorithms.

@posse1995projection proposed a random search algorithm that samples a candidate basis $\mathbf{A}_{l}$ in the neighbourhood of the current basis $\mathbf{A}_{\text{cur}}$ by $\mathbf{A}_{l} = \mathbf{A}_{\text{cur}} + \alpha \mathbf{A}_{rand}$, where $\alpha$ controls the radius of the sampling neighbourhood and $\mathbf{A}_{rand}$ is a randomly generated matrix with the same dimension as $\mathbf{A}_{\text{cur}}$. The optimiser keeps sampling bases near the current basis until it finds one with higher index value than the current basis and then outputs it for guided tour to construct the interpolation path. A new round of search continues to find a better basis after the interpolation finishes. The halving parameter $c$ with default value of 30 is designed to adjust the searching neighbourhood $\alpha$. When the search needs to sample more than $c$ number of basis to find an outputted basis, the neighbourhood parameter $\alpha$ will be reduced by half in the next iteration. If the optimiser can't find a better basis within the maximum number of tries $l_{\max}$, the algorithm stops. The algorithm is summarised in Algorithm \ref{random-search} for one iteration.

\begin{algorithm}
\SetAlgoLined
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
    \Input{The current projection basis: $\mathbf{A}_{\text{cur}}$; The index function: $f$}
    \Output{The target basis: $\mathbf{A}_{l}$}
  initialisation\;
  Set $l = 1$ and $c = 0$\;
  \While{$l < l_{\max}$}{
    Generate $\mathbf{A}_{l} = \mathbf{A}_{\text{cur}} + \alpha \mathbf{A}_{\text{rand}}$ ensuring $\mathbf{A}_{l}$ is orthonormal\;
    Compute $I_{l}  = f(\mathbf{A}_{l})$\;
    \eIf{$I_{l} > I_{\text{cur}}$}{
      \KwRet{$\mathbf{A}_{l}$} \;
      }{
      $c = c + 1$\;
      }
    $l = l + 1$\;
  }
  \caption{random search}
  \label{random-search}
\end{algorithm}


@cook1995grand explained the use of a gradient ascent optimisation with an assumption that the index function is continuous and differentiable. Since some indices could be non-differentiable, the derivative is replaced by a pseudo-derivative of evaluating five randomly generated directions in a tiny nearby neighbourhood. Once the best direction is determined, the optimiser finds the best projection $\mathbf{A}_{**}$ on the geodesic between the current basis $\mathbf{A}_{\text{cur}}$ and the best directional basis $\mathbf{A}_{*}$. If the percentage change in the index value between $\mathbf{A}_{**}$ and $\mathbf{A}_{\text{cur}}$ is greater than a threshold value, $\mathbf{A}_{**}$ is outputted for the current iteration to construct the interpolation or the algorithm repeats the above steps until $l_{\max}$ is reached and the searching terminates. Algorithm \ref{search-geodesic} summarise the steps in geodesic search.

\begin{algorithm}
\SetAlgoLined
\SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
    \Input{The current projection basis: $\mathbf{A}_{\text{cur}}$; The index function: $f$}
    \Output{The target basis: $\mathbf{A}_{**}$}
  initialisation\;
  Set $l = 1$\;
  \While{$l < l_{\max}$}{
    Generate ten bases in five random directions: $\mathbf{A}_{l}: \mathbf{A}_{l+9}$ within a small neighbourhood $\text{dist}$\;
    Find the direction with the largest index value: $\mathbf{A}_{*}$ where $l < * < l+ 9$\;
    Construct the geodesic $\mathcal{G}$ from $\mathbf{A}_{\text{cur}}$ to $\mathbf{A}_{*}$\;
    Find $\mathbf{A}_{**}$ on the geodesic $\mathcal{G}$ that has the largest index value \;
    Compute $I_{**} = f(\mathbf{A}_{**})$, $p_{\text{diff}} = (I_{**} - I_{\text{cur}})/I_{**}$\;
      \If{$p_{\text{diff}} > 0.001$}{
         \KwRet{$\mathbf{A}_{**}$} \;
      }
    $l = l + 1$\;
  }
  \caption{search geodesic}
  \label{search-geodesic}
\end{algorithm}

Simulated annealing [@bertsimas1993simulated, @kirkpatrick1983optimization] is a non-derivative procedure based on a non-increasing cooling scheme $T(l)$. Given an initial $T_0$, the temperature at iteration $l$ is defined as $T(l) = \frac{T_0}{\log(l + 1)}$. The simulated annealing algorithm works as follows. Given a neighbourhood parameter $\alpha$ and a randomly generated orthonormal basis $A_{\text{rand}}$, a candidate basis is sampled by $\mathbf{A}_{t+1} = (1 - \alpha)\mathbf{A}_{t} + \alpha \mathbf{A}_{\text{rand}}$. If the index value of the candidate basis is larger than the one of the current basis, the candidate basis is outputted. While if it is smaller, the candidate has another chance of being accepted with probability $P= \min\left\{\exp\left[\frac{I_{cur} - I_{l}}{T(l)}\right],1\right\}$ where $I_{\cdot}$ denotes the index value of a given basis. The algorithm can be summarised as in Algorithm \ref{simulated_annealing}.

\begin{algorithm}
\SetAlgoLined
\SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
    \Input{The current projection basis: $\mathbf{A}_{\text{cur}}$; The index function: $f$; Initial temperature: $T_0$}
    \Output{The global object; The target basis: $\mathbf{A}_{l}$}
  initialisation\;
  Set $l = 1$\;
  \While{$l < l_{\max}$}{
    Generate $\mathbf{A}_{l} = (1 - \alpha)\mathbf{A}_{\text{cur}} + \alpha \mathbf{A}_{\text{rand}}$ ensuring $\mathbf{A}_{l}$ is orthonormal \;
    Compute $I_{l} = f(\mathbf{A}_{l})$ and $T(l) = \frac{T_0}{\log(l + 1)}$\;
      \eIf{$I_{l} > I_{\text{cur}}$}{
        \KwRet{$\mathbf{A}_{l}$} \;
      }{
        Compute $P= \min\left\{\exp\left[\frac{I_{\text{cur}} -I_{l}}{T(l)}\right],1\right\}$\;
        Draw $U$ from a uniform distribution: $U \sim \text{Unif(0, 1)}$\;
        \If{$P > U$}{
           \KwRet{$\mathbf{A}_{l}$} \;
        }
      }
    $l = l + 1$\;
  }
  \caption{simulated annealing}
  \label{simulated_annealing}
\end{algorithm}

Notice that the three algorithms above constitute one search. The outputted basis is then sent to construct the interpolation and one such search-and-interpolate step is one round.

<!-- Below listed several features characterise the optimisation procedures needed in projection pursuit -->

<!-- - *Being able to handle non-differentiable PPI function*: The PPI function could be non-differentiable, thus derivative free methods are preferred.  -->

<!-- - *Being able to optimise with constraints*: The constraint comes from projection matrix being an orthonormal matrix.  -->

<!-- - *Being able to find both local and global maximum*: Although the primary interest is to find the global maximum, local maximum could also reveal structures in the data that are of our interest.  -->


\newpage 

# Visual diagnostic system {#vis-diag}

## Motivation
<!-- Visual diagnostics can be real-time or post-run. Real time diagnostic directly uses the data produced in the algorithm to produce visual representation and thus doesn't need to store the data.   This section focuses on the definition and production of post-run diagnostics and the next section discusses real-time diagnostics. -->

We can see from Figure \ref{tour-path} that only the data projected using the outputted and interpolated bases are animated by tour; none of the searching bases sampled by the optimsiation algorithms are presented in the tour animation. Although these algorithms have been demonstrated to find the global maximum in various literature as we cited above, they focus solely on finding the global maximum and fewer tools are available to explore how each algorithm searches the parameter space and compare the results between different algorithms. 

Visualisation has been widely used in exploratory data analysis. Presenting information in a graphical format often allows people to see information they would otherwise not see. This motivates us **to develop a visual framework to diagnose optimisation algorithms in projection pursuit with the aim to understand and compare different existing algorithms.** This study is meaningful because **as more complex index functions are proposed based on statistical theory, only computationally effective optimisation algorithms allow us to find the projection basis that presents interesting structure in the data**.

The idea of generalised framework for diagnostic plots is inspired by the concept of grammar of graphic [@wickham2010layered], which powers the primary graphical system in R, ggplot2 [@ggplot2]. In grammar of graphic, plots are not defined by its appearance (i.e. boxplot, histogram, scatter plot etc) but by "stacked layers". Using this design, ggplot does not have to develop a gazillion of functions that each produces a different type of plot from a different data structure. Instead, it aesthetically maps variables (and its statistical transformation) in a dataset to different geometric objects (points, lines, box-and-whisker etc) and builds the plot through overlaying different layers. 

There are different ways to represent the same data in rectangular form. Certain cleaning steps are needed, before using ggplot2, to bring the data into a tidy data format [@wickham2014tidy] where 1) each observation forms a row; 2) each variable forms a column and 3) each type of observational unit forms a table. In a tidy format, data wrangling and visualisation are greatly simplified.

Hence the study **contributes to set up a tidy data structure for collecting observations from optimisation algorithms and prescribe a set of rules for generalising diagnostic plots for optimisation algorithms, specifically those used for projection pursuit.** 

<!-- [Do I need this paragraph to details the grammar of graphic? xxx - don't think so; not particularly relevant - assume most people know it roughly] -->
<!-- A layer includes 1) the data that powers the plot; 2) a geometric object that represents the visual shape of the data and 3) relevant statistical transformation that transform the information in the data to the information used to draw the geometric object. An important concept in the grammar of graphic is *aesthetic mapping*. Aesthetic mapping links the variable in a dataset to information needed to produce a geometric object. For example, we map one variable on the x-axis and another on the y-axis to create a scatterplot. Another example of creating a boxplot involves 1) mapping one variable on the x-axis and 2) transforming another variable to its five-point-summary and mapping the five-point-summary to the y-axis.  -->


## Global object

Global object is a data construction with key components from optimisation algorithms and is designed for making diagnostic plots easier. In the optimisation algorithms for projection pursuit, three key elements are 1) projection bases: $\mathbf{A}$, 2) index values: $I$ and 3) State: $S$, which labels the observation with detailed stage of searching or interpolation.

Multiple iterators are needed to index the data collected at different levels. $t$ is a unique identifier that prescribes the natural ordering of each observation; $j$ is the counter for each search-and-interpolate round, which remains the same within one round and has an increment of one once a new round starts. $l$ is the counter for each search/interpolation allowing us to know how many basis the algorithm has searched before finding one to output. 

<!-- For each level of iteration, we design two types of indices: *index per iteration: $j$* is fixed for each observations in one iteration and has an increment of one once a new iteration starts. *index within iteration: $l$* has an increment of one for each observation in an iteration and starts over from one once a new iteration starts. -->

<!-- In projection pursuit optimisation algorithms, there is one level of iteration and hence exists three iterators: `id` indices each observation by a unique number; `tries` is the index per iteration iterator that gets updated once a search-and-interpolate step is finished; and `loop` is the index within iteration iterator and starts over from one at the beginning of a new `tries`. We give the interpolating basis a different index $k$ for projection pursuit guided tour. It is similar in nature to $l$ but is specific for interpolating bases, which is usually not part of the optimisation. -->


<!-- # ```{r iterators, out.width = "100%", out.height = "25%", fig.cap = "\\label{iterators} A sketch of the design of iterators in iterative algorithms."} -->
<!-- # knitr::include_graphics(here::here("figures", "iterators.png")) -->
<!-- # ``` -->


There are other parameters that are of our interest and we denote them as *$V_{p}$*. In projection pursuit, this includes $V_1 = \text{method}$, which tags the name of the algorithm used and $V_2 = \text{alpha}$, the neighbourhood parameter that controls the size in sampling candidate bases. The data structure can thus be shown as in Equation \ref{eq:data-structure}.

\begin{equation}
\left[
\begin{array}{c|ccc|cc|cc}
t & \mathbf{A} & I & S & j &  l  & V_{1} & V_{2}\\
\hline
1 & \mathbf{A}_1 & I_1 & S_1 & 1 & 1 & V_{11} & V_{21}\\
\hline
2 & \mathbf{A}_2 & I_2 & S_2 & 2 & 1  & V_{12}  & V_{22}\\
3 & \mathbf{A}_3 & I_3 & S_3 & 2 & 2  & V_{13}  & V_{23}\\
\vdots & \vdots &\vdots &\vdots  &\vdots & \vdots &\vdots  &\vdots\\
\vdots & \vdots & \vdots &\vdots & 2 & l_2 & \vdots  & \vdots\\
\hline
\vdots &\vdots & \vdots &\vdots & 2  & 1& \vdots & \vdots\\
\vdots &\vdots &\vdots &\vdots & 2 & 2& \vdots &  \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots & \vdots  &\vdots \\
\vdots &\vdots &\vdots &\vdots & 2 & k_2 &\vdots  & \vdots\\
\hline
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
\hline
\vdots & \vdots & \vdots &\vdots  & J &  1 & \vdots & \vdots \\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
T & \mathbf{A}_T & I_T &S_T  & J &  l_{J} & V_{1T}& V_{2T}\\
\hline
\vdots &\vdots & \vdots &\vdots & J  & 1& \vdots & \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots & \vdots  &\vdots \\
\vdots &\vdots &\vdots &\vdots & J & k_J &\vdots  & \vdots\\
\hline
\vdots& \vdots & \vdots & \vdots & J+1 & 1 & \vdots& \vdots\\
\vdots &\vdots &\vdots &\vdots &\vdots & \vdots &\vdots &\vdots \\
T^\prime & \mathbf{A}_{T^\prime} & I_{T^\prime} &S_{T^\prime}  & J+1 &  l_{J+1} & V_{1T^\prime}& V_{2T^\prime}\\
\end{array}
\right]
= 
\left[
\begin{array}{c}
\text{column name} \\
\hline
\text{search (start basis)} \\
\hline
\text{search} \\
\text{search} \\
\vdots \\
\text{search (outputted basis)} \\
\hline
\text{interpolate} \\
\text{interpolate} \\
\vdots \\
\text{interpolate} \\
\hline
\vdots \\
\hline
\text{search} \\
\vdots \\
\text{search (final basis)} \\
\hline
\text{interpolate} \\
\vdots \\
\text{interpolate} \\
\hline
\text{search (no output)} \\
\vdots \\
\text{search (no output)} \\
\end{array}
\right]
\label{eq:data-structure}
\end{equation}

where $T^{\prime} = T + k_{J}+ l_{J+1}$. Note that we deliberately denote the last round of search as $j = J+1$ and in that round there is no output/interpolation basis and the algorithm terminates. This notation allows us to denote the last complete search-and-interpolate round as round $J$ and hence the final basis is $A_T$ and highest index value found is $I_T$.

[outside the paper: I find the notation of current/target basis is confusing because the target basis in round $j$ becomes the current basis in round $j+1$. Also, when we start to have polish, the target basis may not be the current basis in the next round... The place where current/target is most appropriate is probably when describing the interpolation where the first one is always the current basis and the last is always the target basis. I think it is better to leave this language in the code]



A sketch of the global object for projection pursuit guided tour is presented in Figure \ref{fig:glb-obj}. 

[I feel this sketch was initially useful but now since we have the data matrix and the printed output of the global object, it doesn't any additional information. I'm still keeping it here but we may need to remove it if it's not useful :(]

```{r glb-obj, out.width= "100%", out.height="20%", fig.cap="\\label{glb-obj}The global object in projection pursuit guided tour."}
knitr::include_graphics(path = here::here("figures/global_obj.png"))
```



<!-- \begin{equation} -->
<!-- \left[ -->
<!-- \begin{array}{ccccccc} -->
<!-- \ast & \mathbf{A}_{1,1,\ast} & \mathbf{A}_{2, l_1 +1,\ast} & \cdots & \mathbf{A}_{J,\sum_{i = 1}^{J-1}l_i +1,\ast} & \mathbf{A}_{J+1,\sum_{i = 1}^{J}l_i +1 ,\ast} \\ -->
<!-- \ast & \mathbf{A}_{1,2,\ast} & \mathbf{A}_{2,l_1 +2,\ast} & \cdots & \mathbf{A}_{J,\sum_{i = 1}^{J-1}l_i +2,\ast} & \mathbf{A}_{J+1,\sum_{i = 1}^{J}l_i +2, \ast} \\ -->
<!-- \vdots & \vdots & \vdots & \ddots & \vdots &\vdots   \\ -->
<!-- \ast & \mathbf{A}_{1, l, \ast} & \mathbf{A}_{2, l_1 + l, \ast} & \cdots & \mathbf{A}_{J+1, \sum_{i = 1}^{J-1}l_i + l, \ast}  & \mathbf{A}_{J + 1, \sum_{i = 1}^{J}l_i + l, \ast}\\ -->
<!-- \vdots & \vdots & \vdots & \ddots & \vdots &\vdots   \\ -->
<!-- \ast & \mathbf{A}_{1, l_1-1, \ast} & \mathbf{A}_{2, l_1 + l_2-1, \ast} & \cdots & \mathbf{A}_{J, \sum_{i = 1}^{J-1}l_i -1, \ast} & \mathbf{A}_{J+1, \sum_{i = 1}^{J+1} l_i-1, \ast}\\ -->
<!-- \mathbf{A}_{0,\ast,\ast} & \mathbf{A}_{1, l_1, \ast} & \mathbf{A}_{2, l_1 + l_2, \ast} & \cdots & \mathbf{A}_{J, \sum_{i = 1}^{J}l_i , \ast} & \ast\\ -->
<!-- \hline -->
<!-- \ast &\mathbf{A}_{1, \ast, 1} & \mathbf{A}_{2, \ast, k_1 + 1} & \cdots &\mathbf{A}_{J, \ast,\sum_{i = 1}^{J-1}k_i + 1}  & \ast\\ -->
<!-- \ast &\mathbf{A}_{1, \ast, 2} & \mathbf{A}_{2, \ast, k_1 + 2} & \cdots &\mathbf{A}_{J, \ast, \sum_{i = 1}^{J-1}k_i +2}  & \ast \\ -->
<!-- \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ -->
<!-- \ast &\mathbf{A}_{1, \ast, k} & \mathbf{A}_{2, \ast, k_1 + k} & \cdots &\mathbf{A}_{J, \ast, \sum_{i = 1}^{J-1}k_i +k}  & \ast \\ -->
<!-- \vdots & \vdots & \vdots & \ddots & \vdots   & \vdots \\ -->
<!-- \ast &\mathbf{A}_{1, \ast, k_1} & \mathbf{A}_{2, \ast, k_1 + k_2} & \cdots &\mathbf{A}_{J, \ast,\sum_{i = 1}^{J}k_i} & \ast \\ -->
<!-- \end{array} -->
<!-- \right] -->
<!-- = -->
<!-- \left[ -->
<!-- \begin{array}{c} -->
<!-- \vphantom{A^J_1}\text{candidate basis} \\ -->
<!-- \vphantom{A^J_2}\text{candidate basis} \\ -->
<!-- \vphantom{\vdots} \vdots \\ -->
<!-- \vphantom{A^J_2}\text{candidate basis} \\ -->
<!-- \vphantom{\vdots} \vdots \\ -->
<!-- \vphantom{A^J_2}\text{candidate basis} \\ -->
<!-- \vphantom{A^J}\text{current/target basis} \\ -->
<!-- \hline -->
<!-- \vphantom{\text{NULL}}\text{interpolation basis} \\ -->
<!-- \vphantom{\text{NULL}}\text{interpolation basis} \\ -->
<!-- \vphantom{\text{NULL}}\vdots \\ -->
<!-- \vphantom{\text{NULL}}\text{interpolation basis} \\ -->
<!-- \vphantom{\vdots} \vdots \\ -->
<!-- \vphantom{\text{NULL}}\text{interpolation basis} \\ -->
<!-- \end{array} -->
<!-- \right] -->
<!-- \label{eq:matrixA} -->
<!-- \end{equation} -->





<!-- An optimisation procedure can easily generate hundreds or thousands sampling points and the question becomes what are the points we want to explore... [the classification of searching or updating points goes here and people should be clear what part of the points goes to the plot - this is the data part not the plot part. ] -->

<!-- In tour, all the points recorded in the global object can be divided into two broad categories: searching points and interpolating points -->

<!-- - *Searching points* include the observations recorded in the searching algorithm to find the target basis. The points for target bases is also included in the searching points and there is one such point per `tries`. -->

<!-- - *interpolating points* exist in the guided tour to produce continuous animated view from one target basis to another and it doesn't have `loop` value. -->

<!-- The concept of grammar of graphic requires a dataset to be supplied in making a plot, thus a global object needs to be created in the iterative algorithm. The variables will be mapped into element of a graphic to explore and thus should contain all the parameters of interest. The next section shows how a global object is created in projection pursuit guided tour. -->


<!-- In the current implementation of the `tourr` package, while the target basis generated by the projection pursuit can be accessed later via `save_history()`, interpolating bases and those randomly nearby bases generated in the optimisation are not stored. This creates difficulties for fully understand the behaviour of the optimisation and interpolation of tour in complex scenario **[need a rephrase this part].**  -->

## Simulated data

We simulate some random variables of size 1000 with different structures. `x1`, `x8`, `x9` and `x10` are simulated from normal distribution with zero mean and variance of one as in equation \ref{eq:sim-norm}. When using projection pursuit to explore the data structure based on its departure from normality, the entry in the projection basis for these variables should be close to zero in theory. `x2` to `x7` are mixture of normal distributions with different weights and locations. Equation \ref{eq:sim-x2} to \ref{eq:sim-x7} outlines the distribution where each variable is simulated from and Figure \ref{origin-data} shows the histogram of each variable except `x3`. All the variables are then scaled to have unit variance before running the projection pursuit.

\begin{align}
x_1 \overset{d}{=} x_8 \overset{d}{=} x_9 \overset{d}{=} x_{10}& \sim \mathcal{N}(0, 1) \label{eq:sim-norm} \\
x_2 &\sim 0.5 \mathcal{N}(-3, 1) + 0.5 \mathcal{N}(3, 1)\label{eq:sim-x2}\\
\Pr(x_3) &= 
\begin{cases}
0.5 & \text{if $x_3 = -1$ or $1$}\\
0 & \text{otherwise}
\end{cases}\label{eq:sim-x3}\\
x_4 &\sim 0.25 \mathcal{N}(-3, 1) + 0.75 \mathcal{N}(3, 1) \label{eq:sim-x4}\\
x_5 &\sim \frac{1}{3} \mathcal{N}(-5, 1) + \frac{1}{3} \mathcal{N}(0, 1) + \frac{1}{3} \mathcal{N}(5, 1)\label{eq:sim-x5}\\
x_6 &\sim 0.45 \mathcal{N}(-5, 1) + 0.1 \mathcal{N}(0, 1) + 0.45 \mathcal{N}(5, 1)\label{eq:sim-x6}\\
x_7 &\sim 0.5 \mathcal{N}(-5, 1) + 0.5 \mathcal{N}(5, 1) 
\label{eq:sim-x7}
\end{align}

```{r origin-data, fig.cap="\\label{origin-data} The distribution of simulated data except x3"}
origin_dt_bi <- data_mult %>% 
  dplyr::select(-x3) %>% 
  gather(names, values) %>%
  mutate(names = as_factor(names))
  
origin_dt_bi %>%
  ggplot(aes(x = values)) +
  geom_histogram(binwidth = 0.3) +
  geom_density(aes(y = 0.3 * ..count..)) +
  facet_wrap(vars(names), ncol = 3)
```

We form our first dataset using variables `x1`, `x2`, `x8`, `x9` and `x10` and run the guided tour with optimiser `search_better`. When the optimisation ends, the global object will be stored and printed (it can be turned off by supplying argument `print = FALSE`). Additional messages during the optimisation can be displayed by argument `verbose = TRUE`. Below shows the first ten rows of the global object. Notice that the tibble object allows the list-column `basis` to be printed out nicely with the dimension of the projection basis readily available.

[I notice that in the implementation the loop index for interpolation starts from 0 rather than 1. Need more investigation to see if it is easy to change it to 1 or we will have to change the definition of the data structure.]

```{r code, eval = FALSE}
set.seed(123456)
holes_1d_better <- animate_dist(data, tour_path = guided_tour(holes(), d = 1,
                                           search_f =  search_better),
                  rescale = FALSE, verbose = TRUE)

#save(temp, file = here::here("vignettes", "data", "temp.rda"))
```

\newpage

```{r glb-obj-1}
holes_1d_better %>% 
  dplyr::select(id, basis, index_val, info, tries, loop, method, alpha) %>% 
  head(10)
```

<!-- `tries` has an increment of one once the generator is called (equivalently a new target basis is generated); `info` records the stage the basis is in. This would include the `interpolation` stage and the detailed stage in the optimisation i.e. `direction_search`, `best_direction_search`, `line_search`and `best_line_search` for geodesic searching (`search_geodesic`); `random_search` and `new_basis` for simulating annealing (`search_better`). `loop` is the counter used for the optimisation procedure and thus will be `NA` for interpolation steps. `id` creates a sequential order of the basis.  -->

<!--  Another examples is a 2D projection of the larger dataset with two informative variable (`x2` and `x7`) using search_better method. Notice in this example, the dimension of the bases becomes 6 by 2. -->


<!-- ```{r glb-obj-2, echo = TRUE} -->
<!-- holes_2d_better %>% head(5)  -->
<!-- ``` -->




# Visual diagnostic plots {#vis-diag-plots}

Below we will present several examples of diagnosing different aspects of the projection pursuit optimisation. We will present:  

1) static plots to explore the index value,

2) animated plots to explore the projection basis and,  

3) a self-contained example to optimise a complex index function. 

In the first two sections, we will first provide a toy example that is easy to grasp and then more examples that can help us to understand the algorithm and parameter choice. Remember the research question we raised earlier, the purpose of visual diagnostics is to understand: 

- Whether the algorithm has successfully found the maximum and how the index value changes throughout the algorithm?

- How does the searching space look like, that is, geometrically, where are the projection bases located in the space?

The first question can be answered using a static plot with x-axis showing the progression of the optimisation and y-axis showing the value of the objective function. The second question can be addressed via visualising the rotating high dimensional space or its projection on the reduced 2D space. Thus animated visualisation is needed to perceive the optimisation path in the searching space.

<!-- [This seems to be a hard task to see the rotation of high dimensional space, but the way humans understand the dimension of a 3D object is either through rotating the physical 3D object or using shade and line type to annotate the 3D object in a paper or electronically. Thus we can do a similar rotation on the screen to precieve even higher dimension. there is likely to be a learning curve - remember when learning geometry in school, it takes a while to get used to see 3D cube on a paper].  -->

Since the global object is already tidy, not further tidying steps is needed, while certain data wrangling steps [@wickham2016rfordatascience] are still needed to transform the global object into  desirable format for one particular visualisation. To emphasize on this good practice of data analysis, we will describe the transformation steps needed for each diagnostic plots before stepping into visualisation.

<!-- The plot design of visual diagnostics depends on the characteristics of the variables to plot. If the searching points are of interest [point geom is not good; need summarise. While updating points are more manageable -> point geom]. However, one must realise that with hundreds or thousands of searching points, exploring the sample space in animation could be slow and this is because of the time it takes to render hundreds point. [stratefy may help? - may need more work here.] -->

## Explore the value of objective function {#static}

### Searching points

A primary interest of diagnosing an optimisation algorithm is to study how it finds its optimum progressively. We could plot the index value across its natural ordering, however, different iterations may have different number of points and, towards the end of the search there could easily be hundreds of bases being tested before the target basis is found. In the plot, points from those iterations towards the end will occupy the vast majority of the plot space. This motivates to use summarisation.  Rather than knowing the index value of *every* basis, we are more interested to have a general summary of all the index value in that iteration and more importantly, the basis with the largest index value (since it prescribes the next geodesic interpolation and future searches). 

Boxplot is a suitable candidate that provides five points summary of the data, however it has one drawback: it does not report the number of points in each box. We may risk losing information on how many points it takes to find the target basis by displaying the boxplot alone for all `tries`. Thus, the number of point in each iteration is displayed at the bottom of each box and we provide options to switch iteration with small number of points to a point geometry, which is achieved via a `cutoff` argument. A line geometry is also added to link the points with the largest index value in each iteration. This helps to visualise the improvement made in each iteration. Using the concept of *gramma of graphics* [@wickham2010layered], the plot for exploring index value can be defined in three layers as:

- Layer 1: boxplot geom
  - data: group by $j$ and filter the observations in the group that have count greater than `cutoff = 15`.
  - x: $j$ is mapped to the x-axis
  - y: the statistical transformed index value: $Q_{I^{\prime}_t}(q)$ is mapped to the y-axis where $Q_X(q)$, $q = 0, 25, 50, 75, 100$ finds the qth-quantile of $X$ and $I^{\prime}_t$ denotes the index value of all the searching bases defined in Matrix \ref{eq:data-structure}.



- Layer 2: point geom
  - data: group by $j$ and filter the observations in the group that have count less than `cutoff = 15`.
  - x: $j$ is mapped to the x-axis
  - y: $I$ is mapped to the y-axis 

- Layer 3: line geom
  - data: filter the points with the highest index value in group $j$
  - x: $j$ is mapped to the x-axis
  - y: $I$ is mapped to the y-axis 

#### Toy example: exploring searching points 

We choose variables `x1`, `x2`, `x3`, `x8`, `x9` and `x10` to perform a 2D projection with tour. Parameter `search_f = tour::search_better` and `max.tries = 500` is used. The index value of the searching points are shown in Figure \ref{points}. The label at the bottom indicates the number of observations in each iteration and facilitates the choice of `cutoff` argument (by default `cutoff = 15`). We learn that the `search_better` quickly finds better projection basis with higher index value at first and then takes longer to find a better one later.

```{r points-tries,fig.asp=0.5, fig.cap="\\label{points}A comparison of plotting the same search points with different plot designs. The left plot does not use the plot space efficiently to convey information from the plot while the right plot provides good summarisation of data and number of points in each tries."}

dt <- holes_2d_better_max_tries %>%
  filter(info != "interpolation") %>% 
  mutate(id = row_number()) 

dt %>% explore_trace_search()

```

### Interpolating points

Sometimes, rather than exploring the searching points, we may be interested in exploring the points on the interpolation path (target and interpolating points) since these points will be played by the tour animation. Since interpolation paths are geodesically the shortest, a summarisation using boxplot geometry is no longer needed. The slightly modified plot definition is shown below:

- Layer 1: point geom
  - data: filter the observations with $S = \text{interpolation}$ and mutate $t$ to be the row number of the subsetted tibble
  - x: $t$ is mapped to the x-axis
  - y: $I$ is mapped to the y-axis 

- Layer 2: line geom
  - using line geometry for the same data and aesthetics 

#### A more complex example: Interruption

```{r interpolation}
#points is always included in the newest version
# p1 <- holes_2d_better %>%
#   filter(info == "interpolation") %>%
#   ggplot(aes(x = tries, y = index_val, col = as.factor(tries))) +
#   geom_point() +
#   geom_line(aes(group = 1))
# 
# p2 <- holes_2d_better %>% explore_trace_interp() + geom_point(aes(col = as.factor(tries)))
# 
# (p1 | p2)  &
#   theme(legend.position = "bottom")
# 
```

We use the same dataset as the toy example above to explore the search function `search_better` and we want to learn how the index value changes on the interpolation path for the `holes` index. From the left panel of Figure \ref{interruption}, we observe that when interpolating from the current basis to the target basis, the index value may not be monotone: we could reach a basis with a higher index value than the target basis on the interpolation path. In this sense, we would be better off using the basis with the highest index value on the interpolation path as the current basis for the next iteration (rather than using the target basis). Hence, an interruption is constructed to accept the interpolating bases only up to the one with the largest index value. After implementing this interruption, the search finds higher final index value with fewer steps as shown in the right panel of Figure \ref{interruption}.

```{r interruption,fig.asp=0.5, fig.cap = "\\label{interruption}Trace plots of the interpolated basis with and without interruption. The interruption stops the interpolation when the index value starts to decrease at id = 60. The implementation of the interuption finds an ending basis with higher index value using fewer steps. "}
p1_anno <- interrupt_no %>% filter(info == "interpolation") %>% mutate(id = row_number()) %>% filter(id %in% c(44, 60, 62)) %>% 
  mutate(anno = c("current basis", "interpolated basis", "target basis"))

p1 <- interrupt_no %>% mutate(id = row_number() - 1) %>% explore_trace_interp() + ggtitle("without interruption") + 
  geom_point(data = p1_anno) + 
  geom_label_repel(data = p1_anno, aes(label = anno), box.padding = 0.5) + ylim(0.8, 0.9) + xlim(0, 80) + 
  theme(legend.position = "none")
  
p2 <- interrupt_yes %>% explore_trace_interp() + ggtitle("with interruption") + ylim(0.8, 0.9) + xlim(0, 80) + 
  theme(legend.position = "none")

p1 | p2
#xxx up and down plot allows for displaying time series features while left and right display makes it easier for comparison? The same problem for plot of comparing between different searching methods
```

### Polishing points

<!-- In the previous two sections, only the iterator and the index value are mapped onto the x and y aesthetics of the plot; while more aesthetics i.e. colour, could be added to compare other parameters in the global object. -->

<!-- Two examples are shown below to explore and compare different searching methods and neighbourhood parameter alpha.  -->

In principle, all the optimisation routines should result in the same output for the same problem while this may not be the case in real application. This motivates the creation of a polishing search that polishes the final basis found and achieves unity across different methods. 

`search_polish` takes the final basis of a given search as a start and uses a brutal-force approach to sample a large number of basis (`n_sample`) in the neighbourhood. Among those sampled basis, the one with the largest index value is chosen to be compared with the current basis. If its index value is larger than that of the current basis, it becomes the current basis in the next iteration. If no basis is found to have larger index value, the searching neighbourhood will shrink and the search continues. The polishing search ends when one of the four stopping criteria is satisfied:

1) the chosen basis can't be too close to the current basis 
2) the percentage improvement of the index value can't be too small 
3) the searching neighbourhood can't be too small
4) the number of iteration can't exceed `max.tries`

The usage of search_polish is as follows. After the first search, the final basis from the interpolation is extracted and supplied to the second search as the `start` argument.  `search_polish` is used as the search function. All the other arguments should remain the same. 

```{r sample-code, eval = FALSE, echo = TRUE}
set.seed(123456)
holes_2d_geo <- animate_xy(data_mult[,c(1,2, 7:10)],tour_path = 
                             guided_tour(holes(), d = 2, 
                                         search_f = tourr:::search_geodesic),
                           rescale = FALSE, verbose = TRUE)

last_basis <- holes_2d_geo %>% filter(info == "interpolation") %>% 
  tail(1) %>% pull(basis) %>% .[[1]]

set.seed(123456)
holes_2d_geo_polish <- animate_xy(data_mult[,c(1,2, 7:10)], tour_path = 
                                    guided_tour(holes(), d = 2, 
                                                search_f = tourr:::search_polish),
                                  rescale = FALSE, verbose = TRUE, 
                                  start = last_basis)
```

A slight variation of the plot definition due to the addition of polishing points is as follows:

- Layer 1: point geom
  - data: filter the observations with  $S = \text{interpolation}$; bind the global object from optimisation and interpolation and form polishing; mutate $t$ to be the row number of the binded tibble.
  - x: $t$ is mapped to the x-axis
  - y: $I$ is mapped to the y-axis 
  - colour: $V$ is mapped to the colour aesthetic

- Layer 2: line geom

<!-- Polishing points form on itself a global structure with candidate bases and output bases. If we're allowed to abuse the notation a bit, we denote $$A_{J, \ast, \sum_{i = 1}^J k_i + 1}$$ as the first polishing basis and $$A_{J, \ast, \sum_{i = 1}^J k_i + M}$$ as the last polishing basis, where $$M = \sum_i S_{i, \text{method}} \mathds{1}(\text{method} = \text{polish})$$ -->

<!-- Then the index plot incorporating both the interpolating points and polishing points can be defined as  -->

<!-- $$x = \{1, 2, \cdots, \sum_{i = 1}^J k_i,(\sum_{i = 1}^J k_i + 1), \cdots,  \sum_{i = 1}^J k_i + M\}$$ -->
<!-- $$y = \{I_{jlk}: k = x\}$$ -->

#### Another example: Polish

Again using the same data, we are interested to compare the effect of different `max.tries` in the 2D projection setting. `max.tries` is a hyperparameter that controls the maximum number of try before the search ends. The default value of 25 is suitable for 1D projection while we suspect it may not be a good option for the 2D case and hence want to compare it with an alternative, 500. As shown in Figure \ref{trace-compare}, both trials attain the same index value after polishing while the small `max.tries` of 25 is not sufficient for `search_better` to find its global maximum and we will need to adjust the `max.tries` argument for the search to sufficiently explore the parameter space.

```{r polish, fig.asp=0.5,fig.cap="\\label{trace-compare}Breakdown of index value when using different max.tries in search better in conjunction with search polish. Both attain the same final index value after the polishing while using a `max.tries` of 25 is not sufficient to find the true maximum."}

p1 <- bind_rows(holes_2d_better, holes_2d_better_polish) %>% 
  explore_trace_interp(col = method) + ggtitle("max.tries = 25")

p2 <- bind_rows(holes_2d_better_max_tries, holes_2d_better_max_tries_polish) %>% 
  explore_trace_interp(col = method) + ggtitle("max.tries = 500")

(p1 | p2) &
  theme(legend.position = "none") 
```


<!-- *Example: The neighbourhood parameter alpha* Add an example on comparing the neighbourhood parameter in search_better & search_posse. -->


<!-- ```{r nrow, echo = TRUE} -->
<!-- # nrow(holes_2d_better_max_tries) -->
<!-- # nrow(holes_2d_pos) -->
<!-- ``` -->


<!-- ```{r alpha, fig.asp=0.5,eval = FALSE} -->
<!-- p1 <- holes_2d_better_max_tries %>% -->
<!--   filter(info != "interpolation") %>% -->
<!--   mutate(id = row_number()) %>% -->
<!--   explore_trace_parameter(var = alpha) + -->
<!--   ylim(0.46, 0.505) + ggtitle("search_better") -->

<!-- p3 <- holes_2d_better_max_tries %>% explore_trace_interp(col = alpha) + ylim(0.8, 0.96) -->

<!-- p2 <- holes_2d_pos %>% -->
<!--   filter(info != "interpolation") %>% -->
<!--   mutate(id = row_number()) %>% -->
<!--   explore_trace_parameter(var = alpha) + ylim(0.46, 0.505) + ggtitle("search_posse") -->

<!-- p4 <- holes_2d_pos %>% explore_trace_interp(col = alpha) + ylim(0.8, 0.96) -->

<!-- (p1 | p2) / (p3 | p4) + plot_layout(guides = "collect") -->
<!-- ``` -->

\newpage

## Explore searching space {#animated}

In projection pursuit, the projection bases $\mathbf{A}_{p \times d}$ are usually of dimension $p \times d$ and hence can't be visualised in a 2D plot. An option to explore the searching space of these bases is to explore a reduced space via principal component analysis (PCA). The visualisation can thus be defined as

<!-- We first flat each projection bases $\mathbf{A}$ to a row vector and stack them by rows. This gives a matrix of dimension $n$ by $(p*d)$. Centering and scaling are usually done before performing PCA and we denotes the pre-processed matrix as $\tilde{\mathbf{A}}_{n \times (p \times d)}$. By definition, the principal components maximise the variance of the projection bases in the reduced space. Using lagrange multipler, this is equivalent to find the eigenvector of the variance-covariance matrix: $\Sigma = \tilde{\mathbf{A}}^T\tilde{\mathbf{A}}$. Decomposing $\tilde{\mathbf{A}}$ using singular value decomposition (SVD) gives $$\tilde{\mathbf{A}} = \mathbf{U} \mathbf{\lambda} \mathbf{V}$$ and hence the variance-covariance matrix -->

<!-- $$\Sigma = \tilde{\mathbf{A}}^T\tilde{\mathbf{A}} = (\mathbf{V}^T \mathbf{\lambda}^T \mathbf{U}^T) (\mathbf{U} \mathbf{\lambda} \mathbf{V}) = \mathbf{V}^T \mathbf{\lambda}^2 \mathbf{V}$$ -->

<!-- has eigenvector $\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, \cdots, \mathbf{v}_{p\times d}]$. The projection mapping to the x- and y-axis uses the first two principal components and hence $$\tilde{\mathbf{A}} \mathbf{v}_1 \text{ and } \tilde{\mathbf{A}} \mathbf{v}_2$$ -->

<!-- The resulting two vectors are of dimension $n \times 1$, which is compatible with the dimension of the global object. THis means all the iterators and interesting other variables defined in the global object can be bound with the these two projections and can be mapped to other aesthetics (i.e. colour). -->

- Layer 1: point geom
  - data: subset the basis of interest and arrange into a matrix format; perform PCA on the basis matrix and compute the projected basis on the first two principal components; bind the variables from the original global object and form a tibble
  - x: the projected basis on the first principal component
  - y: the projected basis on the second principal component
  - colour: $V$ is mapped to the colour aesthetic
  
<!-- In this way, the first two principle components will take up the x and y axis and all the other information will be mapped using other aesthetic attribution. [need to think further about how different type of variables can be shown].  -->


### A toy example: understand different stage of search_geodesic

*Example: understand search_geodesic* [feel like this example is merely explaining search geodesic algorithm, so maybe introduce the animated plot here? xxx] `search_geodesic` is a two-stage ascending algorithm with four different stages in the search and a PCA plot useful to understand how the algorithm progresses and the relative position of each basis in the PCA projected 2D space. Starting from the start basis, a directional search is conducted in a narrow neighbourhood on five random directions. The best one is picked and a line search is then run on the geodesic direction to find the target basis. The starting and target bases are then interpolated. In the next iteration, the target basis becomes the current basis and then the procedures continues. 

```{r pca, fig.cap = "\\label{pca}PCA plot of search geodesic colouring by info allows for better understanding of each stage in the geodesic search"}

holes_1d_geo %>% explore_proj_pca(col = info) 

#xxx the info level need to be reorder to show the start and directional search points better.
```

### A more complex example: Choosing the initial value for polishing parameter


*Example: initial value for polishing alpha* `search_polish` is a brute-force algorithm that evaluate 1000 points in the neighbourhood at each loop. Setting an appropriate initial value for polish_alpha would avoid wasting search on large vector space that are not likely to produce higher index value. The default initial value for polishing step is 0.5 and we are interested in whether this is an appropriate initial value to use after `search_geodesic`. The problem is a 1D projection of the small dataset using `search_geodesic` and followed by `search_polish`. The top-left panel of Figure \ref{polish-alpha} displays all the projection bases on the first two principal components, coloured by the `polish_alpha`. We can observe that rather than concentrating on the ending basis from `search_geodesic` as what polishing step is designed, `search_polish` searches a much larger vector space, which is unnecessary. Thus a customised smaller initial value for `polish_alpha` would be ideal. One way to do this is to initialised `polish_alpha` as the projection distance between the last two target bases. The top-right panel of Figure \ref{polish-alpha} shows a more desirable concentrated searching space near the ending basis. Both specifications of initial value allow the searches to reach the same ending index values. 


```{r polish-alpha, fig.cap = "\\label{polish-alpha}PCA plot of two different polish alpha initialisations. A default polish alpha = 0.5 searches a larger space that is unncessary while a small customised initial value of polish alpha will search near the ending basis. Both intialisations reach the same ending index values. "}
p1 <- bind_rows(holes_1d_geo, holes_1d_geo_polish_default_alpha) %>% 
  explore_proj_pca(col = as.factor(alpha))

p2 <- bind_rows(holes_1d_geo, holes_1d_geo_polish) %>% 
  explore_proj_pca(col = as.factor(alpha))

p3 <- bind_rows(holes_1d_geo, holes_1d_geo_polish_default_alpha) %>%
  explore_trace_interp(col = method) + ylim(0.75, 0.95)

p4 <- bind_rows(holes_1d_geo, holes_1d_geo_polish) %>% 
  explore_trace_interp(col = method) +  ylim(0.75, 0.95)

((p1 | p2) /(p3 | p4)) +  plot_layout(heights = c(2, 1)) & theme(legend.position = "none") 

```



While explore the reduced space is an initial attempt to understand the searching space, there are existing technology for rotating a higher dimensional space for visualisation. Geozoo is an option. It generates random points on the high dimensional space and we can overlay it with the points on the optimisation path to visualise the spread of it on the high-D sphere.

[add example from geozoo]

<!-- # Animated diagnostic plots -->

<!-- ```{r pca-animated, echo = TRUE, eval = FALSE} -->
<!-- holes_1d_geo %>% explore_proj_pca(animate = TRUE, col = info) +  -->
<!--   theme(legend.position = "bottom") -->
<!-- ``` -->

\newpage

## A comprehensive example of diagnosing a noisy index function

The interpolation path of holes index, as seen in Figure \ref{interruption}, is smooth, while this may not be the case for more complicated index functions. `kol_cdf` index, an 1D projection index function based on Kolmogorov test, compares the difference between the 1D projected data, $\mathbf{P}_{n \times 1}$ and a randomly generated normal distribution, $y_n$ based on the empirical cumulated distribution function (ECDF). Denotes the ECDF function as $F(u)$ with subscript indicating the variable,  the Kolmogorov statistics defined by

$$\max \left[F_{\mathbf{P}}(u) - F_{y}(u)\right]$$

can be seen as a function of the projection matrix $\mathbf{A}_{p \times 1}$ and hence a valid index function. 


### Explore index value

Figure \ref{kol-cdf} compares the tracing plot of the interpolating points when using different optimisation algorithms: `search_geodesic` and `search_better`. One can observe that 

- The index value of `kol_cdf` index is much smaller than that of holes index
- The link of index values from interpolation bases  are no longer smooth
- Both algorithms reach a similar final index value after polishing


Polishing step has done much more work to find the final index value in `search_geodesic` than `search_better` and this indicates `kol_cdf` function favours of a random search method than ascent method. 

```{r kol-cdf,fig.asp=0.5, fig.cap = "\\label{kol-cdf}Comparison of two different searching methods: search_geodesic and search_better on 1D projection problem for a noisier index: kol_cdf. The geodesic search rely heavily on the polishing step to find the final index value while search better works well."}
p1 <- bind_rows(kol_cdf_1d_geodesic, kol_cdf_1d_geodesic_polish) %>% explore_trace_interp(col = method)  + ggtitle("search_geodesic")

p2 <- bind_rows(kol_cdf_1d_better, kol_cdf_1d_better_polish) %>% explore_trace_interp(col = method) + ggtitle("search_better")

(p1 | p2 ) & theme(legend.position = "bottom") & ylim(0, 0.23)
```

Now we enlarge the dataset to include two informative variables: `x2` and `x3` and remain 1D projection. In this case, two local maxima appear with projection matrix being $[0, 1, 0, 0, 0, 0]$ and $[0, 0, 1 ,0, 0, 0]$. 

Using different seeds in `search_better` allows us to find both local maxima d as in Figure \ref{1d-2var-different-seeds}. Comparing the maximum of both, we can see that the global maximum happens when `x2` is found. It is natural to ask then if there is an algorithm that can find the global maximum without trying on different seeds? `search_better_random` manages to do it via a Metropolis-hasting random search as shown in Figure \ref{1d-2var-better-random}, although at a higher cost of number of points to evaluate. 

```{r 1d-2var-different-seeds,  fig.asp=0.5, fig.cap="\\label{1d-2var-different-seeds}The trace plot search better in a 1D projection problem with two informative variables using different seeds (without polishing). Since there are two informative variables, setting different value for seed will lead search better to find either of the local maximum."}
colour <- RColorBrewer::brewer.pal(3, "Dark2")
p5 <- kol_cdf_1d_2var_better %>% explore_trace_interp(col = 0) + 
  scale_colour_manual(values = colour[2]) + ggtitle("search_better, seed 123456")
p6 <- kol_cdf_1d_2var_better_2 %>% explore_trace_interp(col = 0) + 
  scale_colour_manual(values = colour[1]) + ggtitle("search_better, seed 12345")

(p5 | p6) & theme(legend.position = "none") & ylim(0, 0.25)
```

```{r 1d-2var-better-random, fig.asp=0.5, fig.cap = "\\label{1d-2var-better-random}Using search better random for the problem above will result in finding the global maximum but much larger number of iteration is needed."}
kol_cdf_1d_2var_better_random %>% 
  explore_trace_interp(col = 0) + 
  ylim(0, 0.25) + 
  scale_colour_manual(values = colour[3]) + 
  theme(legend.position = "none") + 
  ggtitle("search_better_random")
```

### Explore searching space

We can also plot the searching points of all three algorithms in the searching space and explore their relative position against each other using principal components. As shown in Figure \ref{1d-2var-explore-proj-pca}, the bases from better1 and better2 only search a proportion of the searching space while better_random produces a more exhaustive search. The large overlapping of better1 and better_random is explained by the fact that both algorithms find x2 in the end. 

```{r 1d-2var-explore-proj-pca, fig.cap = "\\label{1d-2var-explore-proj-pca} The projected projection basis using principal components. The bases from better1 and better2 only search a proportion of the searching space while better_random produces a more exhausive search. The large overlapping of better1 and better_random is explained by the fact that both algorithms finds x2 in the end."}
interp1 <- kol_cdf_1d_2var_better 
interp2 <- kol_cdf_1d_2var_better_2
interp3 <- kol_cdf_1d_2var_better_random 

bind_rows(interp1, interp2, interp3) %>% 
  mutate(algorithm = c(rep("better1", nrow(interp1)), 
                       rep("better2", nrow(interp2)), 
                       rep("better_random", nrow(interp3))),
         algorithm = fct_relevel(algorithm, c("better1", "better2", "better_random"))) %>% 
  explore_proj_pca(col = algorithm) + 
  scale_colour_brewer(palette = "Dark2") 


```



# Implementation: Ferrn package {#implementation}

Everything is coded up in a package. Package structure

# Conclusion

\clearpage
